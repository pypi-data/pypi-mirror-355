{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Agent State Machine (ASM) - Quick Start Guide\n",
                "\n",
                "An **agent** is a wrapper around a finite state machine designed to accomplish a specific task and will be referred to as ASM.\n",
                "\n",
                "A **StateMachineBuilder** is used to build the ASM from a manifest and a state diagram.\n",
                "\n",
                "```mermaid\n",
                "flowchart TD\n",
                "    A[\"state_diagram\"] -->|Input| B[\"StateMachineBuilder\"]\n",
                "    A2[\"state_manifest\"] -->|Input| C[\"StateModel\"]\n",
                "    C -->|Build| B\n",
                "    B --> D[\"fsm\"]\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Example: Standard Assistant\n",
                "\n",
                "In this example, we will demonstrate how to create a simple **Assistant** agent using a state diagram and state manifest.\n",
                "\n",
                "### a) Define State Diagram\n",
                "\n",
                "The following is a simple example of an **Assistant** agent using 3 states:\n",
                "\n",
                "* **INIT:** Collect initial input.\n",
                "* **GENERATE:** The state where the LLM generates a response.\n",
                "* **FINAL:** Return final output.\n",
                "\n",
                "```mermaid\n",
                "stateDiagram-v2\n",
                "direction LR\n",
                "INIT --> GENERATE: next / action\n",
                "GENERATE --> FINAL: next / action\n",
                "```\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "STATE_DIAGRAM = \"\"\"\n",
                "    INIT --> GENERATE\n",
                "    GENERATE --> FINAL\n",
                "    \"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### b) Define State Manifest\n",
                "\n",
                "The state manifest is a dictionary. \n",
                "Each state in the manifest corresponds to a state in the state diagram.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "STATE_MANIFEST_V1 = {\n",
                "        \"INIT\": {\n",
                "            \"input_data\": {\n",
                "                \"name\": \"Sara\",\n",
                "                \"user_message\": \"Write a one sentence story\"\n",
                "            }\n",
                "        },\n",
                "        \"GENERATE\": {\n",
                "            \"module_path\": \"gai.asm.states\",\n",
                "            \"class_name\": \"PureActionState\",\n",
                "            \"title\": \"GENERATE\",\n",
                "            \"action\": \"generate\",\n",
                "            \"input_data\": {\n",
                "                \"llm_config\": {\n",
                "                    \"type\": \"getter\",\n",
                "                    \"dependency\": \"get_llm_config\"\n",
                "                },\n",
                "            },\n",
                "            \"output_data\": [\n",
                "                \"streamer\",\n",
                "                \"get_assistant_message\"\n",
                "            ]\n",
                "        },\n",
                "        \"FINAL\": {\n",
                "            \"output_data\": [\"monologue\"]\n",
                "        }\n",
                "    }\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### c) Create state action"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from gai.asm import AsyncStateMachine\n",
                "from gai.llm.openai import AsyncOpenAI\n",
                "\n",
                "async def generate_action(state):\n",
                "    \n",
                "    llm_config = state.machine.state_bag[\"llm_config\"]\n",
                "    client = AsyncOpenAI(llm_config)\n",
                "    \n",
                "    # Import data from state_bag\n",
                "    user_message = state.machine.state_bag.get(\"user_message\", \"If you are seeing this, that means I have forgotten to add a user message. Remind me.\")\n",
                "    \n",
                "    # Execute\n",
                "    \n",
                "    response = await client.chat.completions.create(\n",
                "        model=llm_config[\"model\"],\n",
                "        messages=[{\n",
                "            \"role\":\"user\",\n",
                "            \"content\":user_message\n",
                "            }],\n",
                "        max_tokens=50,\n",
                "        stream=True\n",
                "    )\n",
                "    \n",
                "    assistant_message = \"\"\n",
                "    async def streamer():\n",
                "        nonlocal assistant_message\n",
                "        async for chunk in response:\n",
                "            chunk = chunk.choices[0].delta.content\n",
                "            if isinstance(chunk,str) and chunk:\n",
                "                assistant_message += chunk\n",
                "                yield chunk\n",
                "\n",
                "    state.machine.state_bag[\"get_assistant_message\"] = lambda: assistant_message\n",
                "    state.machine.state_bag[\"streamer\"] = streamer()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### c) Build State Machine"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "from gai.asm import AsyncStateMachine\n",
                "\n",
                "with AsyncStateMachine.StateMachineBuilder(STATE_DIAGRAM) as builder:\n",
                "    fsm = builder.build(\n",
                "        STATE_MANIFEST_V1,\n",
                "        get_llm_config=lambda state: {\n",
                "            \"client_type\": \"gai\",\n",
                "            \"model\": \"ttt\",\n",
                "            \"url\": \"http://gai-llm-svr:12031/gen/v1/chat/completions\"\n",
                "        },\n",
                "        generate=generate_action\n",
                "        )\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### d) Run State Machine (INIT->GENERATE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "A young girl named Lily discovers a mystical portal in her grandmother's attic and steps through it, finding herself in a world where time moves backwards.\n",
                        "\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "await fsm.run_async()\n",
                "async for chunk in fsm.state_bag[\"streamer\"]:\n",
                "    print(chunk,end='',flush=True)\n",
                "print(\"\\n\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### d) Continue (GENERATE->FINAL)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "State History:\n",
                        "State: INIT\n",
                        "- input: {'name': 'Sara', 'user_message': 'Write a one sentence story'}\n",
                        "- output: {'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x715ad0046020>, 'step': 0, 'time': datetime.datetime(2025, 6, 12, 18, 35, 47, 9497)}\n",
                        "--------------------\n",
                        "State: GENERATE\n",
                        "- input: {'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x715ad0046020>, 'step': 1, 'time': datetime.datetime(2025, 6, 12, 18, 35, 47, 9786), 'llm_config': {'client_type': 'gai', 'model': 'ttt', 'url': 'http://gai-llm-svr:12031/gen/v1/chat/completions'}}\n",
                        "- output: {'streamer': <async_generator object generate_action.<locals>.streamer at 0x715aaba1f340>, 'get_assistant_message': <function generate_action.<locals>.<lambda> at 0x715aaba4dab0>, 'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x715ac034a950>, 'step': 0, 'time': datetime.datetime(2025, 6, 12, 18, 35, 47, 650188)}\n",
                        "--------------------\n",
                        "State: FINAL\n",
                        "- input: {'streamer': <async_generator object generate_action.<locals>.streamer at 0x715aaba1f340>, 'get_assistant_message': <function generate_action.<locals>.<lambda> at 0x715aaba4dab0>, 'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x715ac034a950>, 'step': 0, 'time': datetime.datetime(2025, 6, 12, 18, 35, 47, 650188)}\n",
                        "- output: {'monologue': <gai.asm.monologue.Monologue object at 0x715ad0046020>}\n",
                        "--------------------\n",
                        "Assistant Message:\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "\"A young girl named Lily discovers a mystical portal in her grandmother's attic and steps through it, finding herself in a world where time moves backwards.\""
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "await fsm.run_async()\n",
                "print(\"State History:\")\n",
                "for state in fsm.state_history:\n",
                "    print(f\"State: {state['state']}\")\n",
                "    print(f\"- input: {state['input']}\")\n",
                "    print(f\"- output: {state['output']}\")\n",
                "    print(\"-\" * 20)\n",
                "print(\"Assistant Message:\")\n",
                "fsm.state_bag[\"get_assistant_message\"]()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Example: Standard Assistant (Part 2)\n",
                "\n",
                "Same example but with an additional state to demonstrate context management by monologue messages.\n",
                "\n",
                "```mermaid\n",
                "stateDiagram-v2\n",
                "direction LR\n",
                "INIT --> GENERATE\n",
                "GENERATE --> CONTINUE\n",
                "CONTINUE --> FINAL\n",
                "```\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "STATE_DIAGRAM = \"\"\"\n",
                "    INIT --> GENERATE\n",
                "    GENERATE --> CONTINUE\n",
                "    CONTINUE --> FINAL\n",
                "    \"\"\"\n",
                "    \n",
                "STATE_MANIFEST_V1 = {\n",
                "        \"INIT\": {\n",
                "            \"input_data\": {\n",
                "                \"name\": \"Sara\",\n",
                "                \"user_message\": \"Write a one sentence story\"\n",
                "            }\n",
                "        },\n",
                "        \"GENERATE\": {\n",
                "            \"module_path\": \"gai.asm.states\",\n",
                "            \"class_name\": \"PureActionState\",\n",
                "            \"title\": \"GENERATE\",\n",
                "            \"action\": \"generate_action\",\n",
                "            \"input_data\": {\n",
                "                \"llm_config\": {\n",
                "                    \"type\": \"getter\",\n",
                "                    \"dependency\": \"get_llm_config\"\n",
                "                },\n",
                "            },\n",
                "            \"output_data\": [\n",
                "                \"streamer\",\n",
                "                \"get_assistant_message\"\n",
                "            ]\n",
                "        },\n",
                "        \"CONTINUE\": {\n",
                "            \"module_path\": \"gai.asm.states\",\n",
                "            \"class_name\": \"PureActionState\",\n",
                "            \"title\": \"CONTINUE\",\n",
                "            \"action\": \"continue_action\",\n",
                "            \"input_data\": {\n",
                "                \"llm_config\": {\n",
                "                    \"type\": \"getter\",\n",
                "                    \"dependency\": \"get_llm_config\"\n",
                "                },\n",
                "            },\n",
                "            \"output_data\": [\n",
                "                \"streamer\",\n",
                "                \"get_assistant_message\"\n",
                "            ]\n",
                "        },\n",
                "        \"FINAL\": {\n",
                "            \"output_data\": [\"monologue\"]\n",
                "        }\n",
                "    }\n",
                "\n",
                "from gai.llm.openai import AsyncOpenAI\n",
                "\n",
                "async def generate_action(state):\n",
                "    \n",
                "    llm_config = state.machine.state_bag[\"llm_config\"]\n",
                "    client = AsyncOpenAI(llm_config)\n",
                "    \n",
                "    # Import data from state_bag\n",
                "    user_message = state.machine.state_bag.get(\"user_message\", \"If you are seeing this, that means I have forgotten to add a user message. Remind me.\")\n",
                "    monologue = state.machine.state_bag[\"monologue\"]\n",
                "    monologue.add_user_message(state=state,content=user_message)\n",
                "    \n",
                "    # Execute\n",
                "    \n",
                "    response = await client.chat.completions.create(\n",
                "        model=llm_config[\"model\"],\n",
                "        messages=monologue.list_chat_messages(),\n",
                "        max_tokens=50,\n",
                "        stream=True\n",
                "    )\n",
                "    \n",
                "    assistant_message = \"\"\n",
                "    async def streamer():\n",
                "        nonlocal assistant_message\n",
                "        async for chunk in response:\n",
                "            chunk = chunk.choices[0].delta.content\n",
                "            if isinstance(chunk,str) and chunk:\n",
                "                assistant_message += chunk\n",
                "                yield chunk\n",
                "        monologue.add_assistant_message(state=state,content=assistant_message)\n",
                "    state.machine.state_bag[\"monologue\"] = monologue\n",
                "    state.machine.state_bag[\"get_assistant_message\"] = lambda: assistant_message\n",
                "    state.machine.state_bag[\"streamer\"] = streamer()\n",
                "\n",
                "async def continue_action(state):\n",
                "    \n",
                "    llm_config = state.machine.state_bag[\"llm_config\"]\n",
                "    client = AsyncOpenAI(llm_config)\n",
                "    \n",
                "    # Import data from state_bag\n",
                "    monologue = state.machine.state_bag[\"monologue\"]\n",
                "    monologue.add_user_message(state=state,content=\"Please continue.\")\n",
                "    \n",
                "    # Execute\n",
                "    \n",
                "    response = await client.chat.completions.create(\n",
                "        model=llm_config[\"model\"],\n",
                "        messages=monologue.list_chat_messages(),\n",
                "        max_tokens=50,\n",
                "        stream=True\n",
                "    )\n",
                "    \n",
                "    assistant_message = \"\"\n",
                "    async def streamer():\n",
                "        nonlocal assistant_message\n",
                "        async for chunk in response:\n",
                "            chunk = chunk.choices[0].delta.content\n",
                "            if isinstance(chunk,str) and chunk:\n",
                "                assistant_message += chunk\n",
                "                yield chunk\n",
                "        monologue.add_assistant_message(state=state,content=assistant_message)\n",
                "    state.machine.state_bag[\"monologue\"] = monologue\n",
                "    state.machine.state_bag[\"get_assistant_message\"] = lambda: assistant_message\n",
                "    state.machine.state_bag[\"streamer\"] = streamer()\n",
                "    \n",
                "from gai.asm import AsyncStateMachine\n",
                "\n",
                "with AsyncStateMachine.StateMachineBuilder(STATE_DIAGRAM) as builder:\n",
                "    fsm = builder.build(\n",
                "        STATE_MANIFEST_V1,\n",
                "        get_llm_config=lambda state: {\n",
                "            \"client_type\": \"gai\",\n",
                "            \"name\": \"dolphin_llama:exl2\",\n",
                "            \"model\": \"ttt\",\n",
                "            \"url\": \"http://gai-llm-svr:12031/gen/v1/chat/completions\"\n",
                "        },\n",
                "        generate_action=generate_action,\n",
                "        continue_action=continue_action\n",
                "        )\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### d) Run State Machine (INIT->GENERATE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "A lone traveler sat on the edge of the world, watching a crimson sun sink slowly into the vast ocean, painting the sky with strokes of gold and purple, as a gentle breeze whispered ancient secrets of forgotten lands.\n",
                        "\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "await fsm.run_async()\n",
                "async for chunk in fsm.state_bag[\"streamer\"]:\n",
                "    print(chunk,end='',flush=True)\n",
                "print(\"\\n\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### c) Run State Machine (GENERATE->CONTINUE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The traveler, a young woman with eyes like the sea and hair that danced in the wind, closed her eyes and let the colors and whispers wash over her, carrying her back to memories of childhood tales told by her grandmother, a wise woman who had sailed\n",
                        "\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "await fsm.run_async()\n",
                "async for chunk in fsm.state_bag[\"streamer\"]:\n",
                "    print(chunk,end='',flush=True)\n",
                "print(\"\\n\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### e) END (GENERATE->FINAL)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "State History:\n",
                        "State: INIT\n",
                        "- input: {'name': 'Sara', 'user_message': 'Write a one sentence story'}\n",
                        "- output: {'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x715aab10f1f0>, 'step': 0, 'time': datetime.datetime(2025, 6, 12, 18, 36, 8, 16780)}\n",
                        "--------------------\n",
                        "State: GENERATE\n",
                        "- input: {'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x715aab10f1f0>, 'step': 1, 'time': datetime.datetime(2025, 6, 12, 18, 36, 8, 17159), 'llm_config': {'client_type': 'gai', 'name': 'dolphin_llama:exl2', 'model': 'ttt', 'url': 'http://gai-llm-svr:12031/gen/v1/chat/completions'}}\n",
                        "- output: {'streamer': <async_generator object generate_action.<locals>.streamer at 0x715aaba89d40>, 'get_assistant_message': <function generate_action.<locals>.<lambda> at 0x715aababedd0>, 'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x715aab10cbb0>, 'step': 0, 'time': datetime.datetime(2025, 6, 12, 18, 36, 8, 39524)}\n",
                        "--------------------\n",
                        "State: CONTINUE\n",
                        "- input: {'streamer': <async_generator object generate_action.<locals>.streamer at 0x715aaba89d40>, 'get_assistant_message': <function generate_action.<locals>.<lambda> at 0x715aababedd0>, 'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x715aab10cbb0>, 'step': 1, 'time': datetime.datetime(2025, 6, 12, 18, 36, 13, 521297), 'llm_config': {'client_type': 'gai', 'name': 'dolphin_llama:exl2', 'model': 'ttt', 'url': 'http://gai-llm-svr:12031/gen/v1/chat/completions'}}\n",
                        "- output: {'streamer': <async_generator object continue_action.<locals>.streamer at 0x715aaba89ec0>, 'get_assistant_message': <function continue_action.<locals>.<lambda> at 0x715aababec20>, 'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x715aab10c550>, 'step': 0, 'time': datetime.datetime(2025, 6, 12, 18, 36, 13, 542955)}\n",
                        "--------------------\n",
                        "State: FINAL\n",
                        "- input: {'streamer': <async_generator object continue_action.<locals>.streamer at 0x715aaba89ec0>, 'get_assistant_message': <function continue_action.<locals>.<lambda> at 0x715aababec20>, 'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x715aab10c550>, 'step': 0, 'time': datetime.datetime(2025, 6, 12, 18, 36, 13, 542955)}\n",
                        "- output: {'monologue': <gai.asm.monologue.Monologue object at 0x715aab10f1f0>}\n",
                        "--------------------\n",
                        "Assistant Message:\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'The traveler, a young woman with eyes like the sea and hair that danced in the wind, closed her eyes and let the colors and whispers wash over her, carrying her back to memories of childhood tales told by her grandmother, a wise woman who had sailed'"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "await fsm.run_async()\n",
                "print(\"State History:\")\n",
                "for state in fsm.state_history:\n",
                "    print(f\"State: {state['state']}\")\n",
                "    print(f\"- input: {state['input']}\")\n",
                "    print(f\"- output: {state['output']}\")\n",
                "    print(\"-\" * 20)\n",
                "print(\"Assistant Message:\")\n",
                "fsm.state_bag[\"get_assistant_message\"]()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

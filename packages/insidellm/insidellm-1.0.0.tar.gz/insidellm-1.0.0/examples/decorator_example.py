import logging
import uuid
import os
import insidellm
from insidellm import InsideLLMClient, track_llm_call

# --- Basic Setup ---
# Configure logging for demonstration purposes
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize the InsideLLMClient
# Replace with your actual API key and endpoint if not using environment variables
insidellm.initialize(
            api_key=os.getenv("INSIDELLM_API_KEY", "iilmn-sample-key"),
            local_testing= True,
            auto_flush_interval=10.0,  # Flush every 10 seconds
            batch_size=20,
            max_retries=2,
            raise_on_error=False  # Don't raise on network errors for demo
        )
client = insidellm.get_client()

# Start a new trace run
# It's good practice to use a unique run_id, e.g., generated by uuid
run_id = str(uuid.uuid4())
run_id = client.start_run(
        run_id=run_id,
        user_id="basic-demo-user",
        metadata={
            "session_type": "demo",
            "environment": "development"
        }
)
#client.start_run(run_id=run_id)

logger.info(f"Started run with ID: {run_id}")


# The @track_llm_call decorator will automatically log LLM_REQUEST and LLM_RESPONSE events.
# We specify `model_name` and `provider` which are important metadata for LLM calls.
@track_llm_call('gpt-4', 'openai')
def example_llm_call_function(prompt: str, temperature: float = 0.7):
    """
    A function that simulates a call to an LLM.
    Its execution, including prompt and response, will be tracked by InsideLLM.
    This will result in an LLM_REQUEST event before the function body executes,
    and an LLM_RESPONSE event after it completes, associated with "mock-llm".
    """
    logger.info(
        f"Inside example_llm_call_function with prompt: '{prompt}', temperature: {temperature}"
    )
    # Simulate LLM processing:
    # In a real scenario, this is where you would interact with an actual LLM API.
    # The decorator handles capturing the prompt from the arguments.
    mock_response = f"This is a mock LLM response to your prompt: '{prompt}' (temp: {temperature})"
    logger.info(f"Simulated LLM response: '{mock_response}'")

    # The decorator will automatically capture the returned string as the LLM's output.
    # If you need to return more complex data or metadata (like token counts, cost),
    # the decorated function can return a dictionary with specific keys
    # (e.g., 'output', 'input_tokens', 'output_tokens', 'cost').
    # For this example, a simple string response is sufficient.
    return mock_response


if __name__ == "__main__":
    logger.info("Running decorator examples...")

    # --- Demonstrating @track_function_execution ---
    logger.info("\n--- Demonstrating @track_function_execution ---")
    # Calling the tracked function.
    # InsideLLM will log an event for this function call under the name 'my_custom_string_concatenator'.
    #tracked_result = example_tracked_function("Hello", "Decorator")
    #logger.info(f"Call to example_tracked_function completed. Result: '{tracked_result}'")
    #logger.info("An event for 'my_custom_string_concatenator' should have been logged by InsideLLM.")


    # Example usage of the LLM call tracked function
    # --- Demonstrating @track_llm_call ---
    logger.info("\n--- Demonstrating @track_llm_call ---")
    llm_prompt = "Explain the theory of relativity in simple terms."
    # Calling the LLM tracked function.
    # InsideLLM will log LLM_REQUEST and LLM_RESPONSE events for this call.
    llm_response = example_llm_call_function(llm_prompt, temperature=0.5)
    logger.info(f"Call to example_llm_call_function completed. Response: '{llm_response}'")
    logger.info("LLM_REQUEST and LLM_RESPONSE events for 'mock-llm' (provider: 'mock_provider') should have been logged by InsideLLM.")

    logger.info("\nDecorator examples finished.")
    # The run will be automatically ended when the client is garbage collected
    # or can be explicitly ended with client.end_run()
    # It's good practice to explicitly end the run.
    client.end_run()
    logger.info(f"Run {run_id} ended.")
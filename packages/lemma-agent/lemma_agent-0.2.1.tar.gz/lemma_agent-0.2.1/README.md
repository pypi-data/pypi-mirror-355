# Lemma ğŸš€

**AI-Powered Debugging and Optimization for AI Agents**

Transform AI agent development from trial-and-error debugging to intelligent, automated problem detection and resolution.

 
## ğŸ¯ Quick Start

Install Lemma:

```bash
pip install lemma
```

Add one line to your AI agent:

```python
from lemma import smart_debug

@smart_debug(project_id="my-agent")
class CustomerSupportAgent:
    def handle_query(self, query):
        # Your existing agent logic
        return self.chain.run(query)
```

That's it! ğŸ‰ Your agent now has:

- âœ… **Intelligent error analysis**
- âœ… **Performance monitoring**
- âœ… **Cost tracking**
- âœ… **Fix suggestions**

## ğŸ”¥ Key Features

### ğŸ§  **AI-Powered Debugging**

- **Root Cause Analysis**: "Agent failed because it's stuck in a clarification loop"
- **Auto-Fix Generation**: AI generates actual code fixes with 90%+ accuracy
- **Pattern Recognition**: Detects infinite loops, context overflows, tool calling errors

### âš¡ **Zero-Config Integration**

- **One Decorator**: `@smart_debug` instantly adds debugging to any function/class
- **Framework Agnostic**: Works with LangChain, CrewAI, AutoGen, or custom frameworks
- **No Code Changes**: Your agent logic remains completely unchanged

### ğŸ’° **Cost & Performance Optimization**

- **LLM Cost Tracking**: Track every API call with precise cost calculations
- **Performance Monitoring**: Identify bottlenecks and optimization opportunities
- **Resource Optimization**: Get recommendations to reduce costs by 40%+

### ğŸ” **Production-Ready Monitoring**

- **Real-time Alerts**: Get notified when agents start failing
- **Performance Dashboards**: Track success rates, response times, costs
- **Team Collaboration**: Share debugging insights across your team

## ğŸš€ Framework Support

Lemma works seamlessly with all major AI agent frameworks:

### LangChain

```python
from langchain.agents import AgentExecutor
from lemma import smart_debug

@smart_debug(project_id="langchain-agent")
agent = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools)
```

### CrewAI

```python
from crewai import Crew
from lemma import smart_debug

@smart_debug(project_id="crew-agents")
crew = Crew(agents=agents, tasks=tasks)
```

### AutoGen

```python
from autogen import ConversableAgent
from lemma import smart_debug

@smart_debug(project_id="autogen-chat")
agent = ConversableAgent(name="assistant")
```

### Custom Frameworks

```python
# Works with any Python function or class
@smart_debug(project_id="custom-agent")
def my_custom_agent(user_input):
    # Your custom agent logic
    return response
```

## ğŸ“Š Real-World Impact

### Before Lemma

- ğŸ˜© **2+ hours** debugging a single agent failure
- ğŸ” **Trial-and-error** development with print statements
- ğŸ’¸ **Hidden costs** from inefficient LLM usage
- ğŸš« **No visibility** into why agents fail

### After Lemma

- âš¡ **30 seconds** to identify and fix failures
- ğŸ§  **AI-powered insights** with specific fix suggestions
- ğŸ’° **40% cost reduction** through optimization recommendations
- ğŸ“ˆ **67% faster** development cycles

## ğŸ’¡ Example: Debug Session

```python
from lemma import smart_debug

@smart_debug(project_id="support-bot", auto_fix=True)
class SupportBot:
    def handle_customer_issue(self, issue):
        # Agent gets stuck in a loop...
        return self.resolve_issue(issue)

# Lemma automatically detects:
```

**AI Analysis Output:**

```
ğŸš¨ Issue Detected: Infinite clarification loop
ğŸ“‹ Root Cause: Agent asking for order number repeatedly 
   because conversation memory isn't checked
ğŸ”§ Confidence: 94%

ğŸ’¡ Auto-Generated Fix:
   1. Add conversation memory check before asking questions
   2. Extract order number from previous messages  
   3. Implement max_clarification_attempts = 2
   
âš¡ Expected Impact: +67% success rate, -23% cost
```

## ğŸ› ï¸ Configuration Options

```python
@smart_debug(
    project_id="my-agent",           # Project identifier
    environment="production",        # Environment tag
    trace_level="detailed",          # basic | detailed | verbose
    auto_fix=True,                  # Enable auto-fix suggestions
    cost_tracking=True,             # Track LLM API costs
    performance_monitoring=True,     # Monitor execution performance
    team_sharing=True,              # Share insights with team
    alert_thresholds={              # Custom alert thresholds
        "error_rate": 0.05,         # Alert if >5% error rate
        "response_time": 2.0,       # Alert if >2s response time
        "cost_per_request": 0.10    # Alert if >$0.10 per request
    }
)
```

## ğŸ“ˆ Pricing

### ğŸ†“ **Free Tier**

- âœ… Local debugging and basic insights
- âœ… Framework adapters (LangChain, CrewAI, AutoGen)
- âœ… Performance monitoring
- âœ… VSCode extension
- âŒ AI-powered analysis (limited)
- âŒ Team collaboration
- âŒ Advanced optimization

### ğŸ’ **Pro Tier - $29/month**

- âœ… **Everything in Free**
- âœ… **AI-powered root cause analysis**
- âœ… **Auto-fix generation**
- âœ… **Team collaboration and sharing**
- âœ… **Advanced optimization

import math
import cv2
import os
import numpy as np
from numpy.core.defchararray import array
from tensorflow.python.eager.context import disable_graph_collection
import pkg_resources
import pickle # pickle ëª¨ë“ˆ ì„í¬íŠ¸
from collections import Counter # Counter ëª¨ë“ˆ ì„í¬íŠ¸

class SketchProcessor:

    # í•™ìŠµ ë°ì´í„° ì €ì¥ íŒŒì¼ ì´ë¦„
    # ì´ íŒŒì¼ì€ self.sketchPath ë‚´ë¶€ì— ì €ì¥ë©ë‹ˆë‹¤.
    TRAINING_DATA_FILE = "sketch_training_data.pkl"

    def __init__(self) -> None:

        self.orbDetector = cv2.ORB_create()
        self.matcherHamming = cv2.DescriptorMatcher_create(cv2.DescriptorMatcher_BRUTEFORCE_HAMMINGLUT)

        self.sketchPath = pkg_resources.resource_filename(__package__,"res/sketch/")
        if os.path.exists(self.sketchPath) is False:
            os.makedirs(self.sketchPath)

        # í•™ìŠµëœ ìŠ¤ì¼€ì¹˜ ë°ì´í„°ë¥¼ ì €ì¥í•  ë³€ìˆ˜ ì´ˆê¸°í™”
        self.orbDescriptors = []
        self.nameIndexList = []
        self.nameIntList = []

        # ì‹¤ì‹œê°„ í•™ìŠµì„ ìœ„í•´ ìº¡ì²˜ëœ íŠ¹ì§•ì ê³¼ ì´ë¦„ì„ ì„ì‹œë¡œ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
        self.captured_descriptors = []
        self.captured_names = []
        self.unique_names_map = {} # ê³ ìœ  ì´ë¦„ì— ëŒ€í•œ ì¸ë±ìŠ¤ ë§µ
        self.current_unique_idx = 0

        # -----------------------------------------------------------------------
        # ì¶”ê°€: ê°ì²´ ì´ˆê¸°í™” ì‹œ í•™ìŠµ ë°ì´í„° ë¡œë“œ ì‹œë„
        self.load_training_data()
        # -----------------------------------------------------------------------

    def __call__(self, image):
        retName = np.array([])
        retRect = np.empty((1,4,2), dtype=int)
        retConfidence = np.array([]) # ì‹ ë¢°ë„ ë°˜í™˜ì„ ìœ„í•œ ë°°ì—´ ì¶”ê°€

        h,w,c = image.shape
        sketchImage = None

        kernel = np.ones((3,3), np.uint8)

        processedImg = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        processedImg = cv2.pyrDown(processedImg)
        processedImg = cv2.pyrUp(processedImg)
        processedImg = cv2.Canny(processedImg,0,100)
        processedImg = cv2.dilate(processedImg, kernel, anchor=(-1,1), iterations=1)

        contours, hierarchy = cv2.findContours(processedImg, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)[-2:]

        for contour in contours:
            area = cv2.contourArea(contour)

            if area < 2500 or area > 75000:
                continue

            approx = cv2.approxPolyDP(contour, cv2.arcLength(contour, True) * 0.02, True)
            edge = len(approx)

            if edge == 4 and cv2.isContourConvex(approx):
                approx = approx.reshape (4,2)
                src_pts = np.array([ approx[1],approx[0],approx[2],approx[3] ], dtype=np.float32)
                dst_pts = np.array([[0,0],[w,0],[0,h],[w,h]], dtype=np.float32)
                M = cv2.getPerspectiveTransform(src_pts, dst_pts)
                sketchImage = cv2.warpPerspective(processedImg, M, (w,h))
                retRect = np.append(retRect, np.array([approx]), axis=0)
                break

        if sketchImage is not None:
            if len(self.matcherHamming.getTrainDescriptors()) > 0:
                sketchImage = cv2.resize(sketchImage, (150,150))
                _, des = self.orbDetector.detectAndCompute(sketchImage, None)

                #idx = self.__checkMatches(des)
                idx, confidence = self.__checkMatches(des) # ì‹ ë¢°ë„ ê°’ë„ í•¨ê»˜ ë°›ì•„ì˜´
                if idx != -1:
                    retName = np.append(retName, np.array([self.nameIndexList[idx]]), axis=0)
                    retConfidence = np.append(retConfidence, np.array([confidence]), axis=0) # ì‹ ë¢°ë„ ì¶”ê°€
                else:
                    retName = np.append(retName, np.array(['Sketch']), axis=0)
                    retConfidence = np.append(retConfidence, np.array([0.0]), axis=0) # ì¸ì‹ ì‹¤íŒ¨ ì‹œ ì‹ ë¢°ë„ 0

            else:
                retName = np.append(retName, np.array(['Sketch']), axis=0)
                retConfidence = np.append(retConfidence, np.array([0.0]), axis=0) # í•™ìŠµëœ ë°ì´í„° ì—†ì„ ì‹œ ì‹ ë¢°ë„ 0

        retRect = np.delete(retRect, [0, 0], axis=0)
        #return retName, retRect
        return retName, retRect, retConfidence # ì‹ ë¢°ë„ë„ ë°˜í™˜í•˜ë„ë¡ ë³€ê²½

    # ìƒˆë¡­ê²Œ ì¶”ê°€ë˜ê±°ë‚˜ ë³€ê²½ëœ ë©”ì„œë“œ
    # ----------------------------------------------------------------------------------

    def _capture_and_process_image(self, image):
        """
        ì£¼ì–´ì§„ ì´ë¯¸ì§€ì—ì„œ ìŠ¤ì¼€ì¹˜ë¥¼ ìº¡ì²˜í•˜ê³  ì „ì²˜ë¦¬í•˜ì—¬ íŠ¹ì§•ì ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        SaveSketchì™€ __call__ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì´ë¯¸ì§€ ì²˜ë¦¬ ë¡œì§ì„ ìº¡ìŠí™”í•©ë‹ˆë‹¤.
        """

        h,w,c = image.shape
        sketchImage = None

        kernel = np.ones((3,3), np.uint8)

        processedImg = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        processedImg = cv2.pyrDown(processedImg)
        processedImg = cv2.pyrUp(processedImg)
        processedImg = cv2.Canny(processedImg,0,100)
        processedImg = cv2.dilate(processedImg, kernel, anchor=(-1,1), iterations=1)

        contours, hierarchy = cv2.findContours(processedImg, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)[-2:]

        for contour in contours:
            area = abs( cv2.contourArea(contour) )

            if area < 2500 or area > 75000:
                continue

            approx = cv2.approxPolyDP(contour, cv2.arcLength(contour, True) * 0.02, True)
            edge = len(approx)

            if edge == 4 and cv2.isContourConvex(approx):
                approx = approx.reshape (4,2)
                if approx[1][1] < approx[3][1]:
                    src_pts = np.array([ approx[1],approx[0],approx[2],approx[3] ], dtype=np.float32)
                else:
                    src_pts = np.array([ approx[0],approx[3],approx[1],approx[2] ], dtype=np.float32)

                dst_pts = np.array([[0,0],[w,0],[0,h],[w,h]], dtype=np.float32)
                M = cv2.getPerspectiveTransform(src_pts, dst_pts)
                sketchImage = cv2.warpPerspective(processedImg, M, (w,h))

                return sketchImage

        #return sketchImage
        return None # ìŠ¤ì¼€ì¹˜ë¥¼ ì°¾ì§€ ëª»í–ˆìœ¼ë©´ None ë°˜í™˜

    def add_sketch_for_training(self, image, name: str):
        """
        ìƒˆë¡œìš´ ìŠ¤ì¼€ì¹˜ ì´ë¯¸ì§€ë¥¼ í•™ìŠµ ë°ì´í„°ì…‹ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        """
        if not name:
            print("ì´ë¦„ íŒŒë¼ë¯¸í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            return -1

        processed_sketch = self._capture_and_process_image(image)

        if processed_sketch is not None:
            resized_sketch = cv2.resize(processed_sketch, (150, 150))

            # íŠ¹ì§•ì  ì¶”ì¶œ
            _, des = self.orbDetector.detectAndCompute(resized_sketch, None)

            if des is not None:
                self.captured_descriptors.append(des)
                self.captured_names.append(name)

                # ê³ ìœ  ì´ë¦„ ì¸ë±ì‹± ì—…ë°ì´íŠ¸
                if name not in self.unique_names_map:
                    self.unique_names_map[name] = self.current_unique_idx
                    self.current_unique_idx += 1

                # nameIntListëŠ” TrainModelì—ì„œ ì¼ê´„ ìƒì„±

                # ë³€ê²½ëœ ë¶€ë¶„: ì´ë¦„ë³„ ì¶”ê°€ëœ ìŠ¤ì¼€ì¹˜ ìˆ˜ ì¶œë ¥
                name_counts = Counter(self.captured_names)
                print(f"'{name}' ìŠ¤ì¼€ì¹˜ ë°ì´í„° {name_counts[name]}ê°œ í•™ìŠµ ì™„ë£Œ.")
                #print(f"âœ”ï¸ '{name}' ìŠ¤ì¼€ì¹˜ ì¶”ê°€ë¨.")
                #print(f" Â  - í˜„ì¬ '{name}' ìŠ¤ì¼€ì¹˜ ê°œìˆ˜: {name_counts[name]}ê°œ")


                #print(f" Â  - ì „ì²´ í•™ìŠµ ëŒ€ê¸° ìŠ¤ì¼€ì¹˜ ìˆ˜: {len(self.captured_descriptors)}ê°œ")
                return 0 # ì„±ê³µ
            else:
                print(f"ê²½ê³ : '{name}' ìŠ¤ì¼€ì¹˜ì—ì„œ íŠ¹ì§•ì ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return -1
        else:
            print("ìŠ¤ì¼€ì¹˜ ì´ë¯¸ì§€ì—ì„œ ìœ íš¨í•œ ì‚¬ê°í˜•ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return -1


    def train_from_captured_data(self):
        """
        í˜„ì¬ê¹Œì§€ ìº¡ì²˜ëœ ìŠ¤ì¼€ì¹˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
        """
        if not self.captured_descriptors:
            print("í•™ìŠµí•  ìŠ¤ì¼€ì¹˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # nameIntListë¥¼ `self.captured_names`ì™€ `self.unique_names_map`ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
        nameIntList = [self.unique_names_map[name] for name in self.captured_names]

        # SketchProcessor ë‚´ë¶€ì˜ TrainModel ë©”ì„œë“œ í˜¸ì¶œ
        self.TrainModel(self.captured_names, nameIntList, self.captured_descriptors)
        #print(f"ì „ì²´ ìŠ¤ì¼€ì¹˜ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ. í•™ìŠµëœ ìŠ¤ì¼€ì¹˜ ê°œìˆ˜: {len(self.captured_descriptors)}")
        # -----------------------------------------------------------------------
        # ì¶”ê°€: í•™ìŠµì´ ì™„ë£Œë˜ë©´ ë°ì´í„° ì €ì¥
        self.save_training_data()
        # -----------------------------------------------------------------------

    # ----------------------------------------------------------------------------------

    def TrainModel(self, nameIndexList:list, nameIntList:list, orbDescriptors:list):
        """
        ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ.
        ë§¤ì¹­ê¸°ì— íŠ¹ì§•ì  ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ê³  í•™ìŠµì‹œí‚µë‹ˆë‹¤.
        """
        self.orbDescriptors = orbDescriptors.copy()
        self.nameIndexList = nameIndexList.copy()
        self.nameIntList = nameIntList.copy()
        self.matcherHamming.clear() # ê¸°ì¡´ í›ˆë ¨ ë°ì´í„° í´ë¦¬ì–´

        if len(self.orbDescriptors) > 0:
            self.matcherHamming.add(self.orbDescriptors)
            self.matcherHamming.train()
            #print("ë§¤ì¹­ê¸° í•™ìŠµ ì™„ë£Œ.")
        else:
            print("í•™ìŠµí•  íŠ¹ì§•ì  ë°ì´í„°ê°€ ì—†ì–´ í•™ìŠµí• ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    def __checkMatches(self, descriptor):
        matchIdx = -1
        best_confidence = 0.0

        if len(self.matcherHamming.getTrainDescriptors()) == 0:
            return -1, 0.0

        if descriptor is None or len(descriptor) == 0:
            return -1, 0.0

        # ë§¤ì¹­ ì„ê³„ê°’. ì´ ê°’ì„ ì¡°ì ˆí•˜ì—¬ ë§¤ì¹­ì˜ ì—„ê²©ë„ë¥¼ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ORB/Hamming ê±°ë¦¬ì—ì„œ 50ì€ ë¹„êµì  ê´€ëŒ€í•œ í¸ì…ë‹ˆë‹¤.
        # ë” ì—„ê²©í•˜ê²Œ í•˜ë ¤ë©´ ê°’ì„ ë‚®ì¶”ì„¸ìš” (ì˜ˆ: 40, 35)
        MATCH_DISTANCE_THRESHOLD = 45 # ì´ì „ ë…¼ì˜ì—ì„œ ì œì•ˆëœ ê°’ ì‚¬ìš© (ì¡°ì • ê°€ëŠ¥)

        for idx, trainDescriptor in enumerate(self.matcherHamming.getTrainDescriptors()):
            if trainDescriptor is None or len(trainDescriptor) == 0:
                continue

            matches = self.matcherHamming.match(descriptor, trainDescriptor)

            if not matches:
                continue

            # ì¢‹ì€ ë§¤ì¹­ì˜ ê°œìˆ˜ë¥¼ ì…‰ë‹ˆë‹¤.
            good_matches_count = sum(1 for dMatch in matches if dMatch.distance <= MATCH_DISTANCE_THRESHOLD)

            # ì‹ ë¢°ë„ ê³„ì‚°:
            # ì¿¼ë¦¬ ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì  ìˆ˜ì™€ í•™ìŠµëœ ëª¨ë¸ì˜ íŠ¹ì§•ì  ìˆ˜ ì¤‘ ë” ì‘ì€ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ í•©ë‹ˆë‹¤.
            # ì´ë ‡ê²Œ í•˜ë©´ íŠ¹ì§•ì  ìˆ˜ê°€ í¬ê²Œ ë‹¤ë¥¸ ê²½ìš°ì—ë„ ì‹ ë¢°ë„ê°€ í•©ë¦¬ì ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.
            max_possible_matches = min(len(descriptor), len(trainDescriptor))

            current_confidence = 0.0
            if max_possible_matches > 0:
                # 0.0 ~ 1.0 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
                current_confidence = good_matches_count / max_possible_matches
                # ì†Œìˆ˜ì  ë‘ ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼
                current_confidence = round(current_confidence, 2)

            # í˜„ì¬ê¹Œì§€ì˜ ìµœê³  ì‹ ë¢°ë„ë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤.
            if current_confidence > best_confidence:
                best_confidence = current_confidence
                matchIdx = idx

        return matchIdx, best_confidence

    def __angle(self, pt1:array, pt2:array, pt0:array):
        # ì´ í•¨ìˆ˜ëŠ” í˜„ì¬ ìŠ¤ì¼€ì¹˜ ì¸ì‹ ë¡œì§ì—ì„œ ì§ì ‘ ì‚¬ìš©ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
        abx1 = pt1[0] - pt0[0]
        aby1 = pt1[1] - pt0[1]
        cbx2 = pt2[0] - pt0[0]
        cby2 = pt2[1] - pt0[1]

        dot = abx1*cbx2 + aby1*cby2
        cross = abx1*cby2 - aby1*cbx2

        alpha = math.atan2(cross,dot)

        return int(math.floor( alpha * 180.0) / 3.1415926535897932384626433832795 + 0.5)

 # -----------------------------------------------------------------------
    # ì¶”ê°€: í•™ìŠµ ë°ì´í„° ì €ì¥ ë° ë¡œë“œ ë©”ì„œë“œ

    def save_training_data(self):
        """
        í˜„ì¬ í•™ìŠµëœ orbDescriptors, nameIndexList, nameIntList,
        ê·¸ë¦¬ê³  captured_descriptors, captured_names, unique_names_mapì„ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        """
        data_to_save = {
            'orbDescriptors': self.orbDescriptors,
            'nameIndexList': self.nameIndexList,
            'nameIntList': self.nameIntList,
            'captured_descriptors': self.captured_descriptors, # ì‹¤ì‹œê°„ í•™ìŠµì„ ìœ„í•´ ìº¡ì²˜ëœ ë°ì´í„°ë„ ì €ì¥
            'captured_names': self.captured_names,             # ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì´ì–´ì„œ í•™ìŠµ ê°€ëŠ¥
            'unique_names_map': self.unique_names_map,
            'current_unique_idx': self.current_unique_idx
        }

        file_path = os.path.join(self.sketchPath, self.TRAINING_DATA_FILE)

        try:
            with open(file_path, 'wb') as f:
                pickle.dump(data_to_save, f)
            print(f"í•™ìŠµ ë°ì´í„°ê°€ '{file_path}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"í•™ìŠµ ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def load_training_data(self):
        file_path = os.path.join(self.sketchPath, self.TRAINING_DATA_FILE)

        if os.path.exists(file_path):
            try:
                with open(file_path, 'rb') as f:
                    loaded_data = pickle.load(f)

                self.orbDescriptors = loaded_data.get('orbDescriptors', [])
                self.nameIndexList = loaded_data.get('nameIndexList', [])
                self.nameIntList = loaded_data.get('nameIntList', [])
                self.captured_descriptors = loaded_data.get('captured_descriptors', [])
                self.captured_names = loaded_data.get('captured_names', [])
                self.unique_names_map = loaded_data.get('unique_names_map', {})
                self.current_unique_idx = loaded_data.get('current_unique_idx', 0)

                if self.orbDescriptors:
                    self.matcherHamming.clear()
                    self.matcherHamming.add(self.orbDescriptors)
                    self.matcherHamming.train()

                    # -------------------------------------------------------------------
                    # ë³€ê²½: ë¡œë“œëœ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì •ë³´ ì¶œë ¥
                    #print(f"âœ”ï¸ '{file_path}'ì—ì„œ í•™ìŠµëœ ìŠ¤ì¼€ì¹˜ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ.")
                    #print(f"   - ì´ í•™ìŠµëœ ìŠ¤ì¼€ì¹˜ ê°œìˆ˜: {len(self.orbDescriptors)}ê°œ")

                    # ê° ìŠ¤ì¼€ì¹˜ ì´ë¦„ë³„ ê°œìˆ˜ë¥¼ ì„¸ì–´ ì¶œë ¥
                    if self.nameIndexList:
                        name_counts = Counter(self.nameIndexList)
                        print(f"âœ”ï¸ '{file_path}'ì—ì„œ í•™ìŠµëœ ìŠ¤ì¼€ì¹˜ ë°ì´í„° {len(name_counts.items())}ê°œ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ.")
                        #print(f"   - í•™ìŠµëœ ìŠ¤ì¼€ì¹˜ ê°œìˆ˜:")
                        for name, count in sorted(name_counts.items()): # ì´ë¦„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¶œë ¥
                            print(f"     - {name}: {count}ê°œ")
                    else:
                        print(f"   - í•™ìŠµëœ ìŠ¤ì¼€ì¹˜ ì¢…ë¥˜: ì—†ìŒ")
                    # -------------------------------------------------------------------
                else:
                    print("âš ï¸ ë¡œë“œëœ í•™ìŠµ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë§¤ì¹­ê¸°ë¥¼ í•™ìŠµí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            except Exception as e:
                print(f"âŒ í•™ìŠµ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                self.orbDescriptors = []
                self.nameIndexList = []
                self.nameIntList = []
                self.captured_descriptors = []
                self.captured_names = []
                self.unique_names_map = {}
                self.current_unique_idx = 0
        else:
            print(f"â„¹ï¸ ì €ì¥ëœ í•™ìŠµ ë°ì´í„° íŒŒì¼ '{file_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    ## New Deletion Functions

    def delete_model_by_name(self, name: str):
        """
        íŠ¹ì • ì´ë¦„ì„ ê°€ì§„ ìŠ¤ì¼€ì¹˜ ëª¨ë¸ì˜ ëª¨ë“  í•™ìŠµ ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
        ì‚­ì œ í›„ ë§¤ì¹­ê¸°ë¥¼ ì¬í•™ìŠµí•˜ê³  ë³€ê²½ì‚¬í•­ì„ ì €ì¥í•©ë‹ˆë‹¤.
        """
        if not name:
            print("ğŸš« ì‚­ì œí•  ëª¨ë¸ì˜ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return

        initial_total_descriptors = len(self.orbDescriptors) + len(self.captured_descriptors)

        # 1. captured_data (í•™ìŠµ ëŒ€ê¸° ì¤‘ì¸ ë°ì´í„°)ì—ì„œ ì‚­ì œ
        new_captured_descriptors = []
        new_captured_names = []
        deleted_from_captured = 0
        for i in range(len(self.captured_names)):
            if self.captured_names[i] != name:
                new_captured_descriptors.append(self.captured_descriptors[i])
                new_captured_names.append(self.captured_names[i])
            else:
                deleted_from_captured += 1

        self.captured_descriptors = new_captured_descriptors
        self.captured_names = new_captured_names

        # 2. orbDescriptors (ì´ë¯¸ í•™ìŠµëœ ë°ì´í„°)ì—ì„œ ì‚­ì œ
        new_orbDescriptors = []
        new_nameIndexList = []
        new_nameIntList = []
        deleted_from_trained = 0

        # unique_names_mapì—ì„œ í•´ë‹¹ ì´ë¦„ì„ ì œê±°í•˜ê³  ë‹¤ì‹œ ë§¤í•‘í•©ë‹ˆë‹¤.
        # ì´ì „ì— ë§¤í•‘ëœ ì¸ë±ìŠ¤ê°€ ë³€ê²½ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, nameIntListë¥¼ ë‹¤ì‹œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
        temp_unique_names = set(self.nameIndexList) - {name}
        temp_unique_names.update(set(self.captured_names)) # captured_namesì—ë„ ë‚¨ì€ ì´ë¦„ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¶”ê°€

        self.unique_names_map = {n: i for i, n in enumerate(sorted(list(temp_unique_names)))}
        self.current_unique_idx = len(self.unique_names_map)

        for i in range(len(self.nameIndexList)):
            if self.nameIndexList[i] != name:
                new_orbDescriptors.append(self.orbDescriptors[i])
                new_nameIndexList.append(self.nameIndexList[i])
                new_nameIntList.append(self.unique_names_map[self.nameIndexList[i]]) # ìƒˆ ì¸ë±ìŠ¤ ì ìš©
            else:
                deleted_from_trained += 1

        self.orbDescriptors = new_orbDescriptors
        self.nameIndexList = new_nameIndexList
        self.nameIntList = new_nameIntList

        total_deleted = deleted_from_captured + deleted_from_trained

        if total_deleted > 0:
            print(f"ğŸ—‘ï¸ ëª¨ë¸ '{name}'ì— ëŒ€í•œ í•™ìŠµ ë°ì´í„° {total_deleted}ê°œë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
            if deleted_from_captured > 0:
                print(f" Â  - í•™ìŠµ ëŒ€ê¸° ì¤‘ì¸ ë°ì´í„° {deleted_from_captured}ê°œ ì‚­ì œë¨.")
            if deleted_from_trained > 0:
                print(f" Â  - ì´ë¯¸ í•™ìŠµëœ ë°ì´í„° {deleted_from_trained}ê°œ ì‚­ì œë¨.")

            # ì‚­ì œ í›„ ë§¤ì¹­ê¸° ì¬í•™ìŠµ
            print("ë§¤ì¹­ê¸°ë¥¼ ì¬í•™ìŠµí•©ë‹ˆë‹¤...")
            self.TrainModel(self.nameIndexList, self.nameIntList, self.orbDescriptors)
            self.save_training_data()
            print("âœ… ëª¨ë¸ ì‚­ì œ ë° ì¬í•™ìŠµ, ë°ì´í„° ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"â„¹ï¸ ëª¨ë¸ '{name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì‚­ì œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    def clear_all_models(self):
        """
        ëª¨ë“  í•™ìŠµëœ ìŠ¤ì¼€ì¹˜ ëª¨ë¸ê³¼ ìº¡ì²˜ëœ ìŠ¤ì¼€ì¹˜ ë°ì´í„°ë¥¼ ì™„ì „íˆ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        ì´ˆê¸°í™” í›„ ë³€ê²½ì‚¬í•­ì„ ì €ì¥í•©ë‹ˆë‹¤.
        """
        print("ğŸš¨ ëª¨ë“  ìŠ¤ì¼€ì¹˜ ëª¨ë¸ ë° í•™ìŠµ ë°ì´í„° ì´ˆê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        self.orbDescriptors = []
        self.nameIndexList = []
        self.nameIntList = []
        self.captured_descriptors = []
        self.captured_names = []
        self.unique_names_map = {}
        self.current_unique_idx = 0

        self.matcherHamming.clear() # ë§¤ì¹­ê¸° ë°ì´í„°ë„ í´ë¦¬ì–´

        self.save_training_data()
        print("âœ¨ ëª¨ë“  ìŠ¤ì¼€ì¹˜ ëª¨ë¸ê³¼ í•™ìŠµ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ê³  ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


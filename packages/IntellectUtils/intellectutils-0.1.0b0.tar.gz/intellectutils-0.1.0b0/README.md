# IntellectUtils

**ğŸš€ IntellectUtils** is a high-performance Python utility library built to **accelerate AI workloads on CPU**.  
Designed by **ARCX Studios** and the **Intellect** team â€” led by [Rayhan Aman](https://github.com/rayhanAman) and [Cooper Johnson](https://github.com/) â€” this toolkit simplifies and supercharges training, preprocessing, and inference on machines without powerful GPUs.

> ğŸ¯ â€œAI shouldnâ€™t be slow just because you donâ€™t have a GPU.â€

---

## âœ¨ Features

- ğŸ”¥ JIT acceleration with **Numba**
- âš™ï¸ Multi-threaded & multi-process parallelism with **Joblib** and **concurrent.futures**
- ğŸ”— Distributed task execution using **Ray**
- ğŸ§© Graph-based task scheduling using **Dask**
- ğŸ’¡ CPU tuning via **OpenBLAS**, **BLIS**, and **Intel MKL**
- ğŸ§  Efficient model inference using **ONNX Runtime**
- ğŸ“¦ Built-in helpers for **TensorFlow** and **PyTorch** CPU optimization
- ğŸ§ª Auto-tuning of environment variables for threading & performance

---

## ğŸ Quick Start

```bash

git clone https://github.com/LaserRay1234/IntellectUtils
cd IntellectUtils
pip install intellectutils

1. JIT Acceleration with Numba

    from intellect_utils.jit_utils import jit_compile

    @jit_compile
    def add_loop(arr):
        total = 0
        for x in arr:
            total += x
        return total

    print(add_loop([1, 2, 3, 4]))

2. Parallel Task Execution (CPU)

    from intellect_utils.parallel_utils import run_parallel_threadpool

    def slow_add(x, y):
        return x + y

    tasks = [(i, i * 2) for i in range(10)]
    results = run_parallel_threadpool(slow_add, tasks)
    print(results)

3. Distributed Computing with Ray

    from intellect_utils.ray_utils import init_ray, run_parallel, shutdown_ray

    def square(x):
        return x * x

    init_ray(num_cpus=4)
    tasks = [(i,) for i in range(8)]
    results = run_parallel(square, tasks)
    shutdown_ray()

    print(results)

4. ONNX Model Inference (CPU Optimized)

    from intellect_utils.onnx_utils import run_onnx_model

    output = run_onnx_model("model.onnx", input_dict={"input": input_array})

5. BLAS / MKL Thread Optimization
    from intellect_utils.blas_utils import set_blas_threads

    set_blas_threads(4)  # Apply to NumPy, SciPy, PyTorch, etc.

6. PyTorch & TensorFlow CPU Tuning

    from intellect_utils.torch_tf_utils import set_torch_num_threads, set_tf_num_threads

    set_torch_num_threads(4)
    set_tf_num_threads(4)

ğŸ“ Project Structure

intellect_utils/
â”œâ”€â”€ jit_utils.py
â”œâ”€â”€ parallel_utils.py
â”œâ”€â”€ dask_utils.py
â”œâ”€â”€ mkl_utils.py
â”œâ”€â”€ onnx_utils.py
â”œâ”€â”€ torch_tf_utils.py
â”œâ”€â”€ ray_utils.py
â”œâ”€â”€ blas_utils.py
â””â”€â”€ __init__.py

ğŸ”§ Configuration
Use these utilities before importing NumPy/TensorFlow/PyTorch:

from intellect_utils.blas_utils import set_blas_threads
set_blas_threads(4)

ğŸ§‘â€ğŸ’» About the Creators
ARCX Studios

Where AI meets speed.

ğŸ’¡ Built by Rayhan Aman, founder of Intellect, developer and AI optimization engineer.

ğŸ’¼ Co-created with Cooper Johnson, CFO, co-CEO of ARCX Studios, and systems strategist.

ğŸ§  Designed to help students, developers, and startups train AI faster using just CPUs.

ğŸªª License
MIT License

ğŸŒ Vision
Making AI development faster, more accessible, and GPU-free â€” powered by Intellect and ARCX Studios.

ğŸ“« Contact
GitHub: LaserRay1234

GitHub: ARCX-OFFICIAL
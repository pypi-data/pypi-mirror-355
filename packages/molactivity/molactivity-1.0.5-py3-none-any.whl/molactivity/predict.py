
from .tensor_T import Tensor
from .operations_T import sigmoid
from .pure_data_loader import prepare_pure_predicting_dataset
from .autograd_T import no_grad
from .Transformer import MolecularTransformer
from .model_save_load import load
from . import arrays
from . import data_manager as dm

def flatten_nested_dict(d, prefix="", separator="."):
    """é€’å½’å±•å¼€åµŒå¥—å­—å…¸"""
    if d is None:
        print(f"[ERROR] flatten_nested_dict: è¾“å…¥å­—å…¸ä¸ºNone")
        return {}
        
    if not isinstance(d, dict):
        print(f"[ERROR] flatten_nested_dict: è¾“å…¥ä¸æ˜¯å­—å…¸ç±»å‹ï¼Œè€Œæ˜¯: {type(d)}")
        return {}
    
    flattened = {}
    try:
        for key, value in d.items():
            new_key = f"{prefix}{separator}{key}" if prefix else key
            if isinstance(value, dict):
                nested_result = flatten_nested_dict(value, new_key, separator)
                if nested_result is not None:
                    flattened.update(nested_result)
            else:
                flattened[new_key] = value
    except Exception as e:
        return {}
        
    return flattened

def load_trained_network_pure(network, model_file):
    """Load trained network model parameters using pure Python approach"""
    print(f"å¼€å§‹åŠ è½½æ¨¡å‹æ–‡ä»¶: {model_file}")

     
    try:
        with open(model_file, 'rb') as f:
            saved_state = load(f)
                
        print(f"æ¨¡å‹åŠ è½½å®Œæˆ")
            
        if 'model_parameters' not in saved_state:
            print(f"ä¿å­˜çš„çŠ¶æ€ä¸­ç¼ºå°‘ 'model_parameters' é”®")
            print(f"å¯ç”¨é”®: {list(saved_state.keys()) if saved_state else 'saved_stateä¸ºNone'}")
            return False
                
        if saved_state['model_parameters'] is None:
            print(f"æ¨¡å‹å‚æ•°ä¸ºNone")
            return False
            
        saved_parameters = saved_state['model_parameters']
        if not isinstance(saved_parameters, dict):
            print(f"æ¨¡å‹å‚æ•°åº”è¯¥æ˜¯å­—å…¸ç±»å‹ï¼Œä½†å¾—åˆ°: {type(saved_parameters)}")
            return False
                
        if len(saved_parameters) == 0:
            print(f"æ¨¡å‹å‚æ•°å­—å…¸ä¸ºç©º")
            return False
                
        print(f"åŠ è½½äº† {len(saved_parameters)} ä¸ªæ¨¡å—")
            
        try:
            flattened_params = flatten_nested_dict(saved_parameters)
            if flattened_params is None:
                print(f"[ERROR] å‚æ•°å±•å¼€ç»“æœä¸ºNone")
                return False
            print(f"[DEBUG] å±•å¼€åæ€»å‚æ•°æ•°: {len(flattened_params)}")
        except AttributeError as e:
            print(f"å‚æ•°å±•å¼€å¤±è´¥: {e}")
            print(f"é—®é¢˜å‚æ•°ç±»å‹: {type(saved_parameters)}")
            print(f"é—®é¢˜å‚æ•°å†…å®¹: {saved_parameters}")
            return False
        except Exception as e:
            print(f"å‚æ•°å±•å¼€æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

            return False
            
        param_dict = dict(network.named_parameters())
        print(f"[DEBUG] ç½‘ç»œå‚æ•°æ•°: {len(param_dict)}")
            
        loaded_count = 0
        missing_params = []
        for name, param in param_dict.items():
            if name in flattened_params:
                saved_param = flattened_params[name]
                    
                if isinstance(saved_param, dict) and saved_param.get('__type__') == 'FinalArrayCompatible':
                        # é‡å»ºFinalArrayCompatibleå¯¹è±¡
                    from .final_array import FinalArrayCompatible
                    restored_data = FinalArrayCompatible(
                        saved_param['data'], 
                        saved_param['shape'], 
                        saved_param['dtype']
                    )
                    param.data = restored_data
                elif isinstance(saved_param, Tensor):
                    param.data = saved_param.data
                elif hasattr(saved_param, 'shape'):  
                    param.data = saved_param
                else:
                    if hasattr(saved_param, 'data'):
                        array_data = saved_param.data
                        param.data = arrays.asarray_numpy_compatible(array_data).data
                    else:
                        param.data = arrays.asarray_numpy_compatible(saved_param).data
                loaded_count += 1
            else:
                missing_params.append(name)
            
        print(f"æˆåŠŸåŠ è½½ {loaded_count}/{len(param_dict)} ä¸ªå‚æ•°")
        if missing_params:
            print(f"[WARNING] æœªæ‰¾åˆ°çš„å‚æ•°æ•°é‡: {len(missing_params)}")
            if len(missing_params) <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªç¼ºå¤±çš„å‚æ•°
                for mp in missing_params:
                    print(f"[WARNING] ç¼ºå¤±å‚æ•°: {mp}")
                        
        return loaded_count > 0  # åªè¦åŠ è½½äº†è‡³å°‘ä¸€äº›å‚æ•°å°±ç®—æˆåŠŸ
            
    except Exception as e:
        print(f"[ERROR] æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return False


def create_network():
    """åˆ›å»ºä¸è®­ç»ƒæ—¶ç›¸åŒçš„ç½‘ç»œæ¶æ„"""
    print("[DEBUG] åˆ›å»ºç½‘ç»œ...")
    
    network = MolecularTransformer(
        input_features=2048, 
        output_features=1, 
        embedding_size=128, #512
        layer_count=2,
        head_count=2,  #4
        hidden_size=64, #512
        dropout_rate=0.1
    )
    
    return network

def generate_predictions_pure(model, data_provider):
    """Generate predictions using pure Python model"""
    print("[DEBUG] å¼€å§‹ç”Ÿæˆé¢„æµ‹")
    model.eval()
    predictions = []
    

    
    with no_grad():
        for batch_idx, (features, _) in enumerate(data_provider):
            print(f"\nå¼€å§‹å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}")
            
            if features is None:
                print(f"[WARNING] æ‰¹æ¬¡ {batch_idx + 1} çš„ç‰¹å¾ä¸ºç©º")
                continue
            
            
            # ç¡®ä¿è¾“å…¥æ˜¯æˆ‘ä»¬çš„è‡ªå®šä¹‰Tensor
            if not isinstance(features, Tensor):
                features = Tensor(features, requires_grad=False)
            
            print(f"    å¼€å§‹æ¨¡å‹æ¨ç†...")
            
            try:
                outputs = model(features)
                
            except Exception as e:
                print(f"    æ¨¡å‹æ¨ç†å¤±è´¥: {e}")

                break
            
            try:
                outputs_squeezed = outputs.squeeze()
                
                # ç¡®ä¿è¾“å‡ºæ•°æ®æ ¼å¼æ­£ç¡®
                if hasattr(outputs_squeezed, 'data') and hasattr(outputs_squeezed.data, 'dtype'):
                    probs_tensor = sigmoid(outputs_squeezed)
                else:
                    # å¦‚æœæ•°æ®æ ¼å¼æœ‰é—®é¢˜ï¼Œå°è¯•é‡æ–°æ„é€ Tensor
                    safe_data = arrays.asarray_numpy_compatible(outputs_squeezed.data).data
                    safe_tensor = Tensor(safe_data, requires_grad=False)
                    probs_tensor = sigmoid(safe_tensor)
            except Exception as sigmoid_error:
                probs_tensor = Tensor([0.5] * outputs.shape[0] if hasattr(outputs, 'shape') and len(outputs.shape) > 0 else [0.5], requires_grad=False)
            
            # å®‰å…¨åœ°å°†ç»“æœè½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ·»åŠ åˆ°é¢„æµ‹åˆ—è¡¨
            try:
                if hasattr(probs_tensor.data, 'flatten'):
                    batch_predictions = probs_tensor.data.flatten().tolist()
                elif hasattr(probs_tensor.data, 'tolist'):
                    batch_predictions = probs_tensor.data.tolist()
                elif isinstance(probs_tensor.data, (list, tuple)):
                    batch_predictions = list(probs_tensor.data)
                else:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é‡å€¼
                    is_scalar = isinstance(probs_tensor.data, (int, float, complex)) or (hasattr(probs_tensor.data, 'ndim') and probs_tensor.data.ndim == 0)
                    if is_scalar:
                        # ç¡®ä¿æ ‡é‡å€¼å¯ä»¥è½¬æ¢ä¸ºfloat
                        try:
                            scalar_val = float(probs_tensor.data)
                            batch_predictions = [scalar_val]
                        except (TypeError, ValueError):
                            print(f"   æ— æ³•å°†æ ‡é‡å€¼è½¬æ¢ä¸ºfloat: {type(probs_tensor.data)}")
                            batch_predictions = [0.0]  # ä½¿ç”¨é»˜è®¤å€¼
                    else:
                        # å°è¯•è½¬æ¢ä¸ºæ•°ç»„ç„¶åæå–
                        try:
                            data_array = arrays.asarray_numpy_compatible(probs_tensor.data)
                            batch_predictions = data_array.data.flatten().tolist()
                        except Exception as conv_error:
                            print(f"   æ•°æ®è½¬æ¢å¤±è´¥: {conv_error}")
                            batch_predictions = [0.0]  # ä½¿ç”¨é»˜è®¤å€¼
            except Exception as e:
                print(f"   æ‰¹æ¬¡é¢„æµ‹å¤„ç†å¤±è´¥: {e}")
                batch_predictions = [0.0]  # ä½¿ç”¨é»˜è®¤å€¼
            
            predictions.extend(batch_predictions)
            
            print(f"   æ‰¹æ¬¡ {batch_idx + 1} å¤„ç†å®Œæˆï¼Œé¢„æµ‹æ•°é‡: {len(batch_predictions)}")
            print(f"   ç´¯è®¡é¢„æµ‹æ•°é‡: {len(predictions)}")
    
    print(f"\né¢„æµ‹å¤„ç†å®Œæˆ")
    print(f"æ€»é¢„æµ‹æ•°é‡: {len(predictions)}")
    if predictions:
        print(f"æœ€ç»ˆé¢„æµ‹å€¼èŒƒå›´: [{min(predictions):.4f}, {max(predictions):.4f}]")
    return predictions

def analyze_predictions_pure(predictions):
    """åˆ†æé¢„æµ‹ç»“æœ"""
    if not predictions:
        print("[ERROR] æ²¡æœ‰é¢„æµ‹ç»“æœå¯åˆ†æ")
        return
    
    print(f"\né¢„æµ‹ç»“æœåˆ†æ:")
    print(f"æ€»é¢„æµ‹æ•°é‡: {len(predictions)}")
    print(f"é¢„æµ‹å€¼èŒƒå›´: [{min(predictions):.4f}, {max(predictions):.4f}]")
    print(f"å¹³å‡é¢„æµ‹å€¼: {sum(predictions)/len(predictions):.4f}")
    
    # é¢„æµ‹åˆ†å¸ƒç»Ÿè®¡
    very_high = sum(1 for p in predictions if p > 0.9)
    high = sum(1 for p in predictions if 0.8 < p <= 0.9)
    medium = sum(1 for p in predictions if 0.5 < p <= 0.8)
    low = sum(1 for p in predictions if 0.2 < p <= 0.5)
    very_low = sum(1 for p in predictions if p <= 0.2)
    
    print(f"\né¢„æµ‹åˆ†å¸ƒ:")
    print(f"  æé«˜ (>0.9): {very_high} ({very_high/len(predictions)*100:.1f}%)")
    print(f"  é«˜ (0.8-0.9): {high} ({high/len(predictions)*100:.1f}%)")
    print(f"  ä¸­ (0.5-0.8): {medium} ({medium/len(predictions)*100:.1f}%)")
    print(f"  ä½ (0.2-0.5): {low} ({low/len(predictions)*100:.1f}%)")
    print(f"  æä½ (<=0.2): {very_low} ({very_low/len(predictions)*100:.1f}%)")

def save_predictions_to_csv(predictions, output_file='dataset_with_predictions_pure.csv'):
    """ä¿å­˜é¢„æµ‹ç»“æœåˆ°CSVæ–‡ä»¶"""
    print(f"[DEBUG] å¼€å§‹ä¿å­˜é¢„æµ‹ç»“æœåˆ°: {output_file}")
    
    try:
        # è¯»å–åŸå§‹é¢„æµ‹æ•°æ®é›†
        original_data = dm.read_csv('predicting_dataset.csv')
        
        # ç¡®ä¿é¢„æµ‹æ•°é‡ä¸åŸå§‹æ•°æ®æ•°é‡åŒ¹é…
        if len(predictions) != len(original_data):
            print(f"[WARNING] é¢„æµ‹æ•°é‡({len(predictions)})ä¸åŸå§‹æ•°æ®æ•°é‡({len(original_data)})ä¸åŒ¹é…")
            # å¦‚æœé¢„æµ‹æ•°é‡å°‘äºåŸå§‹æ•°æ®ï¼Œç”¨0.0å¡«å……
            while len(predictions) < len(original_data):
                predictions.append(0.0)
            # å¦‚æœé¢„æµ‹æ•°é‡å¤šäºåŸå§‹æ•°æ®ï¼Œæˆªæ–­
            if len(predictions) > len(original_data):
                predictions = predictions[:len(original_data)]
        
        # æ·»åŠ é¢„æµ‹åˆ—
        original_data_dict = original_data.to_dict()
        original_data_dict['PREDICTION'] = predictions
        
        # åˆ›å»ºæ–°çš„æ•°æ®è¡¨
        result_data = dm.DataTable(original_data_dict)
        
        # ä¿å­˜åˆ°CSVæ–‡ä»¶
        result_data.to_csv(output_file)
        
        print(f"é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        print(f"ä¿å­˜äº† {len(predictions)} ä¸ªé¢„æµ‹å€¼")
        
        # ç»Ÿè®¡ä¿å­˜çš„é¢„æµ‹ç»“æœ
        high_predictions = sum(1 for p in predictions if p > 0.8)
        medium_predictions = sum(1 for p in predictions if 0.5 <= p <= 0.8)
        low_predictions = sum(1 for p in predictions if p < 0.5)
        
        print(f"ä¿å­˜çš„é¢„æµ‹ç»Ÿè®¡:")
        print(f"  é«˜é¢„æµ‹å€¼ (>0.8): {high_predictions}")
        print(f"  ä¸­é¢„æµ‹å€¼ (0.5-0.8): {medium_predictions}")
        print(f"  ä½é¢„æµ‹å€¼ (<0.5): {low_predictions}")
        
        return True
        
    except Exception as e:
        print(f"ä¿å­˜é¢„æµ‹ç»“æœå¤±è´¥: {str(e)}")
        return False

def analyze_prediction_quality(output_file='dataset_with_predictions_pure.csv'):
    """åˆ†æé¢„æµ‹è´¨é‡ä¸çœŸå®æ ‡ç­¾å…³ç³» - ä» analyze_prediction_quality.py ç§»æ¤"""
    print("\nåˆ†æé¢„æµ‹è´¨é‡ä¸çœŸå®æ ‡ç­¾å…³ç³»")
    
    try:
        # è¯»å–é¢„æµ‹ç»“æœæ–‡ä»¶
        data = dm.read_csv(output_file)
        
        if 'PREDICTION' not in data.columns or 'ACTIVITY' not in data.columns:
            print("âŒ ç¼ºå°‘å¿…è¦çš„åˆ— (PREDICTION æˆ– ACTIVITY)")
            return
            
        predictions = [data['PREDICTION'][i] for i in range(len(data))]
        activities = [data['ACTIVITY'][i] for i in range(len(data))]
        
        print(f"æ•°æ®åŸºæœ¬ä¿¡æ¯:")
        print(f"  æ€»æ ·æœ¬æ•°: {len(predictions)}")
        print(f"  é¢„æµ‹å€¼èŒƒå›´: [{min(predictions):.4f}, {max(predictions):.4f}]")
        
        # åˆ†æçœŸå®æ ‡ç­¾åˆ†å¸ƒ
        active_count = sum(1 for a in activities if a == 1)
        inactive_count = sum(1 for a in activities if a == 0)
        print(f"  æ´»æ€§åŒ–åˆç‰© (æ ‡ç­¾=1): {active_count} ({active_count/len(activities)*100:.1f}%)")
        print(f"  éæ´»æ€§åŒ–åˆç‰© (æ ‡ç­¾=0): {inactive_count} ({inactive_count/len(activities)*100:.1f}%)")
        
        # åˆ†æé¢„æµ‹å€¼ä¸çœŸå®æ ‡ç­¾çš„å…³ç³»
        print(f"\né¢„æµ‹å‡†ç¡®æ€§åˆ†æ:")
        
        # æ‰¾å‡ºé¢„æµ‹å€¼æœ€é«˜çš„20ä¸ªæ ·æœ¬
        indexed_predictions = [(i, pred, act) for i, (pred, act) in enumerate(zip(predictions, activities))]
        indexed_predictions.sort(key=lambda x: x[1], reverse=True)  # æŒ‰é¢„æµ‹å€¼é™åºæ’åº
        
        print(f"é¢„æµ‹å€¼æœ€é«˜çš„10ä¸ªæ ·æœ¬:")
        print(f"  {'åºå·':<6} {'é¢„æµ‹å€¼':<10} {'çœŸå®æ ‡ç­¾':<8} {'æ­£ç¡®æ€§'}")
        print(f"  {'-'*35}")
        
        correct_high_predictions = 0
        for i in range(min(10, len(indexed_predictions))):
            idx, pred, actual = indexed_predictions[i]
            is_correct = "âœ…" if ((pred > 0.5 and actual == 1) or (pred <= 0.5 and actual == 0)) else "âŒ"
            if pred > 0.5 and actual == 1:
                correct_high_predictions += 1
            print(f"  {idx+1:<6} {pred:<10.6f} {actual:<8} {is_correct}")
        
        # æŒ‰çœŸå®æ ‡ç­¾åˆ†ç»„åˆ†æé¢„æµ‹å€¼
        print(f"\næŒ‰çœŸå®æ ‡ç­¾åˆ†ç»„çš„é¢„æµ‹ç»Ÿè®¡:")
        
        # æ´»æ€§åŒ–åˆç‰©çš„é¢„æµ‹æƒ…å†µ
        active_predictions = [pred for pred, act in zip(predictions, activities) if act == 1]
        if active_predictions:
            print(f"ğŸŸ¢ æ´»æ€§åŒ–åˆç‰© (æ ‡ç­¾=1, å…±{len(active_predictions)}ä¸ª):")
            print(f"  é¢„æµ‹å€¼èŒƒå›´: [{min(active_predictions):.4f}, {max(active_predictions):.4f}]")
            print(f"  å¹³å‡é¢„æµ‹å€¼: {sum(active_predictions)/len(active_predictions):.4f}")
            high_pred_active = sum(1 for p in active_predictions if p > 0.5)
            print(f"  é«˜é¢„æµ‹å€¼(>0.5): {high_pred_active} ({high_pred_active/len(active_predictions)*100:.1f}%)")
        
        # éæ´»æ€§åŒ–åˆç‰©çš„é¢„æµ‹æƒ…å†µ  
        inactive_predictions = [pred for pred, act in zip(predictions, activities) if act == 0]
        if inactive_predictions:
            print(f"ğŸ”´ éæ´»æ€§åŒ–åˆç‰© (æ ‡ç­¾=0, å…±{len(inactive_predictions)}ä¸ª):")
            print(f"  é¢„æµ‹å€¼èŒƒå›´: [{min(inactive_predictions):.4f}, {max(inactive_predictions):.4f}]")
            print(f"  å¹³å‡é¢„æµ‹å€¼: {sum(inactive_predictions)/len(inactive_predictions):.4f}")
            high_pred_inactive = sum(1 for p in inactive_predictions if p > 0.5)
            print(f"  é«˜é¢„æµ‹å€¼(>0.5): {high_pred_inactive} ({high_pred_inactive/len(inactive_predictions)*100:.1f}%)")
        
        
        # ä½¿ç”¨0.5ä½œä¸ºé˜ˆå€¼
        threshold = 0.5
        tp = sum(1 for pred, act in zip(predictions, activities) if pred > threshold and act == 1)  # çœŸæ­£ä¾‹
        fp = sum(1 for pred, act in zip(predictions, activities) if pred > threshold and act == 0)  # å‡æ­£ä¾‹
        tn = sum(1 for pred, act in zip(predictions, activities) if pred <= threshold and act == 0)  # çœŸè´Ÿä¾‹
        fn = sum(1 for pred, act in zip(predictions, activities) if pred <= threshold and act == 1)  # å‡è´Ÿä¾‹
        
        #print(f"  é˜ˆå€¼: {threshold}")
        #print(f"  çœŸæ­£ä¾‹ (TP): {tp}")
        #print(f"  å‡æ­£ä¾‹ (FP): {fp}")
        #print(f"  çœŸè´Ÿä¾‹ (TN): {tn}")
        #print(f"  å‡è´Ÿä¾‹ (FN): {fn}")
        
        if tp + fp > 0:
            precision = tp / (tp + fp)
            #print(f"  ç²¾ç¡®ç‡: {precision:.4f}")
        else:
            print(f"  ç²¾ç¡®ç‡: æ— æ³•è®¡ç®— (æ²¡æœ‰æ­£é¢„æµ‹)")
            
        if tp + fn > 0:
            recall = tp / (tp + fn)
            #print(f"  å¬å›ç‡: {recall:.4f}")
        else:
            print(f"  å¬å›ç‡: æ— æ³•è®¡ç®— (æ²¡æœ‰çœŸå®æ­£ä¾‹)")
            
        accuracy = (tp + tn) / len(predictions)
        #print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
        
        # è¯Šæ–­
        if tp == 0 and active_count > 0:
            print(f"ä¸¥é‡é—®é¢˜ï¼šæ¨¡å‹æ— æ³•è¯†åˆ«ä»»ä½•æ´»æ€§åŒ–åˆç‰©ï¼")
            if len(active_predictions) > 0 and sum(active_predictions) / len(active_predictions) < 0.5:
                print(f"æ´»æ€§åŒ–åˆç‰©çš„å¹³å‡é¢„æµ‹å€¼å¾ˆä½ï¼Œæ¨¡å‹å¯èƒ½å­¦åäº†")
        
            
    except Exception as e:
        print(f"é¢„æµ‹è´¨é‡åˆ†æå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹é¢„æµ‹")
    
    # åˆ›å»ºç½‘ç»œ
    network = create_network()
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    #model_file = 'model_1_continued.dict'
    model_file = 'model_1.dict'

    if load_trained_network_pure(network, model_file):
        print("æ¨¡å‹åŠ è½½æˆåŠŸ")
    else:
        print("æ¨¡å‹åŠ è½½å¤±è´¥")
        return
    
    # å‡†å¤‡é¢„æµ‹æ•°æ®é›† - ä¿®æ­£æ–‡ä»¶å
    print("å‡†å¤‡é¢„æµ‹æ•°æ®é›†...")
    data_provider = prepare_pure_predicting_dataset('predicting_dataset.csv', fingerprint_type='Morgan', 
                                                  batch_size=32, shuffle=False)
    print("æ•°æ®é›†å‡†å¤‡å®Œæˆ")
    
    
    # ç”Ÿæˆé¢„æµ‹
    predictions = generate_predictions_pure(network, data_provider)
    
    # åˆ†æç»“æœ
    analyze_predictions_pure(predictions)
    
    # ä¿å­˜é¢„æµ‹ç»“æœåˆ°CSVæ–‡ä»¶
    output_file = 'dataset_with_predictions_pure.csv'
    if predictions:
        if save_predictions_to_csv(predictions, output_file):
            print("\n" + "="*50)
            # è¿è¡Œé¢„æµ‹è´¨é‡åˆ†æ
            analyze_prediction_quality(output_file)
        else:
            print("é¢„æµ‹ç»“æœä¿å­˜å¤±è´¥ï¼Œè·³è¿‡è´¨é‡åˆ†æ")
    else:
        print("æ²¡æœ‰é¢„æµ‹ç»“æœå¯ä¿å­˜")
    


if __name__ == "__main__":
    main() 
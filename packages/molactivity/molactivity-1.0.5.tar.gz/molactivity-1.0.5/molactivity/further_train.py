from .tensor_T import Tensor
from .Transformer import MolecularTransformer
from .optimizer_T import Adam
from .model_save_load import load, dump
from .new_focal_loss import FocalLoss
from .operations_T import sigmoid
from .autograd_T import no_grad

def flatten_nested_dict(d, prefix="", separator="."):
    """é€’å½’å±•å¼€åµŒå¥—å­—å…¸"""
    if d is None:
        print(f"[ERROR] flatten_nested_dict: è¾“å…¥å­—å…¸ä¸ºNone")
        return {}
        
    if not isinstance(d, dict):
        print(f"[ERROR] flatten_nested_dict: è¾“å…¥ä¸æ˜¯å­—å…¸ç±»åž‹ï¼Œè€Œæ˜¯: {type(d)}")
        return {}
    
    flattened = {}
    try:
        for key, value in d.items():
            new_key = f"{prefix}{separator}{key}" if prefix else key
            if isinstance(value, dict):
                # é€’å½’å±•å¼€åµŒå¥—å­—å…¸
                nested_result = flatten_nested_dict(value, new_key, separator)
                if nested_result is not None:
                    flattened.update(nested_result)
            else:
                flattened[new_key] = value
    except Exception as e:
        print(f"[ERROR] flatten_nested_dict: å¤„ç†å­—å…¸æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        print(f"[DEBUG] é—®é¢˜å­—å…¸å†…å®¹: {d}")
        return {}
        
    return flattened

def load_model_for_continue_training(network, optimizer, model_file):
    """åŠ è½½å·²è®­ç»ƒçš„æ¨¡åž‹å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼Œç”¨äºŽç»§ç»­è®­ç»ƒ"""
    print(f"ðŸ”„ å°è¯•åŠ è½½æ¨¡åž‹: {model_file}")
    try:
        # ä½¿ç”¨pickleåŠ è½½
        with open(model_file, 'rb') as f:
            saved_state = load(f)
        
        print(f"âœ… æ¨¡åž‹æ–‡ä»¶åŠ è½½æˆåŠŸ")
        
        # åŠ è½½æ¨¡åž‹å‚æ•°
        saved_model_params = saved_state['model_parameters']
        flattened_params = flatten_nested_dict(saved_model_params)
        
        # å°†ä¿å­˜çš„å‚æ•°åŠ è½½åˆ°ç½‘ç»œä¸­
        param_dict = dict(network.named_parameters())
        loaded_count = 0
        
        for name, param in param_dict.items():
            if name in flattened_params:
                saved_param = flattened_params[name]
                
                # å¤„ç†pickleå®‰å…¨æ ¼å¼
                if isinstance(saved_param, dict) and saved_param.get('__type__') == 'FinalArrayCompatible':
                    from .final_array import FinalArrayCompatible
                    restored_data = FinalArrayCompatible(
                        saved_param['data'], 
                        saved_param['shape'], 
                        saved_param['dtype']
                    )
                    param.data = restored_data
                elif isinstance(saved_param, Tensor):
                    param.data = saved_param.data
                elif hasattr(saved_param, 'shape'):  
                    param.data = saved_param
                else:
                    from . import arrays
                    param.data = arrays.array(saved_param)
                loaded_count += 1
        
        print(f"âœ… æˆåŠŸåŠ è½½ {loaded_count}/{len(param_dict)} ä¸ªæ¨¡åž‹å‚æ•°")
        
        # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
        if 'optimizer_state' in saved_state and saved_state['optimizer_state']:
            try:
                optimizer.load_state_dict(saved_state['optimizer_state'])
                print("âœ… ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤çŠ¶æ€: {e}")
        
        # èŽ·å–ä¹‹å‰çš„è®­ç»ƒè½®æ•°
        previous_epochs = saved_state.get('epoch', 0)
        print(f"ðŸ“Š ä¹‹å‰è®­ç»ƒè½®æ•°: {previous_epochs}")
        
        return True, previous_epochs
        
    except Exception as e:
        print(f"âŒ æ¨¡åž‹åŠ è½½å¤±è´¥: {str(e)}")
        return False, 0

def continue_train(model_file, data_handler, additional_epochs=2, activation='gelu', 
                  optimal_parameters=None, network_index=0, new_model_suffix='_continued'):
    """
    ç»§ç»­è®­ç»ƒå·²æœ‰çš„æ¨¡åž‹
    
    å‚æ•°:
    - model_file: è¦åŠ è½½çš„æ¨¡åž‹æ–‡ä»¶è·¯å¾„
    - data_handler: è®­ç»ƒæ•°æ®å¤„ç†å™¨
    - additional_epochs: é¢å¤–è®­ç»ƒçš„è½®æ•°
    - activation: æ¿€æ´»å‡½æ•°ç±»åž‹
    - optimal_parameters: ä¼˜åŒ–å‚æ•°é…ç½®
    - network_index: ç½‘ç»œç´¢å¼•ï¼ˆç”¨äºŽæ—¥å¿—ï¼‰
    - new_model_suffix: æ–°æ¨¡åž‹æ–‡ä»¶çš„åŽç¼€
    """
    print(f"ðŸ”„ å¼€å§‹ç»§ç»­è®­ç»ƒæ¨¡åž‹: {model_file}")
    
    # ä½¿ç”¨é»˜è®¤å‚æ•°å¦‚æžœæ²¡æœ‰æä¾›
    if optimal_parameters is None:
        optimal_parameters = {
            'learning_rate': 0.001,
            'transformer_depth': 2,
            'attention_heads': 2,
            'hidden_dimension': 64
        }
    
    # åˆ›å»ºç½‘ç»œæž¶æž„ï¼ˆä¸Žè®­ç»ƒæ—¶ç›¸åŒï¼‰
    print(f"ðŸŽ¯ åˆ›å»ºç½‘ç»œæž¶æž„ - æ¿€æ´»å‡½æ•°: {activation.upper()}")
    network = MolecularTransformer(
        input_features=2048,
        output_features=1, 
        embedding_size=128,
        layer_count=optimal_parameters['transformer_depth'],
        head_count=optimal_parameters['attention_heads'], 
        hidden_size=optimal_parameters['hidden_dimension'], 
        dropout_rate=0.1,
        activation=activation
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = Adam(network.parameters(), lr=optimal_parameters['learning_rate'])
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹
    success, previous_epochs = load_model_for_continue_training(network, optimizer, model_file)
    
    if not success:
        print("âŒ æ— æ³•åŠ è½½æ¨¡åž‹ï¼Œç»§ç»­è®­ç»ƒå¤±è´¥")
        return None
    
    # å¼€å§‹ç»§ç»­è®­ç»ƒ
    print(f"ðŸš€ å¼€å§‹ç»§ç»­è®­ç»ƒ {additional_epochs} ä¸ªè½®æ•°")
    print(f"ðŸ“Š ä»Žç¬¬ {previous_epochs + 1} è½®å¼€å§‹")
    
    criterion = FocalLoss(alpha=0.25, gamma=2.0, high_pred_penalty=5.0, reduction='mean')
    
    for epoch in range(additional_epochs):
        current_epoch = previous_epochs + epoch + 1
        print(f"\n=== ç»§ç»­è®­ç»ƒ Epoch {current_epoch} ===")
        
        epoch_losses = []
        batch_count = 0
        high_pred_false_count = 0
        very_high_pred_false_count = 0 
        extreme_high_pred_false_count = 0
        all_predictions = []
        
        # è®­ç»ƒæ¯ä¸ªæ‰¹æ¬¡
        for batch_idx, (features, labels) in enumerate(data_handler):
            batch_count += 1
            
            # ç¡®ä¿è¾“å…¥æ•°æ®æ˜¯æˆ‘ä»¬çš„è‡ªå®šä¹‰Tensor
            if not isinstance(features, Tensor):
                features = Tensor(features, requires_grad=False)
            if not isinstance(labels, Tensor):
                labels = Tensor(labels, requires_grad=False)
            
            # å‰å‘ä¼ æ’­
            outputs = network(features)  
            
            # ç¡®ä¿æ ‡ç­¾ç»´åº¦æ­£ç¡®
            if labels.data.ndim > 1:
                labels = Tensor(labels.data.squeeze(), requires_grad=False)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(outputs.squeeze(), labels)
            
            # å¼‚å¸¸æŸå¤±æ£€æµ‹
            loss_value = loss.data.item() if hasattr(loss.data, 'item') else float(loss.data)
            if loss_value > 5.0: 
                print(f"âš ï¸ å¼‚å¸¸é«˜æŸå¤± {loss_value:.4f}ï¼Œè·³è¿‡å‚æ•°æ›´æ–°")
                continue
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_losses.append(loss_value)
            
            # è®¡ç®—é¢„æµ‹å‡†ç¡®æ€§ç»Ÿè®¡
            with no_grad():
                predictions = sigmoid(outputs.squeeze())
                all_predictions.extend(predictions.data.flatten().tolist())
                
                pred_data = predictions.data.flatten()
                label_data = labels.data.flatten()
                
                for pred, label in zip(pred_data, label_data):
                    if pred > 0.9 and label < 0.5:
                        high_pred_false_count += 1
                    if pred > 0.95 and label < 0.5:
                        very_high_pred_false_count += 1
                    if pred > 0.98 and label < 0.5:
                        extreme_high_pred_false_count += 1
            
            # æ¯10ä¸ªæ‰¹æ¬¡æ‰“å°æŸå¤±
            if batch_count % 10 == 0:
                print(f"    æ‰¹æ¬¡ {batch_count}, æŸå¤±: {loss_value:.4f}")
        
        # Epochæ€»ç»“
        avg_loss = sum(epoch_losses) / len(epoch_losses) if epoch_losses else 0
        print(f"  Epoch [{current_epoch}], å¹³å‡æŸå¤±: {avg_loss:.4f}, "
              f"High Pred False: {high_pred_false_count}, "
              f"Very High Pred False: {very_high_pred_false_count}, "
              f"Extreme High Pred False: {extreme_high_pred_false_count}")
        
        # æ‰“å°é¢„æµ‹å€¼èŒƒå›´
        if all_predictions:
            min_pred = min(all_predictions)
            max_pred = max(all_predictions)
            print(f"  [DEBUG] Epoch {current_epoch} é¢„æµ‹å€¼èŒƒå›´: [{min_pred:.4f}, {max_pred:.4f}]")
    
    # ç”Ÿæˆæ–°çš„æ¨¡åž‹æ–‡ä»¶å
    base_name = model_file.rsplit('.', 1)[0]  # ç§»é™¤æ‰©å±•å
    extension = model_file.rsplit('.', 1)[1] if '.' in model_file else 'dict'
    new_model_file = f"{base_name}{new_model_suffix}.{extension}"
    
    # ä¿å­˜ç»§ç»­è®­ç»ƒåŽçš„æ¨¡åž‹
    print(f"ðŸ’¾ ä¿å­˜ç»§ç»­è®­ç»ƒåŽçš„æ¨¡åž‹åˆ°: {new_model_file}")
    
    # èŽ·å–æ¨¡åž‹å‚æ•°ï¼Œå¹¶æ·»åŠ éªŒè¯
    model_params = network.state_dict()
    print(f"[DEBUG] èŽ·å–åˆ°æ¨¡åž‹å‚æ•°ï¼Œç±»åž‹: {type(model_params)}")
    
    # éªŒè¯æ¨¡åž‹å‚æ•°ä¸ä¸ºç©º
    if model_params is None:
        print(f"âŒ é”™è¯¯ï¼šç½‘ç»œçŠ¶æ€å­—å…¸ä¸ºNone")
        return None
    
    if not isinstance(model_params, dict):
        print(f"âŒ é”™è¯¯ï¼šç½‘ç»œçŠ¶æ€å­—å…¸ä¸æ˜¯å­—å…¸ç±»åž‹ï¼Œè€Œæ˜¯: {type(model_params)}")
        return None
        
    if len(model_params) == 0:
        print(f"âŒ é”™è¯¯ï¼šç½‘ç»œçŠ¶æ€å­—å…¸ä¸ºç©º")
        return None
    
    print(f"[DEBUG] æ¨¡åž‹å‚æ•°éªŒè¯é€šè¿‡ï¼ŒåŒ…å« {len(model_params)} ä¸ªæ¨¡å—")
    
    # èŽ·å–ä¼˜åŒ–å™¨çŠ¶æ€
    try:
        optimizer_state = optimizer.state_dict()
        print(f"[DEBUG] èŽ·å–ä¼˜åŒ–å™¨çŠ¶æ€æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ èŽ·å–ä¼˜åŒ–å™¨çŠ¶æ€å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ç©ºçŠ¶æ€")
        optimizer_state = {}
    
    save_data = {
        'model_parameters': model_params,
        'optimizer_state': optimizer_state,
        'epoch': previous_epochs + additional_epochs
    }
    
    try:
        with open(new_model_file, 'wb') as f:
            dump(save_data, f)
        print(f"âœ… æ¨¡åž‹ä¿å­˜æˆåŠŸ: {new_model_file}")
        
        # éªŒè¯ä¿å­˜çš„æ–‡ä»¶ï¼ˆå¯é€‰æ­¥éª¤ï¼Œå¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹ï¼‰
        print(f"[DEBUG] éªŒè¯ä¿å­˜çš„æ–‡ä»¶...")
        try:
            with open(new_model_file, 'rb') as f:
                test_load = load(f)
                if 'model_parameters' in test_load and test_load['model_parameters'] is not None:
                    print(f"[DEBUG] æ–‡ä»¶éªŒè¯æˆåŠŸï¼ŒåŒ…å« {len(test_load['model_parameters'])} ä¸ªæ¨¡å—å‚æ•°")
                else:
                    print(f"âš ï¸ æ–‡ä»¶éªŒè¯å‘çŽ°é—®é¢˜ï¼šmodel_parameters ä¸º None æˆ–ç¼ºå¤±ï¼Œä½†æ–‡ä»¶å·²ä¿å­˜")
        except Exception as verification_error:
            print(f"âš ï¸ æ–‡ä»¶éªŒè¯è¿‡ç¨‹å‡ºçŽ°é”™è¯¯: {verification_error}")
            print(f"   ä½†æ¨¡åž‹æ–‡ä»¶å·²æˆåŠŸä¿å­˜åˆ°: {new_model_file}")
            print(f"   å¯ä»¥å°è¯•ä½¿ç”¨è¯¥æ–‡ä»¶è¿›è¡Œé¢„æµ‹")
                
    except Exception as e:
        print(f"âŒ æ¨¡åž‹ä¿å­˜å¤±è´¥: {e}")

        return None
    
    print(f"ðŸŽ‰ ç»§ç»­è®­ç»ƒå®Œæˆï¼")
    print(f"ðŸ“Š æ€»è®­ç»ƒè½®æ•°: {previous_epochs + additional_epochs}")
    print(f"ðŸ’¾ æ–°æ¨¡åž‹æ–‡ä»¶: {new_model_file}")
    
    return network, new_model_file 
# local_torch/_tensor/autograd.py

from . import ref1
from .tools import wraps, Enum, auto, Lock
from . import arrays
from . import strong_matmul

# === Global debug flag ===
DEBUG_AUTOGRAD = False  # Set to True to enable debug output

# === æ ¸å¿ƒç±»å‹å®šä¹‰ ===
class GradMode(Enum):
    """æ¢¯åº¦æ¨¡å¼
    
    å®šä¹‰äº†ä¸¤ç§æ¢¯åº¦æ¨¡å¼ï¼š
    - TRAINING: è®­ç»ƒæ¨¡å¼ï¼Œå¯ç”¨æ¢¯åº¦è®¡ç®—
    - INFERENCE: æ¨ç†æ¨¡å¼ï¼Œç¦ç”¨æ¢¯åº¦è®¡ç®—
    """
    TRAINING = auto()
    INFERENCE = auto()

class Context:
    __slots__ = ['saved_tensors', 'non_differentiable', 'metadata', 'module_ref', 'module_ref_a', 'module_ref_b', 'module_refs']
    
    def __init__(self):
        self.saved_tensors = None  # åˆå§‹åŒ–ä¸º None è€Œä¸æ˜¯ç©ºå…ƒç»„
        self.non_differentiable = set()
        self.metadata = {}
        self.module_ref = None
        self.module_ref_a = None
        self.module_ref_b = None
        self.module_refs = {}  # å­˜å‚¨æ¯ä¸ªè¾“å…¥å‚æ•°çš„æ¨¡å—å¼•ç”¨
    
    def save_for_backward(self, *tensors):  # ç§»é™¤Tensorç±»å‹æ³¨è§£
        """ä¿å­˜å¼ é‡ç”¨äºåå‘ä¼ æ’­
        
        Args:
            *tensors: è¦ä¿å­˜çš„å¼ é‡
        """
        self.saved_tensors = tensors
    
    def mark_non_differentiable(self, *tensors):  # ç§»é™¤Tensorç±»å‹æ³¨è§£
        """æ ‡è®°å¼ é‡ä¸ºä¸å¯å¾®
        
        Args:
            *tensors: è¦æ ‡è®°çš„å¼ é‡
        """
        self.non_differentiable.update(id(t) for t in tensors)

class FunctionMeta(type):
    """å‡½æ•°å…ƒç±»
    
    ç”¨äºå¤„ç†å‡½æ•°çš„å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬æ³¨å†Œå’ŒåŒ…è£…æ–¹æ³•ã€‚
    """
    _registry = {}
    
    def __new__(mcls, name, bases, attrs):
        """åˆ›å»ºæ–°çš„å‡½æ•°ç±»
        
        Args:
            name: ç±»å
            bases: åŸºç±»
            attrs: ç±»å±æ€§
        
        Returns:
            æ–°çš„å‡½æ•°ç±»
        """
        for method in ['forward', 'backward']:
            if method in attrs:
                attrs[method] = FunctionMeta._wrap_method(attrs[method])
        cls = super().__new__(mcls, name, bases, attrs)
        if name != 'Function':
            mcls._registry[name] = cls
        return cls
    
    @staticmethod
    def _wrap_method(method):
        """åŒ…è£…æ–¹æ³•
        
        Args:
            method: è¦åŒ…è£…çš„æ–¹æ³•
        
        Returns:
            åŒ…è£…åçš„æ–¹æ³•
        """
        @wraps(method)
        def wrapper(ctx, *args, **kwargs):
            return method(ctx, *args, **kwargs)
        return staticmethod(wrapper)

class Function(metaclass=FunctionMeta):
    """å‡½æ•°åŸºç±»
    
    æ‰€æœ‰æ”¯æŒè‡ªåŠ¨å¾®åˆ†çš„å‡½æ•°éƒ½åº”è¯¥ç»§æ‰¿è¿™ä¸ªç±»ã€‚
    """
    __slots__ = ['requires_grad', 'ctx']
    
    @staticmethod
    def forward(ctx, *args):
        """å‰å‘ä¼ æ’­
        
        Args:
            ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
            *args: è¾“å…¥å‚æ•°
        
        Returns:
            å‰å‘ä¼ æ’­çš„ç»“æœ
        """
        raise NotImplementedError("å¿…é¡»å®ç°forwardæ–¹æ³•")
    
    @staticmethod
    def backward(ctx, *grad_outputs):
        """åå‘ä¼ æ’­
        
        Args:
            ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
            *grad_outputs: ä¸Šæ¸¸æ¢¯åº¦
        
        Returns:
            æ¯ä¸ªè¾“å…¥å‚æ•°çš„æ¢¯åº¦
        """
        raise NotImplementedError("å¿…é¡»å®ç°backwardæ–¹æ³•")
    
    @classmethod
    def apply(cls, *args, **kwargs):
        """åº”ç”¨å‡½æ•°
        
        Args:
            *args: è¾“å…¥å‚æ•°
            **kwargs: å…³é”®å­—å‚æ•°
        
        Returns:
            å‡½æ•°çš„ç»“æœ
        """
        # å»¶è¿Ÿå¯¼å…¥+è¿è¡Œæ—¶ç±»å‹æ£€æŸ¥
        
        def is_tensor(obj):
            return hasattr(obj, '_data') and hasattr(obj, 'requires_grad')
        
        ctx = Context()
        tensor_args = []
        processed_args = []
        
        # æŸ¥æ‰¾å¯èƒ½çš„æ¨¡å—å¼•ç”¨
        source_module = None
        module_refs = {}
        
        # é¦–å…ˆæ”¶é›†æ‰€æœ‰è¾“å…¥å¼ é‡çš„æ¨¡å—å¼•ç”¨
        for i, arg in enumerate(args):
            if is_tensor(arg):
                if hasattr(arg, '_module') and arg._module is not None:
                    module_refs[i] = arg._module
                    if source_module is None:
                        source_module = arg._module
        
        # å¤„ç†è¾“å…¥å‚æ•°ï¼ˆä¸å†è½¬ä¸º .dataï¼Œç›´æ¥ä¼ é€’ Tensorï¼‰
        for i, arg in enumerate(args):
            if is_tensor(arg):
                tensor_args.append(arg)
                processed_args.append(arg)  # ç›´æ¥ä¼ é€’ Tensor
            else:
                processed_args.append(arg)
        
        # ä¿å­˜æ¨¡å—å¼•ç”¨ä¿¡æ¯åˆ°ä¸Šä¸‹æ–‡
        ctx.module_ref = source_module
        ctx.module_refs = module_refs
        
        raw_output = cls.forward(ctx, *processed_args, **kwargs)
        if hasattr(raw_output, '_data') and hasattr(raw_output, 'data'):
            # å¦‚æœraw_outputæ˜¯Tensorå¯¹è±¡ï¼Œæå–å…¶dataå±æ€§
            raw_output = raw_output.data
        elif hasattr(raw_output, 'data') and hasattr(raw_output, 'shape'):
            # å¦‚æœraw_outputæ˜¯Arrayå¯¹è±¡ï¼Œæå–å…¶dataå±æ€§
            raw_output = raw_output.data
        elif not isinstance(raw_output, (list, float)):
            try:
                raw_output = arrays.array(raw_output, dtype=float)
            except (ValueError, TypeError):
                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                if hasattr(raw_output, 'tolist'):
                    raw_output = raw_output.tolist()
                else:
                    raw_output = float(raw_output)
        
        # æ£€æŸ¥å½“å‰æ¢¯åº¦æ¨¡å¼
        #if _engine._grad_mode == GradMode.INFERENCE:
         #   requires_grad = False
        #else:
            # ä¿®æ”¹æ£€æµ‹é€»è¾‘ï¼šåªè¦æœ‰ä¸€ä¸ªè¾“å…¥tensoréœ€è¦æ¢¯åº¦ï¼Œè¾“å‡ºå°±éœ€è¦æ¢¯åº¦
        requires_grad = any(getattr(t, 'requires_grad', False) for t in tensor_args)
            # å…è®¸é€šè¿‡kwargsæ˜¾å¼è®¾ç½®requires_grad
        if 'requires_grad' in kwargs:
            requires_grad = kwargs['requires_grad']
        
        # åˆ›å»ºè¾“å‡ºtensor
        from .tensor_T import Tensor
        output = Tensor(
            raw_output,
            requires_grad=requires_grad,
            _grad_fn=cls if requires_grad else None,
            _children=tensor_args if requires_grad else []
        )
        
        # ä¼ é€’æ¨¡å—å¼•ç”¨
        if source_module is not None and hasattr(output, 'attach_module_reference'):
            output.attach_module_reference(source_module)
        
        # å¦‚æœéœ€è¦æ¢¯åº¦ï¼Œè®¾ç½®è®¡ç®—å›¾
        if requires_grad:
            output._ctx = ctx
                
            # ä¸ºæ¯ä¸ªéœ€è¦æ¢¯åº¦çš„è¾“å…¥å¼ é‡æ·»åŠ å¯¹å½“å‰è¾“å‡ºçš„å¼•ç”¨
            for t in tensor_args:
                if getattr(t, 'requires_grad', False):
                    if not hasattr(t, '_output_refs'):
                        t._output_refs = []
                    t._output_refs.append(ref1.ref(output))
        
        return output

class DistAutogradContext:
    """åˆ†å¸ƒå¼è‡ªåŠ¨å¾®åˆ†ä¸Šä¸‹æ–‡
    
    ç”¨äºç®¡ç†åˆ†å¸ƒå¼ç¯å¢ƒä¸‹çš„è‡ªåŠ¨å¾®åˆ†ã€‚
    """
    def __init__(self):
        """åˆå§‹åŒ–åˆ†å¸ƒå¼è‡ªåŠ¨å¾®åˆ†ä¸Šä¸‹æ–‡"""
        self._worker_id = 0
        self._contexts = {}

class BackwardEngine:
    _instance = None
    _lock = Lock()
    _grad_mode = GradMode.TRAINING
    
    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance._init()
            return cls._instance
    
    def _init(self):
        self._dist_context = None
    
    def _compute_backward(self, func, *args, grad_outputs):
        from .tensor_T import Tensor
        try:
            # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æœ‰æ•ˆ
            for grad in (grad_outputs if isinstance(grad_outputs, (tuple, list)) else [grad_outputs]):
                if grad is not None:
                    grad_array = arrays.Array(grad.data.flatten())
                    isnan_result = arrays.isnan(grad_array)
                    isinf_result = arrays.isinf(grad_array)
                    if any(isnan_result.data) or any(isinf_result.data):
                        print("Warning: Invalid gradient in backward computation")
                        return tuple(Tensor(arrays.zeros_like(arg.data)) for arg in args)
            
            # æ‰§è¡Œåå‘ä¼ æ’­
            if isinstance(grad_outputs, (tuple, list)):
                grads = func.backward(*args, *grad_outputs)
            else:
                grads = func.backward(*args, grad_outputs)
            
            # æ£€æŸ¥è®¡ç®—ç»“æœ
            if grads is None:
                return tuple(None for _ in args)
            
            # ç¡®ä¿è¿”å›çš„æ˜¯å…ƒç»„
            if not isinstance(grads, tuple):
                grads = (grads,)
            
            # æ£€æŸ¥æ¯ä¸ªæ¢¯åº¦
            valid_grads = []
            for grad, arg in zip(grads, args):
                if grad is None:
                    valid_grads.append(None)
                else:
                    grad_array = arrays.Array(grad.data.flatten())
                    isnan_result = arrays.isnan(grad_array)
                    isinf_result = arrays.isinf(grad_array)
                    if any(isnan_result.data) or any(isinf_result.data):
                        print("Warning: Invalid gradient after backward computation")
                        valid_grads.append(Tensor(arrays.zeros_like(arg.data)))
                    else:
                        valid_grads.append(grad)
            
            return tuple(valid_grads)
            
        except Exception as e:
            print(f"Error in backward computation: {str(e)}")
            return tuple(Tensor(arrays.zeros_like(arg.data)) for arg in args)
    
    def execute_backward(self, root, grad=None):
        """æ‰§è¡Œåå‘ä¼ æ’­"""
        from .tensor_T import Tensor
        if grad is None:
            root_array = arrays.Array(root.data)
            ones_like_array = arrays.ones_like(root_array)
            grad = Tensor(arrays.array(ones_like_array.data))
        
        # åˆå§‹åŒ–æ¢¯åº¦å­—å…¸ï¼Œç”¨äºç´¯ç§¯æ¢¯åº¦
        all_grads = {}
        all_grads[id(root)] = grad
        
        # æ„å»ºæ‹“æ‰‘æ’åº
        visited = set()
        topo = []
        
        def build_topo(node):
            if id(node) in visited or node is None:
                return
            visited.add(id(node))
            if hasattr(node, '_children'):
                for child in node._children:
                    if child is not None:
                        build_topo(child)
            topo.append(node)
        
        build_topo(root)
        
        # åå‘éå†æ‹“æ‰‘æ’åº
        for node in reversed(topo):
            if node is None or not hasattr(node, '_grad_fn') or node._grad_fn is None:
                continue
                
            # è·å–å½“å‰èŠ‚ç‚¹çš„æ¢¯åº¦
            node_grad = all_grads.get(id(node))
            if node_grad is None:
                continue
            
            # è°ƒç”¨backwardè®¡ç®—æ¢¯åº¦
            ctx = getattr(node, '_ctx', None)
            if ctx is None:
                continue
                
            try:
                # è°ƒç”¨backwardå‡½æ•°
                grads = node._grad_fn.backward(ctx, node_grad)
                
                if grads is not None:
                    # ç¡®ä¿gradsæ˜¯å…ƒç»„
                    if not isinstance(grads, tuple):
                        grads = (grads,)
                    
                    # å°†æ¢¯åº¦åˆ†é…ç»™å­èŠ‚ç‚¹ï¼ˆè¾“å…¥ï¼‰
                    children = getattr(node, '_children', [])
                    for i, (child, grad) in enumerate(zip(children, grads)):
                        if child is None or grad is None:
                            continue
                            
                        # ç¡®ä¿gradæ˜¯Tensor
                        if not isinstance(grad, Tensor):
                            grad = Tensor(grad)
                        
                        # ç´¯ç§¯æ¢¯åº¦åˆ°all_gradså­—å…¸
                        child_id = id(child)
                        if child_id in all_grads:
                            all_grads[child_id] = all_grads[child_id] + grad
                        else:
                            all_grads[child_id] = grad
                            
            except Exception as e:
                print(f"åå‘ä¼ æ’­é”™è¯¯: {e}")

        
        # å°†ç´¯ç§¯çš„æ¢¯åº¦åˆ†é…ç»™éœ€è¦æ¢¯åº¦çš„å¼ é‡
        for node in topo:
            if node is None or not getattr(node, 'requires_grad', False):
                continue
                
            node_id = id(node)
            if node_id in all_grads:
                grad = all_grads[node_id]
                if node.grad is None:
                    node.grad = grad
                else:
                    node.grad = node.grad + grad

# åˆ›å»ºå…¨å±€å¼•æ“å®ä¾‹
_engine = BackwardEngine()

def backward(tensor, grad_tensor=None):
    """æ‰§è¡Œåå‘ä¼ æ’­
    
    Args:
        tensor: è¦è®¡ç®—æ¢¯åº¦çš„å¼ é‡
        grad_tensor: åˆå§‹æ¢¯åº¦
    """
    _engine.execute_backward(tensor, grad_tensor)

def enable_grad():
    """å¯ç”¨æ¢¯åº¦è®¡ç®—
    
    Returns:
        æ¢¯åº¦æ¨¡å¼å®ˆå«
    """
    return GradModeGuard(GradMode.TRAINING)

def no_grad():
    """ç¦ç”¨æ¢¯åº¦è®¡ç®—
    
    Returns:
        æ¢¯åº¦æ¨¡å¼å®ˆå«
    """
    return GradModeGuard(GradMode.INFERENCE)

class GradModeGuard:
    """æ¢¯åº¦æ¨¡å¼å®ˆå«
    
    ç”¨äºä¸´æ—¶åˆ‡æ¢æ¢¯åº¦æ¨¡å¼ï¼Œå¹¶åœ¨é€€å‡ºæ—¶æ¢å¤åŸæ¥çš„æ¨¡å¼ã€‚
    """
    __slots__ = ['prev_mode']
    
    def __init__(self, mode):
        """åˆå§‹åŒ–æ¢¯åº¦æ¨¡å¼å®ˆå«
        
        Args:
            mode: è¦åˆ‡æ¢åˆ°çš„æ¢¯åº¦æ¨¡å¼
        """
        self.prev_mode = _engine._grad_mode
        _engine._grad_mode = mode
    
    def __enter__(self):
        """è¿›å…¥ä¸Šä¸‹æ–‡"""
        pass
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """é€€å‡ºä¸Šä¸‹æ–‡ï¼Œæ¢å¤åŸæ¥çš„æ¢¯åº¦æ¨¡å¼"""
        _engine._grad_mode = self.prev_mode

class FunctionRegistry:
    """å‡½æ•°æ³¨å†Œè¡¨
    
    ç”¨äºæ³¨å†Œå’Œç®¡ç†è‡ªå®šä¹‰å‡½æ•°ã€‚
    """
    _custom_funcs = {}
    
    @classmethod
    def register(cls, name, forward, backward):
        """æ³¨å†Œè‡ªå®šä¹‰å‡½æ•°
        
        Args:
            name: å‡½æ•°åç§°
            forward: å‰å‘ä¼ æ’­å‡½æ•°
            backward: åå‘ä¼ æ’­å‡½æ•°
        """
        class CustomFunction(Function):
            @staticmethod
            def forward(ctx, *args, **kwargs):
                return forward(ctx, *args, **kwargs)
            
            @staticmethod
            def backward(ctx, *grad_outputs):
                return backward(ctx, *grad_outputs)
        
        CustomFunction.__name__ = name
        cls._custom_funcs[name] = CustomFunction
    
    @classmethod
    def get(cls, name):
        """è·å–è‡ªå®šä¹‰å‡½æ•°
        
        Args:
            name: å‡½æ•°åç§°
        
        Returns:
            æ³¨å†Œçš„è‡ªå®šä¹‰å‡½æ•°
        """
        return cls._custom_funcs[name]

def checkpoint(func, *args):
    """æ¢¯åº¦æ£€æŸ¥ç‚¹
    
    ç”¨äºå‡å°‘å†…å­˜ä½¿ç”¨ï¼Œé€šè¿‡åœ¨å‰å‘ä¼ æ’­æ—¶ä¸ä¿å­˜ä¸­é—´ç»“æœï¼Œ
    è€Œæ˜¯åœ¨åå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—ã€‚
    
    Args:
        func: è¦æ£€æŸ¥ç‚¹çš„å‡½æ•°
        *args: å‡½æ•°çš„å‚æ•°
    
    Returns:
        å‡½æ•°çš„è¾“å‡º
    """
    class CheckpointFunction(Function):
        @staticmethod
        def forward(ctx, func, *args):
            ctx.save_for_backward(func, *args)

        
        @staticmethod
        def backward(ctx, *grad_outputs):
            func, *args = ctx.saved_tensors
            return (None,) + _engine._compute_backward(func, *args, *grad_outputs)
    
    return CheckpointFunction.apply(func, *args)

# === Debug control functions ===
def set_debug_mode(enabled=True):
    """è®¾ç½®è°ƒè¯•æ¨¡å¼
    
    Args:
        enabled: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
    """
    global DEBUG_AUTOGRAD
    DEBUG_AUTOGRAD = enabled
    _engine._debug = enabled

def get_debug_mode():
    """è·å–è°ƒè¯•æ¨¡å¼çŠ¶æ€
    
    Returns:
        bool: è°ƒè¯•æ¨¡å¼æ˜¯å¦å¯ç”¨
    """
    return DEBUG_AUTOGRAD

class MatMul(Function):
    @staticmethod
    def forward(ctx, a, b):
        from .tensor_T import Tensor
        ctx.save_for_backward(a, b)  # å…ˆä¿å­˜ Tensor
        
        def extract_data_for_matmul(tensor):
            if hasattr(tensor, 'data'):
                data = tensor.data
                if hasattr(data, '_flat_data') and hasattr(data, 'shape'):
                    from . import arrays
                    try:
                        # ç‰¹æ®Šæƒ…å†µï¼šæ£€æŸ¥æ˜¯å¦æ˜¯ç©ºæ•°ç»„
                        if 0 in data.shape or len(data._flat_data) == 0:
                            print(f"ğŸš¨ å‘ç°ç©ºæ•°ç»„: shape={data.shape}, _flat_dataé•¿åº¦={len(data._flat_data)}")
                            # åˆ›å»ºæ­£ç¡®å½¢çŠ¶çš„é›¶æ•°ç»„
                            zeros = arrays.zeros(data.shape)
                            return zeros.data
                        
                        arrays_obj = arrays.Array(data.tolist())
                        return arrays_obj.data
                    except:
                        flat_data = data._flat_data
                        shape = data.shape
                        
                        # å¤„ç†ç©ºæ•°ç»„æƒ…å†µ
                        if 0 in shape or len(flat_data) == 0:
                            print(f"ğŸš¨ å¤„ç†å¼‚å¸¸æ—¶å‘ç°ç©ºæ•°ç»„: shape={shape}, flat_dataé•¿åº¦={len(flat_data)}")
                            from . import arrays
                            zeros = arrays.zeros(shape)
                            return zeros.data
                        
                        from . import arrays
                        as_compatible = arrays.asarray_numpy_compatible(flat_data, dtype='float')
                        return as_compatible.data.reshape(shape)
                else:
                    # å…¶ä»–ç±»å‹çš„dataï¼Œç›´æ¥è¿”å›
                    return data
            else:
                return tensor
        
        a_data = extract_data_for_matmul(a)
        b_data = extract_data_for_matmul(b)
        
        # æ·»åŠ æ•°æ®æå–çš„è°ƒè¯•ä¿¡æ¯
        
        if hasattr(a_data, 'shape') and hasattr(b_data, 'shape'):
            # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºæ•°ç»„å‚ä¸è¿ç®—
            if 0 in a_data.shape or 0 in b_data.shape:
                print(f"ğŸš¨ æ£€æµ‹åˆ°ç©ºæ•°ç»„! a_data.shape: {a_data.shape}, b_data.shape: {b_data.shape}")
                # å¯¹äºç©ºæ•°ç»„çš„matmulï¼Œè¿”å›é€‚å½“å½¢çŠ¶çš„é›¶æ•°ç»„
                if len(a_data.shape) == 2 and len(b_data.shape) == 2:
                    result_shape = (a_data.shape[0], b_data.shape[1])
                    print(f"ğŸš¨ è¿”å›é›¶æ•°ç»„ï¼Œå½¢çŠ¶: {result_shape}")
                    from . import arrays
                    zeros = arrays.zeros(result_shape)
                    print(f"ğŸš¨ zeros.data: {zeros.data}")
                    return Tensor(zeros.data, requires_grad=a.requires_grad or b.requires_grad)
                else:
                    # å…¶ä»–ç»´åº¦æƒ…å†µï¼Œè¿”å›ç©ºçš„ç»“æœ
                    print(f"ğŸš¨ è¿”å›ç©ºç»“æœ (0,0)")
                    from . import arrays
                    zeros = arrays.zeros((0, 0))
                    return Tensor(zeros.data, requires_grad=a.requires_grad or b.requires_grad)
        
        # æ‰§è¡ŒçŸ©é˜µä¹˜æ³•
        try:
            
            #result = strong_mat.smart_matmul(a_data, b_data)
            result = strong_matmul.perfect_matmul(a_data, b_data)
            

            
            # å®‰å…¨åœ°æå–ç»“æœæ•°æ®
            if hasattr(result, 'data'):
                result_data = result.data
                
                # ç¡®ä¿result_dataæ˜¯å…¼å®¹çš„æ ¼å¼
                if isinstance(result_data, list):
                    # å¦‚æœæ˜¯listï¼Œä½¿ç”¨arrays.asarray_numpy_compatibleè¿›è¡Œè½¬æ¢
                    from . import arrays
                    try:
                        compatible_data = arrays.asarray_numpy_compatible(result_data)
                        final_data = compatible_data.data
                        return Tensor(final_data, requires_grad=a.requires_grad or b.requires_grad)
                    except Exception as conv_error:
                        print(f"   æ•°æ®è½¬æ¢å¤±è´¥: {conv_error}")
                        # ä½¿ç”¨é»˜è®¤çš„é›¶æ•°ç»„
                        zeros = arrays.zeros((1, 1))
                        return Tensor(zeros.data, requires_grad=a.requires_grad or b.requires_grad)
                else:
                    return Tensor(result_data, requires_grad=a.requires_grad or b.requires_grad)
            else:
                result_data = result
                print(f"   ç›´æ¥ä½¿ç”¨resultä½œä¸ºresult_dataï¼Œç±»å‹: {type(result_data)}")
                
                # å¦‚æœresultæœ¬èº«æ˜¯listæˆ–å…¶ä»–éœ€è¦è½¬æ¢çš„ç±»å‹
                if isinstance(result_data, list):
                    from . import arrays
                    try:
                        compatible_data = arrays.asarray_numpy_compatible(result_data)
                        final_data = compatible_data.data
                        return Tensor(final_data, requires_grad=a.requires_grad or b.requires_grad)
                    except Exception as conv_error:
                        print(f"   æ•°æ®è½¬æ¢å¤±è´¥: {conv_error}")
                        # ä½¿ç”¨é»˜è®¤çš„é›¶æ•°ç»„
                        zeros = arrays.zeros((1, 1))
                        return Tensor(zeros.data, requires_grad=a.requires_grad or b.requires_grad)
                else:
                    return Tensor(result_data, requires_grad=a.requires_grad or b.requires_grad)
        except Exception as e:

            if hasattr(a_data, '__len__') and len(str(a_data)) < 300:
                print(f"ğŸš¨ A æ•°æ®å†…å®¹: {a_data}")
            if hasattr(b_data, '__len__') and len(str(b_data)) < 300:
                print(f"ğŸš¨ B æ•°æ®å†…å®¹: {b_data}")
            # è¿”å›ä¸€ä¸ªåˆç†çš„é»˜è®¤å€¼
            from . import arrays
            zeros = arrays.zeros((1, 1))
            print(f"ğŸš¨ è¿”å›é»˜è®¤é›¶å¼ é‡: {zeros.data}")
            return Tensor(zeros.data, requires_grad=a.requires_grad or b.requires_grad)

    @staticmethod
    def backward(ctx, grad_output):
        # è·å–ä¿å­˜çš„è¾“å…¥
        a, b = ctx.saved_tensors
        
        # å¯¼å…¥arraysç”¨äºå®‰å…¨çš„è½¬ç½®æ“ä½œ
        from . import arrays
        
        def smart_transpose(data):
            """æ™ºèƒ½è½¬ç½®ï¼Œå¤„ç†ä¸åŒç»´åº¦çš„æƒ…å†µï¼Œå®Œå…¨ä½¿ç”¨arraysæ¨¡å—"""
            try:
                if hasattr(data, 'shape'):
                    if len(data.shape) == 2:
                        # æ ‡å‡†2DçŸ©é˜µè½¬ç½®
                        transposed = arrays.transpose(arrays.Array(data))
                        if hasattr(transposed, 'data'):
                            result = transposed.data
                        else:
                            result = transposed
                        
                        # ç¡®ä¿ç»“æœæœ‰shapeå±æ€§
                        if not hasattr(result, 'shape'):
                            # ä½¿ç”¨arraysé‡å»ºæ•°ç»„
                            result_array = arrays.asarray_numpy_compatible(result)
                            result = result_array.data.reshape(data.shape[1], data.shape[0])
                        
                        return result
                    elif len(data.shape) == 1:
                        # 1Då‘é‡ï¼Œè½¬æ¢ä¸ºåˆ—å‘é‡ç„¶åè½¬ç½®
                        reshaped = data.reshape(-1, 1)
                        transposed = arrays.transpose(arrays.Array(reshaped))
                        if hasattr(transposed, 'data'):
                            result = transposed.data
                        else:
                            result = transposed
                        
                        # ç¡®ä¿ç»“æœæœ‰shapeå±æ€§
                        if not hasattr(result, 'shape'):
                            result_array = arrays.asarray_numpy_compatible(result)
                            result = result_array.data.reshape(1, -1)
                        
                        return result
                    else:
                        print(f"  ä¸æ”¯æŒçš„ç»´åº¦æ•°: {len(data.shape)}")
                        return data
                else:
                    print(f"  æ•°æ®æ²¡æœ‰shapeå±æ€§")
                    return data
            except Exception as e:
                print(f"  è½¬ç½®å¤±è´¥: {e}")
                # è¿”å›åŸæ•°æ®çš„è½¬ç½®ç‰ˆæœ¬ï¼ˆä½¿ç”¨arraysï¼‰
                try:
                    if hasattr(data, 'shape') and len(data.shape) == 2:
                        result = arrays.transpose(arrays.Array(data))
                        if hasattr(result, 'data'):
                            return result.data
                        else:
                            result_array = arrays.asarray_numpy_compatible(result)
                            return result_array.data
                except:
                    pass
                return data
        
        def smart_matmul_fixed(x, y, operation_name=""):
            try:
                #result = strong_mat.smart_matmul(x, y)
                result = strong_matmul.perfect_matmul(x, y)
                
                # æå–ç»“æœæ•°æ®
                if hasattr(result, 'data'):
                    return result.data
                else:
                    return result
                
            except Exception as e:
                print(f"  {operation_name}å¼‚å¸¸: {e}")
                print(f"  å½¢çŠ¶: {getattr(x, 'shape', 'No shape')} @ {getattr(y, 'shape', 'No shape')}")
                return None
        
        # === å…³é”®ä¿®å¤ï¼šé’ˆå¯¹ä¸åŒçŸ©é˜µä¹˜æ³•ç±»å‹çš„æ­£ç¡®æ¢¯åº¦è®¡ç®—ï¼Œå®Œå…¨ä½¿ç”¨arrays ===
        
        # è·å–å½¢çŠ¶ä¿¡æ¯
        a_shape = getattr(a.data, 'shape', ())
        b_shape = getattr(b.data, 'shape', ())
        grad_shape = getattr(grad_output.data, 'shape', ())
        
        grad_a = None
        grad_b = None
        
        # æƒ…å†µ1: 2D @ 2D
        if len(a_shape) == 2 and len(b_shape) == 2:
            b_t = smart_transpose(b.data)
            grad_a = smart_matmul_fixed(grad_output.data, b_t, "grad_a(2D@2D)")
            
            a_t = smart_transpose(a.data)
            grad_b = smart_matmul_fixed(a_t, grad_output.data, "grad_b(2D@2D)")
        
        # æƒ…å†µ2: 2D @ 1D (çŸ©é˜µ Ã— å‘é‡ = å‘é‡)
        elif len(a_shape) == 2 and len(b_shape) == 1:
            # grad_A = grad_output.reshape(-1, 1) @ B.reshape(1, -1)
            grad_out_reshaped = grad_output.data.reshape(-1, 1)  # (n,) -> (n, 1)
            b_reshaped = b.data.reshape(1, -1)  # (m,) -> (1, m)
            
            # ä½¿ç”¨arraysæ¨¡å—è¿›è¡ŒçŸ©é˜µä¹˜æ³•
            grad_out_array = arrays.Array(grad_out_reshaped)
            b_reshaped_array = arrays.Array(b_reshaped)
            grad_a_result = arrays.matmul(grad_out_array, b_reshaped_array)
            if hasattr(grad_a_result, 'data'):
                grad_a = grad_a_result.data
            else:
                grad_a_array = arrays.asarray_numpy_compatible(grad_a_result)
                grad_a = grad_a_array.data
            
            # grad_B = A.T @ grad_output
            a_t = smart_transpose(a.data)
            grad_b = smart_matmul_fixed(a_t, grad_output.data, "grad_b(2D@1D)")
        
        # æƒ…å†µ3: 1D @ 2D (å‘é‡ Ã— çŸ©é˜µ = å‘é‡)
        elif len(a_shape) == 1 and len(b_shape) == 2:
            # grad_A = grad_output @ B.T
            b_t = smart_transpose(b.data)
            grad_a = smart_matmul_fixed(grad_output.data, b_t, "grad_a(1D@2D)")
            
            # grad_B = A.reshape(-1, 1) @ grad_output.reshape(1, -1)
            a_reshaped = a.data.reshape(-1, 1)  # (n,) -> (n, 1)
            grad_out_reshaped = grad_output.data.reshape(1, -1)  # (m,) -> (1, m)
            
            # ä½¿ç”¨arraysæ¨¡å—è¿›è¡ŒçŸ©é˜µä¹˜æ³•
            a_reshaped_array = arrays.Array(a_reshaped)
            grad_out_array = arrays.Array(grad_out_reshaped)
            grad_b_result = arrays.matmul(a_reshaped_array, grad_out_array)
            if hasattr(grad_b_result, 'data'):
                grad_b = grad_b_result.data
            else:
                grad_b_array = arrays.asarray_numpy_compatible(grad_b_result)
                grad_b = grad_b_array.data
        
        # æƒ…å†µ4: 1D @ 1D (å‘é‡ç‚¹ç§¯ = æ ‡é‡)
        elif len(a_shape) == 1 and len(b_shape) == 1:
            # å¯¹äºç‚¹ç§¯ï¼Œæ¢¯åº¦å°±æ˜¯å¯¹æ–¹å‘é‡ä¹˜ä»¥æ ‡é‡æ¢¯åº¦
            grad_a = grad_output.data * b.data  # æ ‡é‡ * å‘é‡ = å‘é‡
            grad_b = grad_output.data * a.data  # æ ‡é‡ * å‘é‡ = å‘é‡
        
        return grad_a, grad_b

# åœ¨ Tensor ç±»ä¸­æ·»åŠ  matmul æ–¹æ³•
def matmul(self, other):
    """çŸ©é˜µä¹˜æ³•"""
    from .tensor_T import Tensor
    if not isinstance(other, Tensor):
        other = Tensor(other)
    return MatMul.apply(self, other)

__all__ = [
    'Function', 'backward', 'no_grad', 'enable_grad',
    'checkpoint', 'FunctionRegistry'
]

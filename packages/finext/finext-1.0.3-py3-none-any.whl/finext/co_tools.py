#co_tools_beta.py
from finlab import data
import finlab
import pickle
import os
import plotly.express as px
import plotly.graph_objects as go
from finlab.backtest import sim
from finlab.tools.event_study import create_factor_data
import tqdm
import numpy as np 
import pandas as pd
from finlab.dataframe import FinlabDataFrame
import cufflinks as cf
from sklearn.linear_model import LinearRegression
from datetime import datetime
from IPython.display import display, HTML
import time
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from typing import List, Tuple, Dict
import fitz  # PyMuPDF

# CatBoost ç›¸é—œå¥—ä»¶
from catboost import CatBoostRegressor, CatBoostClassifier
import matplotlib.pyplot as plt
import seaborn as sns

db_path = "/home/sb0487/trade/finlab/finlab_db" #è³‡æ–™å„²å­˜è·¯å¾‘



"""
ç¨‹å¼ç¢¼å‚·çœ¼æ»²å…¥


"""
#è‹¥æœªä¾†ç›¸é—œå‡½å¼å¢å¤šå†é–‹ç™¼
class cofindf(FinlabDataFrame):
    @property
    def pr(self):
        # è¨ˆç®—æ¯è¡Œçš„æœ‰æ•ˆå€¼æ•¸é‡
        valid_counts = self.count(axis=1)
        valid_counts = valid_counts.replace(0, np.nan)
        rank_df = self.rank(axis=1, ascending=True, na_option='keep')
        pr_df = rank_df.div(valid_counts, axis=0) * 100
        return pr_df


#è¼‰å…¥å€----------------------------------------------------------------------------------------------------------------

class Codata():
    def __init__(self, df_type="findf", db_path="", force_download=False, 
                 html_file="tmp_finlab_report.html",
                 image_file_1="tmp_finlab_report_img1.png", 
                 image_file_2="tmp_finlab_report_img2.png"):
        # super().__init__()
        self.df_type = df_type
        self.db_path = db_path
        data.set_storage(data.FileStorage(db_path))
        data.use_local_data_only = False
        data.force_cloud_download = force_download

        #HTMLèˆ‡åœ–ç‰‡æš«å­˜
        self.html_file = html_file
        self.image_file_1 = image_file_1
        self.image_file_2 = image_file_2
    
    def get_file_path(self,file_name): 
        return os.path.join(self.db_path, file_name.replace(":", "#") + ".pickle")

    
    def get_update_time(self,filename):
        if os.path.exists(self.get_file_path(filename)):
            modification_time = os.path.getmtime(self.get_file_path(filename))
            last_modified = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(modification_time))
            print(f"æœ€å¾Œæ›´æ–°æ™‚é–“: {last_modified}, {[filename]}")
        else:
            print("æª”æ¡ˆä¸å­˜åœ¨,è«‹æŸ¬æŸ¥è·¯å¾‘")

    
    def ouput_type_df(self,file_df):
        if self.df_type == "findf":
            type_df = file_df
        elif self.df_type == "cudf":
            import cudf
            type_df = cudf.DataFrame(file_df)
        elif self.df_type == "sparkdf":
            from pyspark.sql import SparkSession
            spark = SparkSession.builder.appName("Pandas to Spark DataFrame").getOrCreate()
            type_df = spark.createDataFrame(file_df)
        return type_df


    def get(self, file_name, force_download = False ):
        if not os.path.isdir(self.db_path):
            raise OSError("è³‡æ–™å¤¾è·¯å¾‘éŒ¯èª¤")

        if force_download == True:
            data.force_cloud_download = True 
            type_df = data.get(file_name)
            data.force_cloud_download = False
        else:
            type_df = data.get(file_name)
            
        #é¸æ“‡dfè¼¸å‡ºå‹æ…‹
        type_df = self.ouput_type_df(type_df)
        self.get_update_time(file_name)
        return type_df

    # @staticmethod
    # def get_update_time(filename):
    #     data.get_update_time(filename)  # è°ƒç”¨ data ç±»çš„ get_update_time æ–¹æ³•

    

#ç”¢æ¥­å€----------------------------------------------------------------------------------------------------------------
    
    #æŠŠcategoryæ‹†æˆä¸»åˆ†é¡èˆ‡ç´°åˆ†é¡
    def get_industry_pro(self):
        industry = self.get('security_industry_themes').dropna()
        def extract_majcategory(category_list):
            matching_categories = set([category.split(':')[0] for category in eval(category_list) if ':' in category])
            return str(list(matching_categories))
        
        def extract_subcategory(category_list):
            matching_categories = [category.split(':')[-1] for category in eval(category_list) if ':' in category]
            return str(matching_categories)
        
        # åº”ç”¨è‡ªå®šä¹‰å‡½æ•°åˆ° DataFrame çš„æ¯ä¸€è¡Œ
        industry['maj_category'] = industry['category'].apply(extract_majcategory)
        industry['sub_category'] = industry['category'].apply(extract_subcategory)
        
        return industry

    
    def show_industry(self):
        industry = self.get_industry_pro()
        sub_category_counts_df = pd.DataFrame(industry['sub_category'].apply(eval).explode('sub_category').value_counts()).reset_index()
        maj_category_counts_df = pd.DataFrame(industry['maj_category'].apply(eval).explode('maj_category').value_counts()).reset_index()
        
        industry["maj_category"] = industry["maj_category"].apply(eval)
        industry["sub_category"] = industry["sub_category"].apply(eval)
        industry_explode = industry.explode('maj_category').explode('sub_category')
        industry_explode["count"] = 1
        
        fig = px.treemap(industry_explode, path=[px.Constant("å°è‚¡ç”¢æ¥­ç¸½ç¸½è¦½"), "maj_category", "sub_category","name"], values='count')
        fig.update_layout(
            margin=dict(t=1, l=1, r=1, b=1)
        )
        
        fig.show()
        return maj_category_counts_df,sub_category_counts_df
    
    def filter_industry(self,file_df, keyword_list, category_type = "maj_category", remove_or_add="remove", exact_or_fuzzy="fuzzy"):
        industry_pro = self.get_industry_pro()
        
        if exact_or_fuzzy == "fuzzy":
            if remove_or_add == "remove":
                
                file_filtered_df = (file_df
                    .loc[:, ~file_df.columns.isin(
                        industry_pro[industry_pro[category_type]
                        .apply(lambda x: bool(set(eval(x)) & set(keyword_list)))]['stock_id']
                        .tolist())]
                )
           
            elif remove_or_add == "add":
                file_filtered_df = (file_df
                    .loc[:, file_df.columns.isin(
                        industry_pro[industry_pro[category_type]
                        .apply(lambda x: bool(set(eval(x)) & set(keyword_list)))]['stock_id']
                        .tolist())]
                )
    
        
        if exact_or_fuzzy == "exact":
            if remove_or_add == "remove": # å®Œå…¨ä¸€æ¨£æ‰ç§»é™¤
                
                file_filtered_df = (file_df
                    .loc[:, ~file_df.columns.isin(
                        industry_pro[industry_pro[category_type]
                        .apply(lambda x: bool(set(eval(x)) == set(keyword_list)))]['stock_id']
                        .tolist())]
                )
    
            elif remove_or_add == "add": # å®Œå…¨ä¸€æ¨£æ‰åŠ å…¥
                file_filtered_df = (file_df
                    .loc[:, file_df.columns.isin(
                        industry_pro[industry_pro[category_type]
                        .apply(lambda x: bool(set(eval(x)) == set(keyword_list)))]['stock_id']
                        .tolist())]
                )
        
        return file_filtered_df



    
    
    #æŠŠcategoryæ‹†æˆä¸»åˆ†é¡èˆ‡ç´°åˆ†é¡
    def get_industry_pro(self):
        industry = self.get('security_industry_themes').dropna()
        def extract_majcategory(category_list):
            matching_categories = set([category.split(':')[0] for category in eval(category_list) if ':' in category])
            return str(list(matching_categories))
        
        def extract_subcategory(category_list):
            matching_categories = [category.split(':')[-1] for category in eval(category_list) if ':' in category]
            return str(matching_categories)
        
        # åº”ç”¨è‡ªå®šä¹‰å‡½æ•°åˆ° DataFrame çš„æ¯ä¸€è¡Œ
        industry['maj_category'] = industry['category'].apply(extract_majcategory)
        industry['sub_category'] = industry['category'].apply(extract_subcategory)
        
        return industry

#ä¾¿åˆ©å·¥å…·å€----------------------------------------------------------------------------------------------------------------

        # def month_forward_sell(self,forward_days = 1):
        #     exits_df = self.get('price:æ”¶ç›¤åƒ¹')<0
        #     def update_row(row):
        #         if row.name in self.monthly_revenue.index:
        #             return True
        #         else:
        #             return row
        
        #     rev_date = exits_df.apply(update_row, axis=1)
        #     rev_date_shifted = rev_date.shift(-1)
        #     for i in range(1,forward_days+1):
        #         rev_date_shifted_n = rev_date.shift(-i)
        #         rev_date_shifted = rev_date_shifted  | rev_date_shifted_n
                
        return rev_date_shifted
    
    #æŠŠæ—¥è³‡æ–™è½‰æˆæœˆè³‡æ–™(ç‡Ÿæ”¶ç™¼å¸ƒæˆªæ­¢æ—¥),ä»–å€‘æœ‰èªªä¹‹å¾Œæœƒæ”¹æˆé›»å­æª”ä¸Šå‚³æ—¥
    def day_to_month(self,file_df):
        monthly_index_df = FinlabDataFrame(index=self.get("monthly_revenue:ç•¶æœˆç‡Ÿæ”¶").index)
        file_df  = monthly_index_df.join(file_df, how='left')
        return file_df

    def to_day(self,file_df):
        monthly_index_df = FinlabDataFrame(index=self.get('price:æ”¶ç›¤åƒ¹').index)
        file_df  = monthly_index_df.join(file_df, how='left')
        return file_df
    
    #è½‰ç‚ºæ—¥è³‡æ–™ä¸¦è—‰ç”±è³‡æ–™ç•°å‹•æ™‚é–“é»ä¿ç•™è²¡å ±ç™¼å¸ƒæ—¥è³‡è¨Š(index_str_to_dateæœƒå‘ä¸‹å¡«æ»¿)    
    def q_to_day(self,file_df):
        file_df =file_df.index_str_to_date()
        file_df =file_df.where(file_df.ne(file_df.shift()), np.nan)
        day_index_df = FinlabDataFrame(index=self.get('price:æ”¶ç›¤åƒ¹').index)
        c = pd.concat([file_df,day_index_df])
        file_df = FinlabDataFrame(c[~c.index.duplicated()].sort_index())
        return file_df

    def q_to_weekday(self,quarter_df):
        """
        æ›´å¿«é€Ÿçš„ç‰ˆæœ¬ï¼Œä½¿ç”¨pandaså…§å»ºæ–¹æ³•
        """
        # å–å¾—æ”¶ç›¤åƒ¹æ•¸æ“šä¸¦å‰µå»ºå¹³æ—¥index
        close = self.get('price:æ”¶ç›¤åƒ¹')
        close_df = close < 0  # å‰µå»ºbooleanæ•¸æ“šæ¡†ï¼Œä¿ç•™å¹³æ—¥index
        
        # è™•ç†quarter_dfçš„indexå’Œæ•¸æ“š
        quarter_df = quarter_df.index_str_to_date()
        quarter_df = quarter_df.where(quarter_df.ne(quarter_df.shift()), np.nan)
        
        # ç¢ºä¿indexæ˜¯datetimeæ ¼å¼ä¸¦æ’åº
        close_df = close_df.copy()
        quarter_df.index = pd.to_datetime(quarter_df.index)
        close_df.index = pd.to_datetime(close_df.index)
        
        # å‰µå»ºçµæœæ•¸æ“šæ¡†
        result_df = pd.DataFrame(
            index=close_df.index,
            columns=quarter_df.columns,
            dtype=float
        )
        
        # å°æ¯å€‹quarterçš„æ—¥æœŸï¼Œæ‰¾åˆ°close_dfä¸­æœ€è¿‘çš„è¼ƒå°æ—¥æœŸ
        for quarter_date in quarter_df.index:
            # æ‰¾åˆ°è©²æ—¥æœŸå°æ‡‰çš„å·¥ä½œæ—¥ï¼ˆæœ€è¿‘çš„è¼ƒå°æˆ–ç­‰æ–¼çš„å·¥ä½œæ—¥ï¼‰
            target_date = close_df.index[close_df.index <= quarter_date]
            if len(target_date) > 0:
                target_date = target_date[-1]  # å–æœ€è¿‘çš„
                result_df.loc[target_date] = quarter_df.loc[quarter_date]
        
        # ç”¨closeçš„indexé‡æ–°ç´¢å¼•çµæœ
        result_df = result_df.reindex(close.index)[quarter_df.index[0]:]
        
        return result_df


    def get_pr(self,file_df):
        #åˆ¤æ–·æ˜¯å¦ç‚ºå­£è³‡æ–™
        text = file_df.index[1]  # å‡è¨­ roa.index[0] æ˜¯ '2013-Q1'
        try:
            if 'Q' in text:
                print("finlab 1.2.27ç‰ˆæœ¬ä»¥å¾Œå­£è³‡æ–™rankæœƒè½‰æˆæ—¥æœŸï¼Œget_präº¦åŒ")
        except:
            pass
        # è¨ˆç®—æ¯è¡Œçš„æœ‰æ•ˆå€¼æ•¸é‡
        rank_df = file_df.rank(axis=1, ascending=True, na_option='keep')
        valid_counts = rank_df.count(axis=1)
        valid_counts[valid_counts == 0] = np.nan
        pr_df = rank_df.div(valid_counts, axis=0) * 100
        
        return pr_df
   
    def display_report_statis(self, file_df):
        mean_return = file_df["return"].mean()
        return_std = file_df["return"].std()
        mean_period = file_df["period"].mean()
        
        # è¤‡åˆ©å ±é…¬ç‡è¨ˆç®—
        log_return_mean = mean_return - 0.5 * (return_std ** 2)
        periods_per_year = 240 / mean_period
        annual_compound_return = (1 + log_return_mean) ** periods_per_year - 1
        
        # è¨ˆç®—å‹ç‡
        win_rate = (file_df["return"] > 0).mean()
        
        # çµ„æˆ JSON è³‡æ–™ (å°æ•¸ä½æ•¸æ¯”ç…§ HTML)
        stats_json = {
            "äº¤æ˜“ç­†æ•¸": len(file_df),
            "å¹³å‡å ±é…¬ç‡": f"{mean_return * 100:.2f}%",  # 3ä½å°æ•¸è½‰ç™¾åˆ†æ¯” = 1ä½å°æ•¸
            "å¹³å‡MDD": f"{file_df['mdd'].mean():.3f}",
            "å ±é…¬ç‡æ¨™æº–å·®": f"{return_std:.3f}",
            "å¹³å‡æŒæœ‰æœŸé–“(äº¤æ˜“æ—¥)": f"{mean_period:.3f}",
            "å‹ç‡": f"{win_rate * 100:.2f}%",  # 3ä½å°æ•¸è½‰ç™¾åˆ†æ¯” = 1ä½å°æ•¸
            "æœ€å¤§å¹´åŒ–å ±é…¬ç‡(æ³¢å‹•èª¿æ•´_æ³°å‹’å±•é–‹)": f"{annual_compound_return * 100:.2f}%"  # 3ä½å°æ•¸è½‰ç™¾åˆ†æ¯” = 1ä½å°æ•¸
        }
        
        html_content = """
        <sorry style="font-size: larger;">äº¤æ˜“çµ±è¨ˆ</sorry>
        <ul>
          <li>äº¤æ˜“ç­†æ•¸: {}</li>
          <li>å¹³å‡å ±é…¬ç‡: {}</li>
          <li>å¹³å‡MDD: {}</li>
          <li>å ±é…¬ç‡æ¨™æº–å·®: {}</li>
          <li>å¹³å‡æŒæœ‰æœŸé–“(äº¤æ˜“æ—¥): {}</li>
          <li>å‹ç‡: {}</li>
          <li>æœ€å¤§å¹´åŒ–å ±é…¬ç‡(æ³¢å‹•èª¿æ•´_æ³°å‹’å±•é–‹): {}</li>
        </ul>
        """.format(len(file_df),
                   stats_json["å¹³å‡å ±é…¬ç‡"],
                   stats_json["å¹³å‡MDD"],
                   stats_json["å ±é…¬ç‡æ¨™æº–å·®"],
                   stats_json["å¹³å‡æŒæœ‰æœŸé–“(äº¤æ˜“æ—¥)"],
                   stats_json["å‹ç‡"],
                   stats_json["æœ€å¤§å¹´åŒ–å ±é…¬ç‡(æ³¢å‹•èª¿æ•´_æ³°å‹’å±•é–‹)"])
        
        display(HTML(html_content))
        return stats_json
#çˆ¬èŸ²å€----------------------------------------------------------------------------------------------------------------------
    
    #çˆ¬å¹´å ±
    def crawl_annual_report_(self,year,symbol,save_dir,sleep = 2):
        #init
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # ç„¡é ­æ¨¡å¼
        year = str(year)
        symbol = str(symbol)
        
    
        d = webdriver.Chrome(options=chrome_options)
        d.maximize_window()
    
        try:
            while True:
                d.get(f'https://doc.twse.com.tw/server-java/t57sb01?step=1&colorchg=1&co_id={symbol}&year={year}&mtype=F&dtype=F04&') 
                time.sleep(sleep)
                
                page_content = d.page_source
                if "æŸ¥è©¢éé‡" in page_content:
                    print(f"ç•¶å‰è‚¡ç¥¨ç‚º{symbol},æŸ¥è©¢éé‡ï¼Œè¢«è­‰äº¤æ‰€æª”ä¸‹ï¼Œä¼‘æ¯10ç§’")
                    time.sleep(10)
                    continue  
                else:
                    break  # å¦‚æœæ²¡æœ‰æŸ¥è©¢éé‡ï¼Œé€€å‡ºå¾ªç¯
    
            pdf_link = d.find_element(By.XPATH, "//a[contains(@href, 'javascript:readfile2') and contains(@href, 'F04')]")
            pdf_link.click()
            time.sleep(sleep)
        
            # åˆ‡æ›åˆ†é 
            all_tabs = d.window_handles
            d.switch_to.window(all_tabs[1])
            time.sleep(sleep)
    
            
            # æ‰¾åˆ°pdfé€£çµ,æ³¨æ„æ­¤é€£çµç‚ºä¸å®šæ™‚æµ®å‹•
            pdf_link2 = d.find_element(By.XPATH, "//a[contains(@href, '.pdf')]")
            pdf_url = pdf_link2.get_attribute('href')
        
            
            # å»ºæ§‹dir(è‹¥ç„¡),ä¿å­˜pdf
            os.makedirs(save_dir, exist_ok=True)
            file_name = f"{year}_{symbol}.pdf" 
            file_path = os.path.join(save_dir, file_name)
            
            # ä¸‹è½½ PDF æ–‡ä»¶å¹¶ä¿å­˜
            response = requests.get(pdf_url)
            with open(file_path, "wb") as file:
                file.write(response.content)
            print(f"PDF æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")
            failed_symbol =None
            
        except ModuleNotFoundError as e:
            print(f"Module not found error: {e}")
            
        except NameError as e:
            print(f"Name not defined error: {e}") 
            
        except Exception as e:
            print(f"{symbol}_{year}å¹´å¹´å ±æœªæ‰¾åˆ°")
            failed_symbol = symbol
            
        finally:
            d.quit()
            
        return failed_symbol
    #çˆ¬å¹´å ±,å¤šå€‹
    def crawl_annual_reports(self,year,stock_list,save_dir,sleep = 2):
        failed_list = list(filter(None, (self.crawl_annual_report_(year, x, save_dir, sleep) for x in stock_list)))
        return failed_list
        
    #çˆ¬å­£å ±
    def crawl_quarterly_report_(self,year,quarter,symbol,save_dir,sleep = 2):
        
        #init
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # ç„¡é ­æ¨¡å¼
        year = str(year)
        symbol = str(symbol)
        format_quarter = "0"+str(quarter)
        ad = str(int(year)+1911)
        # åˆå§‹åŒ–Chromeç€è¦½å™¨
        # driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        
    
        d = webdriver.Chrome(options=chrome_options)
        d.maximize_window()
    
        try:
            while True:
                d.get(f'https://doc.twse.com.tw/server-java/t57sb01?step=1&colorchg=1&co_id={symbol}&year={year}&seamon=&mtype=A&dtype=AI1&') 
                time.sleep(sleep)
                
                page_content = d.page_source
                if "æŸ¥è©¢éé‡" in page_content:
                    print(f"ç•¶å‰è‚¡ç¥¨ç‚º{symbol},æŸ¥è©¢éé‡ï¼Œè¢«è­‰äº¤æ‰€æª”ä¸‹ï¼Œä¼‘æ¯10ç§’")
                    time.sleep(10)
                    continue  
                else:
                    break  # å¦‚æœæ²¡æœ‰æŸ¥è©¢éé‡ï¼Œé€€å‡ºå¾ªç¯
           
            pdf_name = f"{ad}{format_quarter}_{symbol}_AI1.pdf"
            pdf_link = d.find_element(By.XPATH, f"//a[contains(@href, 'javascript:readfile2') and contains(@href,'{pdf_name}')]")
            pdf_link.click()
            time.sleep(sleep)
        
            # åˆ‡æ›åˆ†é 
            all_tabs = d.window_handles
            d.switch_to.window(all_tabs[1])
            time.sleep(sleep)
    
            # æ‰¾åˆ°pdfé€£çµ,æ³¨æ„æ­¤é€£çµç‚ºä¸å®šæ™‚æµ®å‹•
            pdf_link2 = d.find_element(By.XPATH, "//a[contains(@href, '.pdf')]")
            pdf_url = pdf_link2.get_attribute('href')
        
            
            # å»ºæ§‹dir(è‹¥ç„¡),ä¿å­˜pdf
            os.makedirs(save_dir, exist_ok=True)
            file_name = f"{year}_Q{quarter}_{symbol}.pdf" 
            file_path = os.path.join(save_dir, file_name)
            
            # ä¸‹è½½ PDF æ–‡ä»¶å¹¶ä¿å­˜
            response = requests.get(pdf_url)
            with open(file_path, "wb") as file:
                file.write(response.content)
            print(f"PDF æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")
            failed_symbol =None
            
        except ModuleNotFoundError as e:
            print(f"Module not found error: {e}")
            
        except NameError as e:
            print(f"Name not defined error: {e}") 
        
        except:
            print(f"{symbol}_{year}_Q{quarter}å­£å ±æœªæ‰¾åˆ°")
            failed_symbol = symbol
            
        finally:
            d.quit()
        return failed_symbol
    #çˆ¬å­£å ±,å¤šå€‹
    def crawl_quarterly_reports(self,year,quarter,stock_list,save_dir,sleep = 2):
        failed_list = list(filter(None, (self.crawl_quarterly_report_(year,quarter, x, save_dir, sleep) for x in stock_list)))
        return failed_list


    #ç”¨save_diræŠ“ä¸‹ä¾†çš„æª”æ¡ˆèˆ‡å…¨éƒ¨çš„è‚¡ç¥¨ä»£è™Ÿæ¸…å–®all_stock_listæ¯”è¼ƒ,æ‰¾å‡ºå°šæœªä¸‹è¼‰çš„pdf
    def get_undownloaded_stocks(self,save_dir,all_stock_list):
        download_stock_list = [f.split('.')[0][-4:] for f in os.listdir(save_dir) if os.path.isfile(os.path.join(save_dir, f))]
        result = [elem for elem in all_stock_list if elem not in download_stock_list]
        return result

#è®€æª”pdfå€----------------------------------------------------------------------------------------------------------------------
    
    #ç”¨sparkåˆ†æ•£å¼è®€å–
    def load_pdf_spark(self,stock_list,pdf_path,memory = "5g"):
        from pyspark.sql import SparkSession
        
        # èµ· Spark
        spark = SparkSession.builder.appName("Read PDFs with Spark")\
        .config("spark.driver.memory", memory)\
        .config("spark.driver.maxResultSize", memory)\
        .getOrCreate() # å…§å­˜å¤§å°
    
        def process_pdf(filename):
            if filename.endswith('.pdf'):
                #stock_symbol = filename.split('_')[1].split('.')[0] # åˆ†å‰²,å–å‡ºè‚¡ç¥¨ä»£è€—
                stock_symbol = filename.split('.')[0][-4:]
                if stock_symbol in stock_list: 
                    file_path = os.path.join(pdf_path, filename)
                    content = "".join(page.get_text() for page in fitz.open(file_path)) #æ‰“é–‹pdf, é€è¡Œè®€å–ä¸¦åˆä½µ
                    return stock_symbol, content
    
        # ä½¿ç”¨ Spark è®€å–æ¯å€‹ PDF æ–‡ä»¶
        pdf_contents = spark.sparkContext.parallelize(os.listdir(pdf_path)).map(process_pdf).filter(lambda x: x).collect()
        
        # å°‡çµæœè½‰æ›ç‚º Pandas DataFrame
        pdf_df = pd.DataFrame(pdf_contents, columns=["Stock Symbol", "PDF Content"]).set_index("Stock Symbol")
        return pdf_df

    #å–®ç·šç¨‹è®€å–
    def load_pdf(self,stock_list, pdf_path):
        def process_pdf(filename):
            if filename.endswith('.pdf'):
                stock_symbol = filename.split('.')[0][-4:]
                if stock_symbol in stock_list:
                    file_path = os.path.join(pdf_path, filename)
                    content = "".join(page.get_text() for page in fitz.open(file_path))  
                    return stock_symbol, content
        
        pdf_contents = [process_pdf(filename) for filename in os.listdir(pdf_path) if filename.endswith('.pdf')]
        pdf_contents = [item for item in pdf_contents if item is not None]
        
        # å°†ç»“æœè½¬æ¢ä¸º Pandas DataFrame
        pdf_df = pd.DataFrame(pdf_contents, columns=["Stock Symbol", "PDF Content"]).set_index("Stock Symbol")
        return pdf_df

#ç›¸é—œä¿‚æ•¸å€----------------------------------------------------------------------------------------------------------------------

    # ç›¸é—œæ•¸æ’å
    def get_corr_ranked(self,stock_symbol: str, close: pd.DataFrame) -> None:
        stock_symbol = str(stock_symbol)
        correlation_with_target = close.corr()[stock_symbol].drop(stock_symbol)
        most_related = correlation_with_target.nlargest(30)
        least_related = correlation_with_target.nsmallest(30)
        
        for title, data in [("Most", most_related), ("Least", least_related)]:
            fig = px.bar(data, title=f'Top 30 Stocks {title} Related to {stock_symbol}', labels={'value': 'Correlation', 'index': 'Stocks'})
            fig.show()

    # æ™‚é–“åºåˆ—æ¯”è¼ƒåœ–
    def get_tm_series_chart(self,stock_symbols: list, close: pd.DataFrame, lag: int = 0) -> None:
        stock1, stock2 = map(str, stock_symbols)
        
        if lag > 0:
            shifted_stock2 = close[stock2].shift(lag)
            valid_idx = ~shifted_stock2.isna()
            stock2_values = shifted_stock2[valid_idx]
            stock1_values = close[stock1][valid_idx]
        else:
            stock1_values = close[stock1]
            stock2_values = close[stock2]
        
        correlation = stock1_values.corr(stock2_values)
        
        fig = go.Figure()
        
        fig.add_trace(go.Scatter(x=close.index, y=close[stock1], mode='lines', name=stock1, yaxis='y1'))
        fig.add_trace(go.Scatter(x=close.index, y=shifted_stock2 if lag > 0 else close[stock2], mode='lines', name=f'{stock2} (lag={lag})', yaxis='y2'))
        
        fig.update_layout(
            title=f'æ™‚é–“åºåˆ—æ¯”è¼ƒåœ– (lag={lag}, correlation={correlation:.2f})',
            xaxis=dict(title='æ—¥æœŸ'),
            yaxis=dict(title=f'{stock1} æ”¶ç›¤åƒ¹', side='left'),
            yaxis2=dict(title=f'{stock2} æ”¶ç›¤åƒ¹', side='right', overlaying='y')
        )
        
        fig.show()

#å› å­æ¸¬è©¦å€----------------------------------------------------------------------------------------------------------------------



    def get_quartertly_factor_analysis(self,factor_list: List[str], pr: Tuple[int, int], n_high_or_low: int)-> pd.DataFrame:
        close = self.get('price:æ”¶ç›¤åƒ¹')
        market_value = self.get('etl:market_value')
        
        # Initialize DataFrame to store results
        factor_report_df_all = pd.DataFrame(columns=[
            "å› å­åç¨±", "ç­–ç•¥æ¢ä»¶", "PR_Range", "é™åˆ¶å› å­æ•¸å€¼æ­£è² ", "å‰µnæœŸé«˜orä½", "ç¸½äº¤æ˜“ç­†æ•¸", 
            "ç­–ç•¥å¹´åŒ–å ±é…¬ç‡", "ç­–ç•¥MDD", "ç­–ç•¥sortino(æ—¥)", "å€‹è‚¡å¹³å‡å ±é…¬ç‡", "å€‹è‚¡å¹³å‡MDD", 
            "å€‹è‚¡å ±é…¬ç‡æ¨™æº–å·®", "å€‹è‚¡å¹³å‡æŒæœ‰æœŸé–“(äº¤æ˜“æ—¥)", "å€‹è‚¡å¹³å‡è™•æ–¼ç²åˆ©å¤©æ•¸", 
            "å€‹è‚¡æœ€å¤§å¹´åŒ–è¤‡åˆ©å ±é…¬"
        ])
    
        def calculate_factor_report(conditions: pd.DataFrame, factor_name: str, strategy_type: str, pr: Tuple[int, int], pn: str, n_high_or_low: int):
            pr_range = np.nan
            if strategy_type in ["qoq", "yoy", "factor_market_value_ratio","origin_value"]:
                n_high_or_low = np.nan
                pr_range = f"{pr[0]}-{pr[1]}"
            
            try:
                # Simulate strategy and generate report
                report = sim(position=conditions, position_limit=1, fee_ratio=1.425/1000*0.2, trade_at_price="open",upload=False)
                trades = report.get_trades()
                
                report_data = {
                    'å› å­åç¨±': factor_name,
                    'ç­–ç•¥æ¢ä»¶': strategy_type,
                    'PR_Range': pr_range,
                    'é™åˆ¶å› å­æ•¸å€¼æ­£è² ': pn,
                    'å‰µnæœŸé«˜orä½': n_high_or_low,
                    'ç¸½äº¤æ˜“ç­†æ•¸': len(trades),
                    'ç­–ç•¥å¹´åŒ–å ±é…¬ç‡': report.get_stats()["cagr"],
                    'ç­–ç•¥MDD': report.get_stats()["max_drawdown"],
                    'ç­–ç•¥sortino(æ—¥)': report.get_stats()["daily_sortino"],
                    'å€‹è‚¡å¹³å‡å ±é…¬ç‡': trades["return"].mean(),
                    'å€‹è‚¡å¹³å‡MDD': trades["mdd"].mean(),
                    'å€‹è‚¡å ±é…¬ç‡æ¨™æº–å·®': trades["return"].std(),
                    'å€‹è‚¡å¹³å‡æŒæœ‰æœŸé–“(äº¤æ˜“æ—¥)': trades["period"].mean(),
                    'å€‹è‚¡å¹³å‡è™•æ–¼ç²åˆ©å¤©æ•¸': trades["pdays"].mean(),
                    'å€‹è‚¡æœ€å¤§å¹´åŒ–è¤‡åˆ©å ±é…¬': (1 + trades["return"].mean()) ** (240 / trades["period"].mean()) - 1,
                }
                return pd.DataFrame([report_data])
            except Exception:
                return None
    
        def generate_conditions(factor_df : pd.DataFrame, strategy_type: str, pr: Tuple[int, int], pn: str, n_high_or_low: int):
            pr_down, pr_up = pr
            conditions = None
            
            if pn == "pos":
                if strategy_type == "qoq":
                    conditions = (
                        (self.get_pr(factor_df / factor_df.shift(1)) > pr_down) & 
                        (self.get_pr(factor_df / factor_df.shift(1)) < pr_up) &
                        (factor_df > 0) & (close > 0)
                    )
                elif strategy_type == "yoy":
                    conditions = (
                        (self.get_pr(factor_df / factor_df.shift(4)) > pr_down) & 
                        (self.get_pr(factor_df / factor_df.shift(4)) < pr_up) &
                        (factor_df > 0) & (close > 0)
                    )
                elif strategy_type == "higest":
                    conditions = (factor_df == factor_df.rolling(n_high_or_low).max()) & (factor_df > 0) & (close > 0)
                elif strategy_type == "factor_market_value_ratio":
                    factor_market_value_ratio = factor_df / market_value
                    conditions = (
                        (self.get_pr(factor_market_value_ratio / factor_market_value_ratio.shift(4)) > pr_down) & 
                        (self.get_pr(factor_market_value_ratio / factor_market_value_ratio.shift(4)) < pr_up) &
                        (factor_df > 0) & (close > 0)
                    )
                elif strategy_type == "lowest":
                    conditions = (factor_df == factor_df.rolling(n_high_or_low).min()) & (factor_df > 0) & (close > 0)
                elif strategy_type == "origin_value":
                    conditions = (self.get_pr(factor_df)>pr_down) & (self.get_pr(factor_df)<pr_up)& (factor_df > 0) & (close > 0)
                    
            elif pn == "neg":
                if strategy_type == "qoq":
                    conditions = (
                        (self.get_pr(factor_df / factor_df.shift(1)) > pr_down) & 
                        (self.get_pr(factor_df / factor_df.shift(1)) < pr_up) &
                        (factor_df < 0) & (close > 0)
                    )
                elif strategy_type == "yoy":
                    conditions = (
                        (self.get_pr(factor_df / factor_df.shift(4)) > pr_down) & 
                        (self.get_pr(factor_df / factor_df.shift(4)) < pr_up) &
                        (factor_df < 0) & (close > 0)
                    )
                elif strategy_type == "higest":
                    conditions = (factor_df == factor_df.rolling(n_high_or_low).max()) & (factor_df < 0) & (close > 0)
                elif strategy_type == "factor_market_value_ratio":
                    factor_market_value_ratio = factor_df / market_value
                    conditions = (
                        (self.get_pr(factor_market_value_ratio / factor_market_value_ratio.shift(4)) > pr_down) & 
                        (self.get_pr(factor_market_value_ratio / factor_market_value_ratio.shift(4)) < pr_up) &
                        (factor_df < 0) & (close > 0)
                    )
                elif strategy_type == "lowest":
                    conditions = (factor_df == factor_df.rolling(n_high_or_low).min()) & (factor_df < 0) & (close > 0)
                elif strategy_type == "origin_value":
                    conditions = (self.get_pr(factor_df)>pr_down) & (self.get_pr(factor_df)<pr_up)& (factor_df < 0) & (close > 0)
                    
            return conditions
    
        def run_factor_analysis(factor_list, strategy_types, pos_neg_options, pr, n_high_or_low):
            factor_report_df_all = pd.DataFrame()
            for factor in factor_list:
                factor_df = self.get(factor)
                factor_name = factor.split(":")[1]
                
                for strategy_type in strategy_types:
                    for pn in pos_neg_options:
                        conditions = generate_conditions(factor_df, strategy_type, pr, pn, n_high_or_low)
                        report_df = calculate_factor_report(conditions, factor_name, strategy_type, pr, pn, n_high_or_low)
                        
                        if report_df is not None:
                            factor_report_df_all = pd.concat([factor_report_df_all, report_df])
            
            return factor_report_df_all
    
        # Run the analysis loop
        strategy_types = ["qoq", "yoy", "higest", "factor_market_value_ratio", "lowest","origin_value"]
        pos_neg_options = ["pos", "neg"]
        factor_report_df_all = run_factor_analysis(factor_list, strategy_types, pos_neg_options, pr, n_high_or_low)
        
        return factor_report_df_all.reset_index(drop=True)



#tg----------------------------------------------------------------------------------------------------------------------


# åœ¨ co_tools_beta.py çš„ Codata é¡ä¸­æ·»åŠ ä»¥ä¸‹æ–¹æ³•

    def tg_extract_position_info(self, report):
        """æå–æŒå€‰è³‡è¨Š
        
        Args:
            report: finlab å›æ¸¬å ±å‘Šç‰©ä»¶
            
        Returns:
            tuple: (é€²å ´è‚¡ç¥¨åˆ—è¡¨, æŒæœ‰è‚¡ç¥¨åˆ—è¡¨, å‡ºå ´è‚¡ç¥¨åˆ—è¡¨, æœ€å¾Œæ—¥æœŸ)
        """
        trades = report.get_trades()
        last_date = report.daily_benchmark.index[-1]
        position_info = report.position_info2()
        
        if isinstance(position_info, dict) and 'positions' in position_info:
            positions = position_info['positions']
        else:
            positions = []
        
        enter_stocks = []
        hold_stocks = []
        exit_stocks = []
        
        for position in positions:
            if not isinstance(position, dict):
                continue
                
            asset_id = position.get('assetId', '')
            asset_name = position.get('assetName', '')
            stock_display = f"{asset_id} {asset_name}" if asset_id and asset_name else (asset_id or asset_name)
            
            action_type = position.get('action', {}).get('type', '')
            current_weight = position.get('currentWeight', 0)
            
            # å¿½ç•¥éå»çš„äº¤æ˜“è¨˜éŒ„
            if action_type == 'exit_p':
                continue
                
            # æ ¹æ“šå‹•ä½œé¡å‹åˆ†é¡
            if action_type == 'entry':
                enter_stocks.append(stock_display)
            elif current_weight != 0:
                hold_stocks.append(stock_display)
                if action_type == 'exit':
                    exit_stocks.append(stock_display)
        
        return enter_stocks, hold_stocks, exit_stocks, last_date
    
    def tg_generate_strategy_message(self, report, strategy_config):
        """ç”Ÿæˆç­–ç•¥ Telegram è¨Šæ¯
        
        Args:
            report: finlab å›æ¸¬å ±å‘Šç‰©ä»¶
            strategy_config: ç­–ç•¥é…ç½®å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹éµå€¼ï¼š
                - name: ç­–ç•¥åç¨±
                - description: ç­–ç•¥èªªæ˜
                - author: ç­–ç•¥ä½œè€…
                - direction: ç­–ç•¥å¤šç©ºæ–¹å‘ (å¤š/ç©º)
                - notes: ç­–ç•¥å‚™è¨» (å¯é¸)
                - enter_label: é€²å ´æ¨™ç±¤ (å¯é¸)
                - hold_label: æŒå€‰æ¨™ç±¤ (å¯é¸)
                - exit_label: å‡ºå ´æ¨™ç±¤ (å¯é¸)
                
        Returns:
            str: æ ¼å¼åŒ–çš„ Telegram è¨Šæ¯
        """
        # æå–æŒå€‰è³‡è¨Š
        trades = report.get_trades()
        enter_stocks, hold_stocks, exit_stocks, last_date = self.tg_extract_position_info(report)
        
        # ç²å–çµ±è¨ˆæ•¸æ“š
        stats_dict = self.display_report_statis(trades)
        
        # è™•ç†ç­–ç•¥é…ç½®é è¨­å€¼
        direction = strategy_config.get('direction', 'å¤š')
        notes = strategy_config.get('notes', '')
        
        # æ ¹æ“šå¤šç©ºæ–¹å‘è¨­å®šæ¨™ç±¤
        if direction == 'ç©º':
            enter_label = strategy_config.get('enter_label', 'æ”¾ç©ºè‚¡ç¥¨')
            exit_label = strategy_config.get('exit_label', 'å›è£œè‚¡ç¥¨')
        else:
            enter_label = strategy_config.get('enter_label', 'è²·å…¥è‚¡ç¥¨')
            exit_label = strategy_config.get('exit_label', 'è³£å‡ºè‚¡ç¥¨')
        
        hold_label = strategy_config.get('hold_label', 'ç•¶å‰æŒå€‰')
        
        # ç”Ÿæˆè¨Šæ¯
        msg = f"""ğŸ””ğŸ””ğŸ””<b>ç­–ç•¥é€šçŸ¥</b>
<pre>ç­–ç•¥åç¨±: {strategy_config['name']}
ç­–ç•¥èªªæ˜: {strategy_config['description']}
ç­–ç•¥ä½œè€…: {strategy_config['author']}
ç­–ç•¥å¤šç©º: {direction}
ç­–ç•¥å‚™è¨»: {notes}

é å®šæ›è‚¡æ—¥: {last_date}

ğŸ“ˆ
{enter_label}: {enter_stocks}
{hold_label}: {hold_stocks}
{exit_label}: {exit_stocks}

ğŸ”¢
ç¸½äº¤æ˜“ç­†æ•¸: {stats_dict["äº¤æ˜“ç­†æ•¸"]}
å¹³å‡å ±é…¬ç‡: {stats_dict["å¹³å‡å ±é…¬ç‡"]}
å¹³å‡æŒæœ‰å¤©æ•¸: {stats_dict["å¹³å‡æŒæœ‰æœŸé–“(äº¤æ˜“æ—¥)"]}
å‹ç‡: {stats_dict["å‹ç‡"]}
æœ€å¤§å¹´åŒ–å ±é…¬ç‡(æ³¢å‹•èª¿æ•´_æ³°å‹’å±•é–‹): {stats_dict["æœ€å¤§å¹´åŒ–å ±é…¬ç‡(æ³¢å‹•èª¿æ•´_æ³°å‹’å±•é–‹)"]}</pre>"""
 
        
        return msg.strip()
    
    def tg_create_strategy_message_quick(self, report, strategy_name, strategy_description, 
                                    strategy_author, strategy_direction="å¤š", 
                                    strategy_notes="", **kwargs):
        """å¿«é€Ÿå‰µå»ºç­–ç•¥è¨Šæ¯çš„ä¾¿åˆ©æ–¹æ³•
        
        Args:
            report: finlab å›æ¸¬å ±å‘Š
            strategy_name: ç­–ç•¥åç¨±
            strategy_description: ç­–ç•¥èªªæ˜
            strategy_author: ç­–ç•¥ä½œè€…
            strategy_direction: ç­–ç•¥æ–¹å‘ (å¤š/ç©º)
            strategy_notes: ç­–ç•¥å‚™è¨»
            **kwargs: å…¶ä»–è‡ªå®šç¾©æ¨™ç±¤
            
        Returns:
            str: æ ¼å¼åŒ–çš„ Telegram è¨Šæ¯
        """
        strategy_config = {
            'name': strategy_name,
            'description': strategy_description,
            'author': strategy_author,
            'direction': strategy_direction,
            'notes': strategy_notes,
            **kwargs
        }
        
        return self.tg_generate_strategy_message(report, strategy_config)

    def tg_capture_report_images(self, html_filename=None, 
                                 output_image1=None,
                                 output_image2=None):
        """
        åŒæ­¥ç‰ˆæœ¬çš„ HTML è½‰åœ–ç‰‡æ–¹æ³•
        
        Args:
            html_filename: HTML æª”æ¡ˆåç¨± (None å‰‡ä½¿ç”¨é è¨­)
            output_image1: ç¬¬ä¸€å¼µæˆªåœ–æª”å (None å‰‡ä½¿ç”¨é è¨­)
            output_image2: ç¬¬äºŒå¼µæˆªåœ–æª”å (None å‰‡ä½¿ç”¨é è¨­)
        """
        import asyncio
        
        # ä½¿ç”¨é è¨­å€¼æˆ–å‚³å…¥å€¼
        html_filename = html_filename or self.html_file
        output_image1 = output_image1 or self.image_file_1
        output_image2 = output_image2 or self.image_file_2
        
        if not os.path.exists(html_filename):
            print(f"éŒ¯èª¤: HTML æª”æ¡ˆ '{html_filename}' ä¸å­˜åœ¨")
            return False
        
        try:
            asyncio.run(self.tg_capture_html_to_image(html_filename, output_image1, output_image2))
            return True
        except Exception as e:
            print(f"æˆªåœ–å¤±æ•—: {e}")
            return False

    
    async def tg_capture_html_to_image(self, html_file_path, output_image_path1, output_image_path2, 
                                       browser_type='chromium', full_page=True, 
                                       viewport_width=1920, viewport_height=1080):
        """
        å°‡æœ¬åœ° HTML æª”æ¡ˆè½‰æ›ç‚ºåœ–ç‰‡
        
        Args:
            html_file_path: HTML æª”æ¡ˆè·¯å¾‘
            output_image_path1: ç¬¬ä¸€å¼µæˆªåœ–è·¯å¾‘ï¼ˆåŸå§‹é é¢ï¼‰
            output_image_path2: ç¬¬äºŒå¼µæˆªåœ–è·¯å¾‘ï¼ˆé»é¸å¾Œé é¢ï¼‰
            browser_type: ç€è¦½å™¨é¡å‹ ('chromium', 'firefox', 'webkit')
            full_page: æ˜¯å¦æˆªå–å®Œæ•´é é¢
            viewport_width: è¦–çª—å¯¬åº¦
            viewport_height: è¦–çª—é«˜åº¦
        """
        from playwright.async_api import async_playwright
        
        async with async_playwright() as p:
            # å•Ÿå‹•ç€è¦½å™¨
            if browser_type == 'chromium':
                browser = await p.chromium.launch()
            elif browser_type == 'firefox':
                browser = await p.firefox.launch()
            elif browser_type == 'webkit':
                browser = await p.webkit.launch()
            else:
                raise ValueError("ä¸æ”¯æ´çš„ç€è¦½å™¨é¡å‹")
            
            # è½‰æ›ç‚ºçµ•å°è·¯å¾‘
            abs_html_file_path = os.path.abspath(html_file_path)
            file_url = f"file:///{abs_html_file_path.replace(os.sep, '/')}"
            print(f"æ­£åœ¨é–‹å•Ÿ: {file_url}")
            
            page = await browser.new_page()
            await page.set_viewport_size({"width": viewport_width, "height": viewport_height})
            
            try:
                # è¼‰å…¥é é¢
                await page.goto(file_url, wait_until="networkidle")
                await page.wait_for_timeout(2000)
                
                # ç¬¬ä¸€æ¬¡æˆªåœ–ï¼šåŸå§‹é é¢
                print(f"æ­£åœ¨æ“·å–åŸå§‹é é¢åˆ°: {output_image_path1}")
                await page.screenshot(path=output_image_path1, full_page=full_page)
                
                # å°‹æ‰¾é¸è‚¡æŒ‰éˆ• - ä½¿ç”¨å¤šç¨®é¸æ“‡å™¨å˜—è©¦
                selectors = [
                    'a:has-text("é¸è‚¡")',  # ç°¡å–®æ–‡å­—é¸æ“‡å™¨
                    'a[role="tab"]:has-text("é¸è‚¡")',  # å¸¶roleå±¬æ€§
                    '.tab:has-text("é¸è‚¡")',  # classé¸æ“‡å™¨
                    'a.tab-active:has-text("é¸è‚¡")',  # åŸå§‹é¸æ“‡å™¨
                ]
                
                element_found = False
                for selector in selectors:
                    try:
                        print(f"å˜—è©¦é¸æ“‡å™¨: {selector}")
                        await page.wait_for_selector(selector, timeout=5000)
                        print(f"æ‰¾åˆ°é¸è‚¡æŒ‰éˆ•ï¼Œæ­£åœ¨é»é¸...")
                        await page.click(selector)
                        element_found = True
                        break
                    except Exception as e:
                        print(f"é¸æ“‡å™¨ {selector} å¤±æ•—: {e}")
                        continue
                
                if not element_found:
                    print("ç„¡æ³•æ‰¾åˆ°é¸è‚¡æŒ‰éˆ•ï¼Œåˆ—å‡ºæ‰€æœ‰å¯èƒ½çš„é¸é …...")
                    # åˆ—å‡ºæ‰€æœ‰åŒ…å«"é¸è‚¡"çš„å…ƒç´ 
                    elements = await page.query_selector_all('*')
                    for element in elements[:20]:  # åªæª¢æŸ¥å‰20å€‹å…ƒç´ é¿å…å¤ªå¤šè¼¸å‡º
                        text = await element.text_content()
                        if text and "é¸è‚¡" in text:
                            tag_name = await element.evaluate('el => el.tagName')
                            class_name = await element.get_attribute('class')
                            print(f"æ‰¾åˆ°åŒ…å«'é¸è‚¡'çš„å…ƒç´ : {tag_name}, class: {class_name}, text: {text}")
                    
                    # å˜—è©¦ç›´æ¥é»æ“Šä»»ä½•åŒ…å«"é¸è‚¡"æ–‡å­—çš„å…ƒç´ 
                    try:
                        await page.click('text=é¸è‚¡')
                        element_found = True
                        print("ä½¿ç”¨ text=é¸è‚¡ æˆåŠŸé»æ“Š")
                    except:
                        print("æ‰€æœ‰æ–¹æ³•éƒ½å¤±æ•—")
                
                if element_found:
                    await page.wait_for_timeout(2000)  # ç­‰å¾…é é¢æ›´æ–°
                    
                    # ç¬¬äºŒæ¬¡æˆªåœ–ï¼šé»é¸å¾Œ
                    print(f"æ­£åœ¨æ“·å–é¸è‚¡å¾Œé é¢åˆ°: {output_image_path2}")
                    await page.screenshot(path=output_image_path2, full_page=full_page)
                    print("æ“·å–æˆåŠŸï¼")
                else:
                    print("ç„¡æ³•é»æ“Šé¸è‚¡æŒ‰éˆ•ï¼Œåªä¿å­˜åŸå§‹æˆªåœ–")
                    
            except Exception as e:
                print(f"ç™¼ç”ŸéŒ¯èª¤: {e}")
                # é™¤éŒ¯ï¼šå°å‡ºé é¢å…§å®¹
                try:
                    html_content = await page.content()
                    print("é é¢ HTML å…§å®¹ç‰‡æ®µï¼š", html_content[:500])
                except:
                    print("ç„¡æ³•å–å¾—é é¢å…§å®¹")
                    
            finally:
                await browser.close()

    def tg_send_photo(self, bot_token, channel_ids, photo_path, caption=""):
        """
        ç™¼é€åœ–ç‰‡åˆ° Telegram
        
        Args:
            bot_token: Bot token
            channel_ids: é »é“IDåˆ—è¡¨
            photo_path: åœ–ç‰‡è·¯å¾‘
            caption: åœ–ç‰‡èªªæ˜
            
        Returns:
            dict: ç™¼é€çµæœ
        """
        if not os.path.exists(photo_path):
            print(f"åœ–ç‰‡æª”æ¡ˆä¸å­˜åœ¨: {photo_path}")
            return {}
            
        url = f"https://api.telegram.org/bot{bot_token}/sendPhoto"
        results = {}
        
        for cid in channel_ids:
            try:
                with open(photo_path, 'rb') as photo:
                    files = {'photo': photo}
                    data = {'chat_id': cid, 'caption': caption}
                    resp = requests.post(url, files=files, data=data)
                    results[cid] = resp.status_code
                    
                    if resp.status_code != 200:
                        print(f"ç™¼é€åœ–ç‰‡è‡³ {cid} å¤±æ•—: {resp.text}")
                    else:
                        print(f"åœ–ç‰‡æˆåŠŸç™¼é€è‡³ {cid}")
                        
            except Exception as e:
                print(f"ç™¼é€åœ–ç‰‡è‡³ {cid} ç™¼ç”ŸéŒ¯èª¤: {e}")
                results[cid] = 0
                
        return results

    def tg_clean_files(self, clean_html=True, clean_images=True):
        """
        æ¸…ç†ç”¢ç”Ÿçš„æª”æ¡ˆ
        
        Args:
            clean_html: æ˜¯å¦åˆªé™¤ HTML æª”æ¡ˆ
            clean_images: æ˜¯å¦åˆªé™¤åœ–ç‰‡æª”æ¡ˆ
        """
        files_to_clean = []
        
        if clean_html and os.path.exists(self.html_file):
            files_to_clean.append(self.html_file)
            
        if clean_images:
            if os.path.exists(self.image_file_1):
                files_to_clean.append(self.image_file_1)
            if os.path.exists(self.image_file_2):
                files_to_clean.append(self.image_file_2)
        
        cleaned_files = []
        for file_path in files_to_clean:
            try:
                os.remove(file_path)
                cleaned_files.append(file_path)
                print(f"å·²åˆªé™¤æª”æ¡ˆ: {file_path}")
            except Exception as e:
                print(f"åˆªé™¤æª”æ¡ˆ {file_path} å¤±æ•—: {e}")
                
        return cleaned_files

    def tg_generate_and_send_complete(self, report, strategy_config, bot_token, channel_ids, 
                                      send_images=True, clean_files=True, 
                                      clean_html=True, clean_images=True):
        """
        å®Œæ•´çš„ç­–ç•¥æ¨é€æµç¨‹ï¼šç”Ÿæˆè¨Šæ¯ -> æˆªåœ– -> ç™¼é€ -> æ¸…ç†
        
        Args:
            report: finlab å›æ¸¬å ±å‘Š
            strategy_config: ç­–ç•¥é…ç½®
            bot_token: Bot token
            channel_ids: é »é“IDåˆ—è¡¨
            send_images: æ˜¯å¦ç™¼é€åœ–ç‰‡
            clean_files: æ˜¯å¦æ¸…ç†æª”æ¡ˆ
            clean_html: æ˜¯å¦åˆªé™¤ HTML æª”æ¡ˆ
            clean_images: æ˜¯å¦åˆªé™¤åœ–ç‰‡æª”æ¡ˆ
            
        Returns:
            dict: åŸ·è¡Œçµæœ
        """
        results = {'message_sent': False, 'images_sent': [], 'files_cleaned': []}
        
        try:
            # 1. ç”Ÿæˆæ–‡å­—è¨Šæ¯ä¸¦ç™¼é€
            msg = self.tg_generate_strategy_message(report, strategy_config)
            
            # ç™¼é€æ–‡å­—è¨Šæ¯
            url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
            for cid in channel_ids:
                payload = {"chat_id": cid, "text": msg, "parse_mode": "HTML"}
                resp = requests.post(url, json=payload)
                if resp.status_code == 200:
                    results['message_sent'] = True
                    print(f"æ–‡å­—è¨Šæ¯æˆåŠŸç™¼é€è‡³ {cid}")
                else:
                    print(f"æ–‡å­—è¨Šæ¯ç™¼é€è‡³ {cid} å¤±æ•—: {resp.text}")
            
            # 2. å¦‚æœéœ€è¦ç™¼é€åœ–ç‰‡
            if send_images:
                # ç”Ÿæˆæˆªåœ–
                if self.tg_capture_report_images():
                    # ç™¼é€ç¬¬ä¸€å¼µåœ–ç‰‡
                    if os.path.exists(self.image_file_1):
                        result1 = self.tg_send_photo(bot_token, channel_ids, self.image_file_1, "ç­–ç•¥å ±å‘Š - åŸå§‹é é¢")
                        if any(status == 200 for status in result1.values()):
                            results['images_sent'].append(self.image_file_1)
                    
                    # ç™¼é€ç¬¬äºŒå¼µåœ–ç‰‡
                    if os.path.exists(self.image_file_2):
                        result2 = self.tg_send_photo(bot_token, channel_ids, self.image_file_2, "ç­–ç•¥å ±å‘Š - é¸è‚¡é é¢")
                        if any(status == 200 for status in result2.values()):
                            results['images_sent'].append(self.image_file_2)
                else:
                    print("æˆªåœ–å¤±æ•—ï¼Œè·³éåœ–ç‰‡ç™¼é€")
            
            # 3. æ¸…ç†æª”æ¡ˆ
            if clean_files:
                cleaned = self.tg_clean_files(clean_html, clean_images)
                results['files_cleaned'] = cleaned
                
        except Exception as e:
            print(f"å®Œæ•´æ¨é€æµç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            
        return results


#CatBoostç‰¹å¾µé‡è¦æ€§åˆ†æå€----------------------------------------------------------------------------------------------------------------

    def _handle_dataframe(self, df, target_col):
        """è™•ç†ä¸åŒé¡å‹çš„ DataFrameï¼Œä¿æŒåŸå§‹æ ¼å¼çš„æ•ˆèƒ½å„ªå‹¢"""
        # æª¢æŸ¥æ˜¯å¦ç‚º polars DataFrame
        if hasattr(df, 'select') and hasattr(df, 'drop'):
            print("âš¡ ä½¿ç”¨ Polars DataFrame (é«˜æ•ˆèƒ½æ¨¡å¼)")
            
            # ä½¿ç”¨ Polars çš„é«˜æ•ˆæ“ä½œ
            feature_cols = [col for col in df.columns if col != target_col]
            X = df.select(feature_cols).to_numpy()  # ç›´æ¥è½‰ç‚º numpyï¼Œé¿å… pandas é–‹éŠ·
            y = df.select(target_col).to_numpy().flatten()
            feature_names = feature_cols
            
            return X, y, feature_names
        
        # pandas DataFrame
        elif isinstance(df, pd.DataFrame):
            print("ğŸ“Š ä½¿ç”¨ Pandas DataFrame")
            X = df.drop(columns=[target_col])
            y = df[target_col]
            feature_names = X.columns.tolist()
            
            return X.values, y.values, feature_names
        
        else:
            raise ValueError("æ”¯æ´çš„æ ¼å¼: pandas.DataFrame æˆ– polars.DataFrame")
    
    def analyze_feature_importance(self, df, target_col='target', model_type='regressor', 
                                 loss_type='RMSE', use_gpu=True, iterations=200, 
                                 depth=6, learning_rate=0.1, l2_leaf_reg=3, top_n=None):
        """
        åˆ†æç‰¹å¾µé‡è¦æ€§ä¸¦ç¹ªè£½ç¾åŒ–åœ–è¡¨
        
        åƒæ•¸:
        df: DataFrame (pandas.DataFrame æˆ– polars.DataFrame)
        target_col: ç›®æ¨™è®Šé‡
        model_type: 'regressor', 'classifier'
        loss_type: 'RMSE', 'MAE', 'both'
        use_gpu: æ˜¯å¦ä½¿ç”¨ GPU åŠ é€Ÿ
        l2_leaf_reg: L2 æ­£å‰‡åŒ–åƒæ•¸ (1-10)
        top_n: é¡¯ç¤ºå‰ N å€‹é‡è¦ç‰¹å¾µ (None = é¡¯ç¤ºå…¨éƒ¨)
        """
        # é«˜æ•ˆè™•ç†ä¸åŒæ ¼å¼çš„ DataFrame
        X, y, feature_names = self._handle_dataframe(df, target_col)
        
        # å¦‚æœæ²’æœ‰æŒ‡å®š top_nï¼Œé¡¯ç¤ºå…¨éƒ¨ç‰¹å¾µ
        if top_n is None:
            top_n = len(feature_names)
        
        # åŸºç¤åƒæ•¸ - MAE ä¸æ”¯æ´ GPUï¼Œè‡ªå‹•èª¿æ•´
        gpu_compatible = use_gpu and (loss_type != 'MAE' and loss_type != 'both')
        
        base_params = {
            'iterations': iterations,
            'depth': depth,
            'learning_rate': learning_rate,
            'l2_leaf_reg': l2_leaf_reg,
            'task_type': 'GPU' if gpu_compatible else 'CPU',
            'verbose': 0
        }
        
        # é¡¯ç¤º GPU/CPU ä½¿ç”¨ç‹€æ³
        if loss_type == 'MAE' and use_gpu:
            print("âš ï¸  MAE ä¸æ”¯æ´ GPUï¼Œè‡ªå‹•åˆ‡æ›ç‚º CPU æ¨¡å¼")
        elif loss_type == 'both' and use_gpu:
            print("âš ï¸  MAE æ¨¡å‹å°‡ä½¿ç”¨ CPUï¼ŒRMSE æ¨¡å‹ä½¿ç”¨ GPU")
        
        # é¡¯ç¤ºé‹ç®—è³‡è¨Š
        device_info = "ğŸš€ GPU" if use_gpu else "ğŸ–¥ï¸  CPU"
        print(f"ä½¿ç”¨ {device_info} é€²è¡Œè¨“ç·´ ")
        print(f"ğŸ“ˆ æ•¸æ“šç¶­åº¦: {X.shape[0]} æ¨£æœ¬, {X.shape[1]} ç‰¹å¾µ")
        print(f"ğŸ“‹ å°‡é¡¯ç¤º {top_n} å€‹ç‰¹å¾µé‡è¦æ€§")
        
        if model_type == 'regressor':
            if loss_type == 'both':
                return self._compare_loss_functions(X, y, feature_names, base_params, top_n)
            else:
                return self._single_model_analysis(X, y, feature_names, base_params, loss_type, model_type, top_n)
        else:  # classifier
            print(f"ğŸ“Š å°‡æ•¸å€¼å‹ç›®æ¨™è½‰æ›ç‚ºäºŒå…ƒåˆ†é¡ï¼šæ­£å€¼â†’1(ä¸Šæ¼²), è² å€¼/é›¶â†’0(ä¸‹è·Œ)")
            return self._single_model_analysis(X, y, feature_names, base_params, loss_type, model_type, top_n)
    
    def _single_model_analysis(self, X, y, feature_names, base_params, loss_type, model_type, top_n):
        """å–®ä¸€æ¨¡å‹åˆ†æ"""
        if model_type == 'regressor':
            if loss_type not in ['RMSE', 'MAE']:
                raise ValueError("regressor çš„ loss_type å¿…é ˆæ˜¯ 'RMSE', 'MAE', æˆ– 'both'")
            
            params = base_params.copy()
            params['loss_function'] = loss_type
            model = CatBoostRegressor(**params)
            model.fit(X, y)
            model_name = f"CatBoost Regressor ({loss_type})"
            
        else:  # classifier
            # æ•¸å€¼å‹ç›®æ¨™è®Šé‡è½‰æ›ç‚ºäºŒå…ƒåˆ†é¡
            y_class = (y > 0).astype(int)
            
            # é¡¯ç¤ºè½‰æ›çµ±è¨ˆï¼ˆä½¿ç”¨ numpy é«˜æ•ˆè¨ˆç®—ï¼‰
            unique, counts = np.unique(y_class, return_counts=True)
            class_counts = dict(zip(unique, counts))
            total = len(y_class)
            
            print(f"   é¡åˆ¥ 0 (ä¸‹è·Œ): {class_counts.get(0, 0)} æ¨£æœ¬")
            print(f"   é¡åˆ¥ 1 (ä¸Šæ¼²): {class_counts.get(1, 0)} æ¨£æœ¬")
            print(f"   ä¸Šæ¼²æ¯”ä¾‹: {class_counts.get(1, 0) / total * 100:.1f}%")
            
            model = CatBoostClassifier(**base_params)
            model.fit(X, y_class)
            model_name = "CatBoost Classifier (Binary)"
        
        print("âœ… è¨“ç·´å®Œæˆ")
        
        # ç‰¹å¾µé‡è¦æ€§ - å–å¾—æ‰€æœ‰ç‰¹å¾µ
        importance = model.get_feature_importance()
        full_importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)
        
        # åœ–è¡¨é¡¯ç¤ºç”¨ (é™åˆ¶é¡¯ç¤ºæ•¸é‡ï¼Œé¿å…åœ–è¡¨éæ–¼æ“æ“ )
        plot_top_n = min(top_n, 20)  # åœ–è¡¨æœ€å¤šé¡¯ç¤º20å€‹
        plot_df = full_importance_df.head(plot_top_n)
        
        # ç¾åŒ–åœ–è¡¨
        fig, ax = plt.subplots(figsize=(12, max(8, plot_top_n * 0.4)))
        
        # é¡è‰²é…ç½®
        colors = plt.cm.viridis(np.linspace(0, 1, len(plot_df)))
        
        # æ°´å¹³æ¢å½¢åœ–
        bars = ax.barh(range(len(plot_df)), plot_df['importance'], 
                       color=colors, alpha=0.8, edgecolor='white', linewidth=0.8)
        
        # è¨­å®š y è»¸æ¨™ç±¤
        ax.set_yticks(range(len(plot_df)))
        ax.set_yticklabels(plot_df['feature'], fontsize=11)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for i, (bar, importance_val) in enumerate(zip(bars, plot_df['importance'])):
            width = bar.get_width()
            ax.text(width + max(plot_df['importance']) * 0.01, 
                    bar.get_y() + bar.get_height()/2, 
                    f'{importance_val:.2f}',
                    ha='left', va='center', fontweight='bold', fontsize=10)
        
        # ç¾åŒ–æ¨™é¡Œå’Œæ¨™ç±¤
        ax.set_xlabel('Feature Importance', fontsize=14, fontweight='bold')
        title_text = f'{model_name} Feature Importance'
        if len(full_importance_df) != plot_top_n:
            title_text += f' (Top {plot_top_n} of {len(full_importance_df)})'
        ax.set_title(title_text, fontsize=16, fontweight='bold', pad=20)
        
        # åè½‰ y è»¸é †åº (é‡è¦æ€§é«˜çš„åœ¨ä¸Š)
        ax.invert_yaxis()
        
        # æ·»åŠ ç¶²æ ¼
        ax.grid(True, axis='x', alpha=0.3, linestyle='--')
        ax.set_axisbelow(True)
        
        # è¨­å®š x è»¸ç¯„åœ
        ax.set_xlim(0, max(plot_df['importance']) * 1.15)
        
        # ç¾åŒ–é‚Šæ¡†
        for spine in ax.spines.values():
            spine.set_visible(False)
        
        # èª¿æ•´ä½ˆå±€
        plt.tight_layout()
        
        # æ·»åŠ èƒŒæ™¯è‰²æ¼¸è®Šæ•ˆæœ
        ax.set_facecolor('#f8f9fa')
        
        plt.show()
        
        # è¿”å›å®Œæ•´çš„é‡è¦æ€§æ’åºçµæœ (æ‰€æœ‰ç‰¹å¾µ)
        return full_importance_df
    
    def _compare_loss_functions(self, X, y, feature_names, base_params, top_n):
        """æ¯”è¼ƒ RMSE å’Œ MAE çš„ç‰¹å¾µé‡è¦æ€§"""
        print("ğŸ”„ æ¯”è¼ƒ RMSE å’Œ MAE çš„ç‰¹å¾µé‡è¦æ€§å·®ç•°...")
        
        # è¨“ç·´ RMSE æ¨¡å‹ (å¯ç”¨ GPU)
        rmse_params = base_params.copy()
        rmse_params['loss_function'] = 'RMSE'
        # RMSE ä¿æŒåŸå§‹ GPU è¨­å®š
        rmse_model = CatBoostRegressor(**rmse_params)
        rmse_model.fit(X, y)
        
        # è¨“ç·´ MAE æ¨¡å‹ (å¼·åˆ¶ CPU)
        mae_params = base_params.copy()
        mae_params['loss_function'] = 'MAE'
        mae_params['task_type'] = 'CPU'  # MAE å¼·åˆ¶ä½¿ç”¨ CPU
        mae_model = CatBoostRegressor(**mae_params)
        mae_model.fit(X, y)
        
        print("âœ… å…©å€‹æ¨¡å‹è¨“ç·´å®Œæˆ")
        
        # ç²å–ç‰¹å¾µé‡è¦æ€§
        rmse_importance = rmse_model.get_feature_importance()
        mae_importance = mae_model.get_feature_importance()
        
        # ä½¿ç”¨ numpy é«˜æ•ˆæ“ä½œ
        comparison_data = {
            'feature': feature_names,
            'RMSE_importance': rmse_importance,
            'MAE_importance': mae_importance,
            'importance_diff': rmse_importance - mae_importance
        }
        
        # å®Œæ•´çš„æ¯”è¼ƒçµæœ
        full_comparison = pd.DataFrame(comparison_data).sort_values('RMSE_importance', ascending=False)
        
        # ç¹ªè£½æ¯”è¼ƒåœ– (é™åˆ¶é¡¯ç¤ºæ•¸é‡)
        plot_top_n = min(top_n, 15)  # æ¯”è¼ƒåœ–æœ€å¤šé¡¯ç¤º15å€‹
        plot_comparison = full_comparison.head(plot_top_n)
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, max(8, plot_top_n * 0.4)))
        
        # RMSE vs MAE æ¯”è¼ƒ
        x = np.arange(len(plot_comparison))
        width = 0.35
        
        bars1 = ax1.bar(x - width/2, plot_comparison['RMSE_importance'], width, 
                        label='RMSE', alpha=0.8, color='#2E86AB')
        bars2 = ax1.bar(x + width/2, plot_comparison['MAE_importance'], width, 
                        label='MAE', alpha=0.8, color='#A23B72')
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                        f'{height:.1f}', ha='center', va='bottom', fontsize=9)
        
        ax1.set_xlabel('Features', fontweight='bold', fontsize=12)
        ax1.set_ylabel('Importance', fontweight='bold', fontsize=12)
        title_text = 'RMSE vs MAE Feature Importance Comparison'
        if len(full_comparison) != plot_top_n:
            title_text += f' (Top {plot_top_n} of {len(full_comparison)})'
        ax1.set_title(title_text, fontweight='bold', fontsize=14)
        ax1.set_xticks(x)
        ax1.set_xticklabels(plot_comparison['feature'], rotation=45, ha='right')
        ax1.legend(fontsize=11)
        ax1.grid(True, alpha=0.3, axis='y')
        ax1.set_facecolor('#f8f9fa')
        
        # é‡è¦æ€§å·®ç•°åœ–
        diff_data = plot_comparison.sort_values('importance_diff')
        colors = ['#FF6B6B' if x < 0 else '#4ECDC4' for x in diff_data['importance_diff']]
        
        bars3 = ax2.barh(range(len(diff_data)), diff_data['importance_diff'], 
                         color=colors, alpha=0.8, edgecolor='white', linewidth=0.8)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for i, (bar, diff) in enumerate(zip(bars3, diff_data['importance_diff'])):
            width = bar.get_width()
            ax2.text(width + (max(abs(diff_data['importance_diff'])) * 0.02 * (1 if width >= 0 else -1)), 
                    bar.get_y() + bar.get_height()/2, 
                    f'{diff:.2f}',
                    ha='left' if width >= 0 else 'right', va='center', 
                    fontweight='bold', fontsize=10)
        
        ax2.set_yticks(range(len(diff_data)))
        ax2.set_yticklabels(diff_data['feature'], fontsize=11)
        ax2.set_xlabel('Importance Difference (RMSE - MAE)', fontweight='bold', fontsize=12)
        ax2.set_title('Feature Importance Difference', fontweight='bold', fontsize=14)
        ax2.axvline(x=0, color='black', linestyle='--', alpha=0.7, linewidth=2)
        ax2.grid(True, alpha=0.3, axis='x')
        ax2.set_facecolor('#f8f9fa')
        ax2.invert_yaxis()
        
        # ç¾åŒ–é‚Šæ¡†
        for ax in [ax1, ax2]:
            for spine in ax.spines.values():
                spine.set_visible(False)
        
        plt.tight_layout()
        plt.show()
        
        # è¿”å›å®Œæ•´çš„æ¯”è¼ƒçµæœ (æ‰€æœ‰ç‰¹å¾µ)
        return full_comparison
    
#é«˜æ•ˆè³‡æ–™åˆä½µå€----------------------------------------------------------------------------------------------------------------

    def combine(self, features: Dict[str, pd.DataFrame], resample=None, sample_filter=None, output_format="pandas", precision="f32", **kwargs):
        """
        åˆä½µå¤šå€‹ç‰¹å¾µè³‡æ–™æ¡†ä¸¦æ”¯æ´ç²¾åº¦å£“ç¸®å’Œè¨˜æ†¶é«”å›æ”¶
        
        åƒæ•¸:
            features: åŒ…å«ç‰¹å¾µè³‡æ–™æ¡†çš„å­—å…¸
            resample: é‡æ¡æ¨£é »ç‡ (å¦‚ '1D', '1H' ç­‰) æˆ–å¯è¿­ä»£çš„ç´¢å¼•
            sample_filter: ç”¨æ–¼éæ¿¾æ¨£æœ¬çš„è³‡æ–™æ¡†
            output_format: è¼¸å‡ºæ ¼å¼ï¼Œ'pandas' æˆ– 'polars'
            precision: æ•¸å€¼ç²¾åº¦ï¼Œ'f16', 'f32'(é»˜èª) æˆ– 'f64'
            **kwargs: å‚³éçµ¦ resample æ–¹æ³•çš„åƒæ•¸
            
        è¿”å›:
            åˆä½µå¾Œçš„è³‡æ–™æ¡†ï¼Œæ ¼å¼æ ¹æ“š output_format åƒæ•¸æ±ºå®š
        """
        from collections.abc import Iterable
        import gc
        
        # æª¢æŸ¥åƒæ•¸
        if output_format not in ["pandas", "polars"]:
            raise ValueError("output_format å¿…é ˆæ˜¯ 'pandas' æˆ– 'polars'")
        
        if precision not in ["f16", "f32", "f64"]:
            raise ValueError("precision å¿…é ˆæ˜¯ 'f16', 'f32' æˆ– 'f64'")
            
        # å¦‚æœé¸æ“‡ polars æ ¼å¼ï¼Œç¢ºä¿å·²å®‰è£ polars
        if output_format == "polars":
            try:
                import polars as pl
            except ImportError:
                raise ImportError("è¦ä½¿ç”¨ polars è¼¸å‡ºï¼Œè«‹å…ˆå®‰è£ polars åº«: pip install polars")
        
        # å…§éƒ¨é‡æ¡æ¨£å‡½æ•¸
        def resampler(df, resample_param, **inner_kwargs):
            if resample_param is None:
                return df
            elif isinstance(resample_param, Iterable) and not isinstance(resample_param, str):
                # å¦‚æœæ˜¯å¯è¿­ä»£å°è±¡ï¼ˆå¦‚åˆ—è¡¨ã€ç´¢å¼•ï¼‰ï¼Œä½¿ç”¨ reindex
                return df.reindex(resample_param, method='ffill')
            else:
                # å¦‚æœæ˜¯å­—ä¸²ï¼ˆå¦‚ '1D', '1H'ï¼‰ï¼Œä½¿ç”¨ resample
                return df.resample(resample_param, closed='right', label='right', **inner_kwargs).last()
        
        if len(features) == 0:
            return pd.DataFrame() if output_format == "pandas" else pl.DataFrame()
        
        unstacked = {}
        union_index = None
        union_columns = None
        concats = []
        
        # è™•ç†ç‰¹å¾µè³‡æ–™æ¡†
        for name, df in features.items():
            # è™•ç†å¯èª¿ç”¨å°è±¡
            if callable(df):
                df = df()
                
            if isinstance(df.index, pd.MultiIndex):
                concats.append(df)
            else:
                # æª¢æŸ¥æ˜¯å¦ç‚º FinlabDataFrame é¡å‹
                if hasattr(df, 'index_str_to_date'):
                    df = df.index_str_to_date()
                    
                # æ‡‰ç”¨é‡æ¡æ¨£
                udf = resampler(df, resample, **kwargs)
                unstacked[name] = udf
                
                # è¨ˆç®—è¯é›†ç´¢å¼•å’Œäº¤é›†æ¬„ä½
                if union_index is not None:
                    union_index = union_index.union(udf.index)
                else:
                    union_index = udf.index
                    
                if union_columns is not None:
                    union_columns = union_columns.intersection(udf.columns)
                else:
                    union_columns = udf.columns
        
        final_index = None
        
        # è™•ç† unstacked è³‡æ–™
        for name, udf in unstacked.items():
            udf = udf\
                .reindex(index=union_index, columns=union_columns)\
                .ffill()\
                .T\
                .unstack()
            unstacked[name] = udf.values
            
            if final_index is None:
                final_index = udf.index
        
        # è™•ç†å·²æœ‰ MultiIndex çš„ DataFrame
        for i, c in enumerate(concats):
            c.index = c.index.set_names(['datetime', 'instrument'])
            if union_index is not None:
                concats[i] = c[c.index.get_level_values('datetime').isin(union_index)]
        
        # åˆä½µæ‰€æœ‰è³‡æ–™
        if unstacked:
            unstack_df = pd.DataFrame(unstacked, index=final_index)
            unstack_df.index = unstack_df.index.set_names(['datetime', 'instrument'])
            concats.append(unstack_df)
        
        ret = pd.concat(concats, axis=1)
        ret.sort_index(inplace=True)
        
        # æ‡‰ç”¨æ¨£æœ¬éæ¿¾å™¨
        if sample_filter is not None:
            if hasattr(sample_filter, 'index_str_to_date'):
                sample_filter = sample_filter.index_str_to_date()
                
            usf = resampler(sample_filter, resample, **kwargs)
            
            if union_index is not None and union_columns is not None:
                usf = usf.reindex(index=union_index, columns=union_columns)
                
            usf = usf.ffill()\
               .T\
               .unstack()\
               .reindex(ret.index)\
               .astype(bool).fillna(False)
               
            ret = ret.loc[usf.values]
        
        # ç²¾åº¦å£“ç¸®
        if precision == "f16":
            for col in ret.select_dtypes(include=['float64', 'float32']).columns:
                ret[col] = ret[col].astype(np.float16)
        elif precision == "f32":
            for col in ret.select_dtypes(include=['float64']).columns:
                ret[col] = ret[col].astype(np.float32)
        
        # å¦‚æœè¦æ±‚ polars è¼¸å‡ºï¼Œå‰‡è½‰æ›ç‚º polars DataFrame
        if output_format == "polars":
            # è™•ç† MultiIndex
            if isinstance(ret.index, pd.MultiIndex):
                df_reset = ret.reset_index()
                pl_df = pl.from_pandas(df_reset)
            else:
                pl_df = pl.from_pandas(ret)
            
            # åœ¨ Polars ä¸­æ‡‰ç”¨ç²¾åº¦å£“ç¸®
            if precision == "f16":
                # Polars æ²’æœ‰ float16ï¼Œä½¿ç”¨ float32
                for col in pl_df.columns:
                    if pl_df[col].dtype in [pl.Float64, pl.Float32]:
                        pl_df = pl_df.with_columns(pl_df[col].cast(pl.Float32))
            elif precision == "f32":
                for col in pl_df.columns:
                    if pl_df[col].dtype == pl.Float64:
                        pl_df = pl_df.with_columns(pl_df[col].cast(pl.Float32))
            
            # çµ±ä¸€è¨˜æ†¶é«”æ¸…ç†
            try:
                del ret, df_reset, unstacked, concats, unstack_df
            except:
                pass
            gc.collect()
            
            return pl_df
        
        # çµ±ä¸€è¨˜æ†¶é«”æ¸…ç†
        try:
            del unstacked, concats, unstack_df
        except:
            pass
        gc.collect()
        
        return ret

 #ä¸€èˆ¬åˆ†æå€------------------------------------------------------------------------------------------------

    def create_factor_data(self, factor, adj_close, days=[1,2], event=None):
        """
        åŸå§‹ finlab çš„ create_factor_data å‡½æ•¸
        """
        factor = {'factor':factor} if isinstance(factor, pd.DataFrame) else factor

        ref = next(iter(factor.values())) if event is None else event
        ref = ref[~ref.index.isna()]

        sids = adj_close.columns.intersection(ref.columns)
        dates = adj_close.index.intersection(
            FinlabDataFrame.to_business_day(ref.index))
        
        ret = {}
        for name, f in factor.items():
            reashaped_f = f.reindex(dates, method='ffill').reindex(columns=sids)
            ret[f'{name}_factor'] = reashaped_f.unstack().values
            ret[f'{name}_factor_quantile'] = (reashaped_f.rank(axis=1, pct=True) // 0.2).unstack().values

        total_index = None
        for d in days:
            temp = (adj_close.shift(-d-1) / adj_close.shift(-1) - 1)\
                .reindex(index=dates, method='ffill').reindex(columns=sids)\
                .unstack()
            
            ret[f"{d}D"] = temp.values
            total_index = temp.index

        if event is not None:
            event = event[event.index.notna()]
            reshaped_event = event.reindex(index=dates, method='ffill').reindex(columns=sids)
            ret['event'] = reshaped_event.unstack().values


        ret = pd.DataFrame(ret, index=total_index.swaplevel(0, 1))\
            .replace([-np.inf, np.inf], np.nan)\
            .dropna()

        if 'event' in ret:
            ret = ret[ret['event'] == True]
            ret.drop(columns=['event'], inplace=True)

        ret.index.names = ['date', 'asset']
        return ret

    def co_event_analysis(self, buy:"dataframe"):
        """
        ç”¨finlabåŸå§‹ç¨‹å¼ç¢¼æ”¹çš„,ç”¨ä»¥åˆ†æäº‹ä»¶ç™¼ç”Ÿå‰å¾Œä¹‹å ±é…¬ç‡è®ŠåŒ–
        æ³¨æ„:
        
        1.è¨˜æ†¶é«”é‡ä¸å¤ å¯èƒ½æœƒéŒ¯èª¤
        2.cross over
        
        åƒè€ƒ:
        https://www.finlab.tw/event-study-usage/
        https://doc.finlab.tw/reference/tools/
        """
        adj_close = self.get('etl:adj_close')
        factor_data = self.create_factor_data(buy, adj_close, event=buy)
        buy_time_distribution = pd.DataFrame(buy.sum(axis=1)).reset_index() 
        buy_time_distribution.rename(columns = {0:'number of times'}, inplace = True)
        buy_time_distribution
        fig1 = px.area(buy_time_distribution, x="date", y="number of times",color="number of times",
                     title="äº‹ä»¶ç™¼ç”Ÿæ¬¡æ•¸èˆ‡æ—¥æœŸåˆ†å¸ƒ")
        fig1.show()
        
        #ç”¨åŠ æ¬ŠæŒ‡æ•¸ç•¶æˆbenchmark,æ’é™¤åŠ æ¬ŠæŒ‡æ•¸æ™‚é–“è®Šå› 
        benchmark = self.get('benchmark_return:ç™¼è¡Œé‡åŠ æ¬Šè‚¡åƒ¹å ±é…¬æŒ‡æ•¸')
        benchmark_pct = benchmark.reindex(adj_close.index, method='ffill').pct_change()
        stock_pct = adj_close.pct_change()
        def get_period(df, date, sample):
            i = df.index.get_loc(date)
            return df.iloc[i+sample[0]: i+sample[1]].values
        
        #è½‰æ›æˆ,ç¨ç«‹äº‹ä»¶èˆ‡æ™‚é–“å ±é…¬ç‡
        ret = []
        sample_period=(-40, -20) #
        estimation_period=(-15, 30)# è§€å¯Ÿäº‹ä»¶å‰15æ—¥èˆ‡å¾Œ30æ—¥è®ŠåŒ–
        for date, sid in tqdm.tqdm(factor_data.index):
        
            X1, Y1 = get_period(benchmark_pct, date, sample_period)[:,0], \
                get_period(stock_pct[sid], date, sample_period)
            X2, Y2 = get_period(benchmark_pct, date, estimation_period)[:,0], \
                get_period(stock_pct[sid], date, estimation_period)
        
            # Run CAPM
            cov_matrix = np.cov(Y1, X1)
            beta = cov_matrix[0, 1] / cov_matrix[1, 1]
            AR = np.array(Y2) - beta * X2
            ret.append(AR)
        #è¨ˆç®—äº‹ä»¶ç™¼ç”Ÿæ—¥å‰å¾Œçš„æ—¥å ±é…¬ç‡è®ŠåŒ–\
        ret = pd.DataFrame(ret, columns=range(*estimation_period))
        
        # range_estimation_period_begin =  estimation_period[0]
        # range_estimation_period_end = len(ret[0])
        # if range_estimation_period_end>30:
        #     range_estimation_period_end =30
        # ret = pd.DataFrame(ret, columns=range(range_estimation_period_begin,range_estimation_period_end))
            
        ret_df = pd.DataFrame(ret.mul(100).mean()).reset_index() 
        ret_df_re = ret_df.rename(columns = {"index":"days",0:"return"})
        ret_df_re
        fig2 = px.bar(ret_df_re, x="days", y="return",color="return",
                     title="äº‹ä»¶ç™¼ç”Ÿæ—¥å‰å¾Œçš„æ—¥å ±é…¬ç‡è®ŠåŒ–")
        # fig.add_trace(go.Scatter(
        #     x=list(ret_df_re["days"]),
        #     y=list(ret_df_re["return"]),
        #     xperiod="M1",
        #     xperiodalignment="middle",
        #     hovertemplate="%{y}%{_xother}"
        # ))
        fig2.show()
        
        #è¨ˆç®—ç´¯è¨ˆå ±é…¬ç‡,ä¸¦å°‡äº‹ä»¶ç™¼ç”Ÿæ—¥ä½œåŸºæº–é»
        accum_ret_df = pd.DataFrame(ret.mul(100).cumsum(axis=1).mean()).reset_index() 
        accum_ret_df_re = accum_ret_df.rename(columns = {"index":"days",0:"return"})
        accum_ret_df_re["return_accumulated"] = accum_ret_df_re["return"] -accum_ret_df_re.at[15,"return"]
        std = ret.mul(100).cumsum(axis=1).std() * 0.1
        accum_ret_df_re
        fig3 = px.line(accum_ret_df_re, x="days", y="return_accumulated",
                 title="ç´¯è¨ˆå ±é…¬ç‡,ä»¥äº‹ä»¶ç™¼ç”Ÿæ—¥ä½œåŸºæº–é»")
        
        fig3.show()
        return accum_ret_df_re   



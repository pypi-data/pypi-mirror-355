Metadata-Version: 2.1
Name: lchemme
Version: 0.0.3
Summary: Training and applying large chemistry models for embeddings.
Author-email: Eachan Johnson <eachan.johnson@crick.ac.uk>
License: MIT License
        
        Copyright (c) [year] [fullname]
        
        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.
Project-URL: Homepage, https://github.com/scbirlab/lchemme
Project-URL: Bug Tracker, https://github.com/scbirlab/lchemme/issues
Keywords: science,chemistry,machine learning,analysis,cheminformatics,large language models,ai
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Chemistry
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3 :: Only
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: carabiner-tools[mpl,pd]>=0.0.4
Requires-Dist: datasets>=3.1.0
Requires-Dist: openpyxl==3.1.0
Requires-Dist: pandas
Requires-Dist: schemist>=0.0.4.post1
Requires-Dist: tensorboard
Requires-Dist: tokenizers
Requires-Dist: transformers[torch]
Requires-Dist: umap-learn

# ðŸš„ lchemme

![GitHub Workflow Status (with branch)](https://img.shields.io/github/actions/workflow/status/scbirlab/lchemme/python-publish.yml)
![PyPI - Python Version](https://img.shields.io/pypi/pyversions/lchemme)
![PyPI](https://img.shields.io/pypi/v/lchemme)

Pretraining large chemistry models for embedding.

- [Installation](#installation)
- [Command-line usage](#command-line-usage)
    - [Example](#example)
    - [Other commands](#other-commands)
- [Python API](#python-api)
- [Documentation](#documentation)

## Installation

### The easy way

Install the pre-compiled version from PyPI:

```bash
pip install lchemme
```

### From source

Clone the repository, then `cd` into it. Then run:

```bash
pip install -e .
```

## Command-line usage

**lchemme**  provides command-line utlities to pre-train BART models.

To get a list of commands (tools), do

```bash
$ lchemme --help
usage: lchemme [-h] [--version] {tokenize,pretrain,featurize} ...

Training and applying large chemistry models.

options:
  -h, --help            show this help message and exit
  --version, -v         show program's version number and exit

Sub-commands:
  {tokenize,pretrain,featurize}
                        Use these commands to specify the tool you want to use.
    tokenize            Tokenize the data inputs.
    pretrain            Pre-train a large language model using self-supervised learning.
    featurize           Get vector embeddings of a chemical dataset using a pre-trained large language model.
```

And to get help for a specific command, do

```bash
$ lchemme <command> --help
```

### Tokenizing

The first step is to build a tokenizer for your dataset. **LChemME** works with BART models,
and pulls their architecture from the [Hugging Face Hub](https://huggingface.co/models) or 
a local directory. Training data can also be pulled from [Hugging Face Hub](https://huggingface.co/datasets)
with the `hf://` prefix, or it can be loaded from local CSV files with a column containing
SMILES strings.

```bash
lchemme tokenize \
    --train hf://scbirlab/fang-2023-biogen-adme@scaffold-split:train \
    --column smiles \
    --model facebook/bart-base \
    --output my-model
```

This should be relatively fast, but could take several hours for millions of rows.

In principle, existing tokenizers trained on natural language could work, but they have
much larger vocabularies which are largely unused in SMILES.

### Model pretraining

**LChemME** performs semi-supervised pretraining on a SMILES canonicalization task.
This requires an understanding of chemical connectivity and atom precedence rules,
forcing the model to build an internal representation of the chemical graph.

```bash
lchemme pretrain \
    --train hf://scbirlab/fang-2023-biogen-adme@scaffold-split:train \
    --column smiles \
    --test hf://scbirlab/fang-2023-biogen-adme@scaffold-split:test \
    --model facebook/bart-base \
    --tokenizer my-model \
    --epochs 0.5 \
    --output my-model \
    --plot my-model/training-log
```

If you want to continue training, you can do so with the `--resume` flag.

```bash
lchemme pretrain \
    --train hf://scbirlab/fang-2023-biogen-adme@scaffold-split:train \
    --column smiles \
    --test hf://scbirlab/fang-2023-biogen-adme@scaffold-split:test \
    --model my-model \
    --epochs 0.5 \
    --output my-model \
    --plot my-model/training-log \
    --resume
```

The dataset state can only be restored if the `--model` was trained with **LChemME**
and the dataset configuration is identical, i.e. `--train`, `--column` are the same.

### Featurizing

With a trained model, you can generate embeddings of your chemical datasets,
optionally with UMAP plots colored by chemical properties.

```bash
lchemme featurize \
    --train hf://scbirlab/fang-2023-biogen-adme@scaffold-split:train \
    --column smiles \
    --model my-model \
    --batch-size 16 \
    --method mean \
    --plot umap \
> featurized.csv
```

You can specify one or several aggregation functions with `--method`. **LChemME**
aggregates the sequence dimension of the encoder and decoder, then concatenates
them.

If you want to use additional columns containing numerical values to color the UMAP 
plots, provide the column names under `--extras`.

## Documentation

(Full API documentation to come at [ReadTheDocs](https://lchemme.readthedocs.org).)

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üß† Building & Evaluating Complex Agents with `strands` and `flotorch-eval`\n",
        "\n",
        "In this notebook, we'll walk through a complete example of evaluating agents using the **`flotorch-eval`** package across key metrics. These metrics help assess both agent quality and system performance.\n",
        "\n",
        "---\n",
        "\n",
        "#### üîç Evaluation Metrics\n",
        "\n",
        "- **`AgentGoalAccuracyMetric`**  \n",
        "  Evaluates whether the agent successfully understood and achieved the user's goal.  \n",
        "  - **Binary** (1 = goal achieved, 0 = not achieved)\n",
        "\n",
        "- **`ToolCallAccuracyMetric`**  \n",
        "  Measures the correctness of tool usage by the agent‚Äîi.e., whether the agent called the right tools to complete a task.  \n",
        "  - **Binary** (1 = relevant tools invoked, 0 = relevant tools not invoked)\n",
        "\n",
        "- **`TrajectoryEvalWithLLM`**  \n",
        "  Evaluates whether the trajectory (based on OpenTelemetry spans) is meaningful, either with or without a reference trajectory.  \n",
        "  - **Binary** (1 = meaningful, 0 = invalid)\n",
        "\n",
        "- **`LatencyMetric`**  \n",
        "  Measures agent latency‚Äîhow fast the agent responds or completes tasks.  \n",
        "  \n",
        "\n",
        "- **`UsageMetric`**  \n",
        "  Evaluates the cost-efficiency of the agent in terms of compute, tokens, or other usage dimensions.  \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Setup and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install numpy pandas langchain-aws ragas openlit -q\n",
        "!pip install flotorch-eval crewai duckduckgo-search uv -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Configure Tracing with OpenLit\n",
        "To evaluate our agent, we first need to record what it does. We'll use OpenLit to automatically create a detailed trace of the agent's execution, including every LLM call and tool usage.\n",
        "\n",
        "We'll store these traces in memory for easy access during the evaluation phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from opentelemetry import trace\n",
        "from opentelemetry.sdk.trace import TracerProvider\n",
        "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
        "from opentelemetry.sdk.trace.export.in_memory_span_exporter import InMemorySpanExporter\n",
        "\n",
        "# Create an in-memory span exporter\n",
        "memory_exporter = InMemorySpanExporter()\n",
        "span_processor = SimpleSpanProcessor(memory_exporter)\n",
        "\n",
        "# Set up the tracer provider and add the span processor\n",
        "tracer_provider = TracerProvider()\n",
        "tracer_provider.add_span_processor(span_processor)\n",
        "trace.set_tracer_provider(tracer_provider)\n",
        "\n",
        "# Initialize OpenLit - this will automatically instrument CrewAI when it's imported\n",
        "import openlit\n",
        "openlit.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../../..\")))\n",
        "\n",
        "# Import required libraries\n",
        "from typing import List\n",
        "import asyncio\n",
        "\n",
        "from crewai.tools import tool\n",
        "from crewai import LLM\n",
        "from crewai import Agent, Task, Crew\n",
        "from duckduckgo_search import DDGS\n",
        "from langchain_aws import ChatBedrockConverse\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "from flotorch_eval.agent_eval.core.evaluator import Evaluator\n",
        "from flotorch_eval.agent_eval.metrics.base import BaseMetric\n",
        "from flotorch_eval.agent_eval.metrics.langchain_metrics import (\n",
        "    TrajectoryEvalWithLLMMetric,\n",
        "    TrajectoryEvalWithoutLLMMetric)\n",
        "from flotorch_eval.agent_eval.metrics.ragas_metrics import (\n",
        "    AgentGoalAccuracyMetric,\n",
        "    ToolCallAccuracyMetric,)\n",
        "from flotorch_eval.agent_eval.metrics.latency_metrics import LatencyMetric\n",
        "from flotorch_eval.agent_eval.metrics.base import MetricConfig\n",
        "from flotorch_eval.agent_eval.metrics.usage_metrics import UsageMetric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation Setup\n",
        "Before we build the agent, let's set up the components needed for its evaluation.\n",
        "\n",
        "##### Configure the LLM Judge\n",
        "Some of our metrics require an LLM to \"judge\" the agent's output for quality and correctness.  \n",
        "For this, we're choosing **`Amazon Nova Micro`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "region = \"us-east-1\"\n",
        "bedrock_model = ChatBedrockConverse(\n",
        "    region_name=region,\n",
        "    endpoint_url=f\"https://bedrock-runtime.us-east-1.amazonaws.com\",\n",
        "    model_id=\"us.amazon.nova-micro-v1:0\"\n",
        ")\n",
        "\n",
        "llm_judge = LangchainLLMWrapper(bedrock_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define the Evaluation Logic\n",
        "This helper function orchestrates the evaluation process. It takes the captured traces (spans) and the evaluation metrics, runs the evaluator, and displays the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from evaluation_utils import create_trajectory, initialize_evaluator, display_evaluation_results\n",
        "\n",
        "async def evaluate_agent(metrics: List[BaseMetric], spans: List):\n",
        "    \"\"\"\n",
        "    Runs an agent with a given prompt, captures its trace, evaluates it,\n",
        "    and displays the results.\n",
        "\n",
        "    Args:\n",
        "        agent: The agent to be evaluated.\n",
        "        prompt: The input prompt for the agent.\n",
        "        metrics: A list of configured metrics for evaluation.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Capture and convert the trace\n",
        "    if not spans:\n",
        "        print(\"\\nEvaluation failed: No spans were provided.\")\n",
        "        return\n",
        "\n",
        "    trajectory = create_trajectory(spans)\n",
        "\n",
        "    # 2. Initialize and run the evaluator\n",
        "    evaluator = initialize_evaluator(metrics)\n",
        "    print(\"\\n--- Running Evaluation ---\")\n",
        "    results = await evaluator.evaluate(trajectory)\n",
        "\n",
        "    # 3. Display results\n",
        "    print(\"\\n--- Evaluation Scores ---\")\n",
        "    display_evaluation_results(results)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use case: AWS Tech Agent\n",
        "\n",
        "Now, let's build, run, and evaluate our agent.\n",
        "\n",
        "#### Build the Agent\n",
        "We'll define the agent's tools, its LLM, its role, and the task it needs to perform.\n",
        "\n",
        "**`Tools`**: It will have a DuckDuckGoSearch tool to look up information, Salesforce API to get data  \n",
        "**`LLM`**: It will be powered by the amazon.nova-pro-v1:0 model on Amazon Bedrock.  \n",
        "**`Role`**: Its purpose is to be a Writer that simplifies GenAI concepts on AWS for beginner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the tools that will be used by the agents\n",
        "@tool('DuckDuckGoSearch')\n",
        "def search_tool(search_query: str):\n",
        "    \"\"\"Search the web for information on a given topic\"\"\"\n",
        "    return DDGS().text(search_query, max_results=5)\n",
        "\n",
        "@tool('SalesforceIntegration')\n",
        "def salesforce_tool(soql_query: str):\n",
        "    \"\"\"Call Salesforce API to get data\"\"\"\n",
        "    return \"Salesforce Integration\"\n",
        "\n",
        "# setup the model that is going to be used with the model\n",
        "model = LLM(\n",
        "    # model=\"sagemaker/INSERT ENDPOINT NAME\",\n",
        "    model=\"bedrock/us.amazon.nova-pro-v1:0\",\n",
        "    temperature=0.7, max_tokens=4*1024,\n",
        ")\n",
        "\n",
        "writer = Agent(\n",
        "        role=\"Writer\",\n",
        "        goal=\"You make GenAI concepts understandable for newbies exploring GenAI on AWS\",\n",
        "        backstory=\"You're an expert in writing crisp summaries about GenAI on AWS.\",\n",
        "        tools=[search_tool],\n",
        "        llm=model\n",
        "    )\n",
        "\n",
        "task = Task(description=(\"What is {topic}?\"),\n",
        "            expected_output=(\"Compose a short summary that includes the answer.\"),\n",
        "            agent=writer)\n",
        "\n",
        "crew = Crew(\n",
        "  agents=[writer],\n",
        "  tasks=[task],\n",
        "  share_crew=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run the Agent\n",
        "Let's kickoff the crew to perform its task with a topic. OpenLit will automatically capture the entire execution in the background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = crew.kickoff({\"topic\": \"AWS Bedrock\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spans = memory_exporter.get_finished_spans()\n",
        "print(\"Number of spans:\", len(spans))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the agent where no ground-truth answers or trajectories are provided. Instead, rely solely on an LLM-based evaluator to assess the agent's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = [\n",
        "    ToolCallAccuracyMetric(),\n",
        "    AgentGoalAccuracyMetric(llm=llm_judge),\n",
        "    TrajectoryEvalWithLLMMetric(llm = bedrock_model),\n",
        "    UsageMetric(config=MetricConfig(\n",
        "        metric_params={\"aws_region\": \"us-east-1\"}\n",
        "    )),\n",
        "    LatencyMetric()\n",
        "    ]\n",
        "\n",
        "# Evaluate trajectory\n",
        "async def evaluate_dataframe_agent():\n",
        "    return await evaluate_agent(metrics, spans)\n",
        "\n",
        "# Execute the evaluation\n",
        "results = asyncio.run(evaluate_dataframe_agent())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define Ground Truth for Evaluation\n",
        "Evaluate the agent using reference data, including the expected agent response and trajectory. Use an LLM to compare the agent's output against this reference to assess performance.\n",
        "\n",
        "**`A reference answer`**: The ideal final output we expect.  \n",
        "**`A reference trajectory`**: The ideal sequence of thoughts and tool calls the agent should have taken."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "REFERENCE_FINAL_ANSWER=\"Amazon Bedrock is a fully managed service that makes it easy to use foundation models from third-party providers and Amazon. It allows users to build generative AI applications with a choice of foundation models from different AI companies, using a single API. Users can customize these models with their data, orchestrate multistep tasks, trace reasoning, and apply guardrails for responsible AI. Additionally, Amazon Bedrock enables the creation of generative AI workflows by connecting its features with other AWS services.\"\n",
        "\n",
        "REFERENCE_TRAJECTORY_OUTPUTS = [\n",
        "                {\"role\": \"user\", \"content\": \"What is AWS Bedrock?\"},\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": \"To compose a poem about Amazon Bedrock, I first need to gather information about what Amazon Bedrock is. I will use the available tool to search for this information.\",\n",
        "                    \"tool_calls\": [\n",
        "                        {\n",
        "                            \"function\": {\n",
        "                                \"name\": \"Search the web for information on a given topic\",\n",
        "                                \"arguments\": \"{\\\"search_query\\\": \\\"Amazon Bedrock\\\"}\"\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                },\n",
        "                {\"role\": \"tool\", \"content\": \"{\\\"searchParameters\\\": {\\\"q\\\": \\\"Amazon Bedrock\\\", \\\"type\\\": \\\"search\\\", \\\"num\\\": 5, \\\"engine\\\": \\\"google\\\"}, \\\"organic\\\": [{\\\"title\\\": \\\"Amazon Bedrock - Generative AI - AWS\\\", \\\"link\\\": \\\"https://aws.amazon.com/bedrock/\\\", \\\"snippet\\\": \\\"Amazon Bedrock Data Automation streamlines the generation of valuable insights from unstructured multimodal content such as documents, images, audio, and videos ...\\\", \\\"position\\\": 1, \\\"sitelinks\\\": [{\\\"title\\\": \\\"Amazon Bedrock\\\", \\\"link\\\": \\\"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html\\\"}, {\\\"title\\\": \\\"Amazon Bedrock Pricing\\\", \\\"link\\\": \\\"https://aws.amazon.com/bedrock/pricing/\\\"}, {\\\"title\\\": \\\"Amazon Bedrock Documentation\\\", \\\"link\\\": \\\"https://docs.aws.amazon.com/bedrock/\\\"}, {\\\"title\\\": \\\"Amazon Bedrock FAQs\\\", \\\"link\\\": \\\"https://aws.amazon.com/bedrock/faqs/\\\"}, {\\\"title\\\": \\\"Amazon Bedrock Agents\\\", \\\"link\\\": \\\"https://aws.amazon.com/bedrock/agents/\\\"}]}, {\\\"title\\\": \\\"Getting Started with Amazon Bedrock - AWS\\\", \\\"link\\\": \\\"https://aws.amazon.com/awstv/watch/6ff4cd6fa97/\\\", \\\"snippet\\\": \\\"So check the region that you're currently in, make sure it's a region that's supported by Bedrock. Then I'm gonna scroll to the bottom of this ...\\\", \\\"position\\\": 2}], \\\"relatedSearches\\\": [{\\\"query\\\": \\\"Amazon Bedrock pricing\\\"}, {\\\"query\\\": \\\"Amazon Bedrock documentation\\\"}, {\\\"query\\\": \\\"Amazon Bedrock Claude\\\"}, {\\\"query\\\": \\\"Amazon Bedrock logo\\\"}, {\\\"query\\\": \\\"Amazon Bedrock DeepSeek\\\"}], \\\"credits\\\": 1}\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"Based on the observation, I have learned that mazon Bedrock is a fully managed service that makes it easy to use foundation models from third-party providers and Amazon. It allows users to build generative AI applications with a choice of foundation models from different AI companies, using a single API. Users can customize these models with their data, orchestrate multistep tasks, trace reasoning, and apply guardrails for responsible AI. Additionally, Amazon Bedrock enables the creation of generative AI workflows by connecting its features with other AWS services.\"}\n",
        "            ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run the Evaluation\n",
        "Finally, we retrieve the captured traces (spans), configure our evaluation metrics, and run the evaluate_agent function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spans = memory_exporter.get_finished_spans()\n",
        "print(\"Number of spans:\", len(spans))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = [\n",
        "    ToolCallAccuracyMetric(),\n",
        "    AgentGoalAccuracyMetric(llm=llm_judge,config=MetricConfig(\n",
        "        metric_params={\n",
        "            \"reference_answer\": REFERENCE_FINAL_ANSWER\n",
        "            }\n",
        "        )),\n",
        "    TrajectoryEvalWithLLMMetric(llm = bedrock_model,config=MetricConfig(\n",
        "        metric_params={\n",
        "            \"reference_outputs\": REFERENCE_TRAJECTORY_OUTPUTS\n",
        "        }\n",
        "    )),\n",
        "    UsageMetric(config=MetricConfig(\n",
        "        metric_params={\"aws_region\": \"us-east-1\"}\n",
        "    )),\n",
        "    LatencyMetric()\n",
        "    ]\n",
        "\n",
        "# Evaluate trajectory\n",
        "async def evaluate_dataframe_agent():\n",
        "    return await evaluate_agent(metrics, spans)\n",
        "\n",
        "# Execute the evaluation\n",
        "results = asyncio.run(evaluate_dataframe_agent())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
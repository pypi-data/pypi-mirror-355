from pydantic_ai import Agent, RunContext
from pydantic import BaseModel, Field
from typing import Type, Any, Optional, List # Added Optional, Type, List
from datetime import datetime # Added for dynamic prompt example

from .config import get_pydantic_ai_model, settings # Import the new model loader
from alo_pyai_sdk.core.llm_loader import load_mcp_servers_from_project_config # For MCP servers
from pydantic_ai.mcp import MCPServer # For type hinting
# from . import tools # Uncomment if you have a tools.py

# --- Define Agent Output Type (Example) ---
# Replace this with your desired output type or use str (default)
class {{ agent_name | capitalize }}Output(BaseModel):
    response: str
    # Add other fields as needed by your agent's structured output

# --- Define Agent Dependencies Type (Example, if needed) ---
# class {{ agent_name | capitalize }}Deps(BaseModel):
#     # database_url: str
#     # external_api_key: str
#     pass # Replace with actual dependencies

# --- Initialize the Pydantic-AI Agent ---
# The LLM configuration will ideally be loaded from the global alo_config.yaml
# based on `settings.LLM_CONFIG_NAME`.
# For this template, we'll use a placeholder or rely on environment variables
# if direct Pydantic-AI model string (e.g., "openai:gpt-4o") is used.

from pydantic_ai import models # Ensure models is imported for the fallback

# Load the configured Pydantic-AI model instance
try:
    # The LLM_CONFIG_NAME is passed during generation and should match a key in alo_config.yaml
    configured_model = get_pydantic_ai_model(settings.LLM_CONFIG_NAME)
    print(f"Agent '{{ agent_name }}' initialized with LLM config: {settings.LLM_CONFIG_NAME}")
except ValueError as e: # Specific error from get_pydantic_ai_model if config not found
    print(f"Warning: {e}") # This will print "LLM configuration '...' not found..."
    print(f"Falling back to model identifier: {settings.LLM_MODEL_IDENTIFIER_FALLBACK}")
    try:
        # Attempt to use infer_model for the fallback.
        # This relies on Pydantic-AI's ability to find API keys from env vars
        # if the provider requires them and they are not in the model identifier string.
        configured_model = models.infer_model(settings.LLM_MODEL_IDENTIFIER_FALLBACK)
        print(f"Agent '{{ agent_name }}' initialized with fallback LLM: {settings.LLM_MODEL_IDENTIFIER_FALLBACK}")
    except Exception as fallback_e:
        print(f"Critical: Could not initialize fallback model '{settings.LLM_MODEL_IDENTIFIER_FALLBACK}': {fallback_e}")
        print("Please ensure your primary LLM config is correct or the fallback model can be initialized (e.g., API key in env).")
        # As a last resort, use TestModel to allow the application to start
        from pydantic_ai.models.test import TestModel
        configured_model = TestModel()
        print(f"Agent '{{ agent_name }}' initialized with TestModel due to previous errors.")
except Exception as e: # Catch any other unexpected errors during get_pydantic_ai_model
    print(f"Unexpected error loading LLM configuration '{settings.LLM_CONFIG_NAME}': {e}")
    print(f"Falling back to model identifier: {settings.LLM_MODEL_IDENTIFIER_FALLBACK}")
    try:
        configured_model = models.infer_model(settings.LLM_MODEL_IDENTIFIER_FALLBACK)
        print(f"Agent '{{ agent_name }}' initialized with fallback LLM: {settings.LLM_MODEL_IDENTIFIER_FALLBACK}")
    except Exception as fallback_e:
        print(f"Critical: Could not initialize fallback model '{settings.LLM_MODEL_IDENTIFIER_FALLBACK}': {fallback_e}")
        from pydantic_ai.models.test import TestModel
        configured_model = TestModel()
        print(f"Agent '{{ agent_name }}' initialized with TestModel due to previous errors.")


_output_type_actual: Type[BaseModel] | Type[str]
if "{{ agent_output_type_is_str }}" == "True":
    _output_type_actual = str
else:
    # Attempt to create a dynamic Pydantic model for the output type
    # This allows users to specify a simple name and have a basic model generated.
    # For complex outputs, users should define this model properly, perhaps in a shared 'models.py'.
    try:
        # This will create a new class {{ agent_output_type }} that inherits from BaseModel
        # with a default 'detail: str' field.
        _output_type_actual = type("{{ agent_output_type }}", (BaseModel,), {"detail": (str, Field(description="Default output field"))})
    except Exception:
        # Fallback if type creation fails for some reason (e.g. invalid name)
        class PlaceholderOutput(BaseModel):
            data: Any = Field(description="Placeholder for {{ agent_output_type }}")
        _output_type_actual = PlaceholderOutput
        print(f"Warning: Could not dynamically create output type '{{ agent_output_type }}', using generic placeholder. Please define it properly.")

_deps_type_actual: Type[Any] = Any # Defaulting to Any, user should customize

# Load MCP Servers from project configuration
mcp_servers_list: List[MCPServer] = []
if settings.PROJECT_ROOT_PATH: # PROJECT_ROOT_PATH should be available from config.py
    try:
        mcp_servers_list = load_mcp_servers_from_project_config(settings.PROJECT_ROOT_PATH)
        if mcp_servers_list:
            print(f"Agent '{{ agent_name }}' loaded {len(mcp_servers_list)} MCP server(s) from configuration.")
    except Exception as e:
        print(f"Warning: Could not load MCP servers for agent '{{ agent_name }}': {e}")
else:
    print(f"Warning: PROJECT_ROOT_PATH not set in agent '{{ agent_name }}' config, cannot load MCP servers.")


agent = Agent(
    model=configured_model,
    output_type=_output_type_actual,
    deps_type=_deps_type_actual, # User should change 'Any' to their specific Deps type if needed
    system_prompt="""
    You are '{{ agent_name }}', an AI assistant.
    {{ agent_system_prompt_instructions | default("Please assist the user with their request.") }}
    """,
    mcp_servers=mcp_servers_list, # Use loaded MCP servers
    # tools=[tools.example_tool], # Uncomment and add your tools
    retries=settings.AGENT_RETRIES,
)

# --- Example Tool (if you create a tools.py) ---
# @agent.tool
# async def example_tool(ctx: RunContext[_deps_type_actual], query: str) -> str:
#     """
#     An example tool that the agent can use.
#     """
#     # Access dependencies via ctx.deps if deps_type is defined
#     # For example, if _deps_type_actual = MyAgentDependencies and it has a field 'db_conn':
#     # conn = ctx.deps.db_conn
#     # result = await conn.execute("SELECT ...")
#     return f"Tool processed query: {query}"

# Example of a dynamic system prompt using RunContext (if deps are defined)
# @agent.system_prompt
# async def dynamic_prompt_example(ctx: RunContext[_deps_type_actual]) -> str:
#     # if isinstance(ctx.deps, MyAgentDependencies): # Check if deps is of the expected type
#     #     return f"User ID is {ctx.deps.user_id}. Current date: {datetime.now().date()}"
#     return f"Current date: {datetime.now().date()}"


def get_agent() -> Agent:
    """Returns the configured Pydantic-AI agent instance."""
    return agent

if __name__ == "__main__":
    # For testing the agent definition directly
    # This section is for local testing and won't run when deployed via FastAPI
    print(f"Agent '{{ agent_name }}' initialized.")
    print(f"Model: {agent.model.model_name if agent.model else 'Not specified'}") # type: ignore
    print(f"Output type: {agent.output_type}")
    print(f"Deps type: {agent._deps_type}") # Accessing protected member for demo

    # Example run (will require API keys to be set if not using a test model)
    # try:
    #     # test_deps = {{ agent_deps_type | default("None") }}() # Instantiate deps if needed
    #     # result = agent.run_sync("Hello, agent!", deps=test_deps)
    #     result = agent.run_sync("Hello, agent!")
    #     print("\nTest Run Output:")
    #     print(result.output)
    #     print("\nTest Run Usage:")
    #     print(result.usage())
    # except Exception as e:
    #     print(f"\nError during test run: {e}")
    #     print("Ensure your LLM API key (e.g., OPENAI_API_KEY) is set in the environment if using a real model.")
    pass

from pydantic_ai import Agent
from pydantic import BaseModel # For output_type example
from typing import Any # For deps_type example

from .config import settings
# from . import tools # Uncomment if you have a tools.py

# --- Define Agent Output Type (Example) ---
# Replace this with your desired output type or use str (default)
class {{ agent_name | capitalize }}Output(BaseModel):
    response: str
    # Add other fields as needed by your agent's structured output

# --- Define Agent Dependencies Type (Example, if needed) ---
# class {{ agent_name | capitalize }}Deps(BaseModel):
#     # database_url: str
#     # external_api_key: str
#     pass # Replace with actual dependencies

# --- Initialize the Pydantic-AI Agent ---
# The LLM configuration will ideally be loaded from the global alo_config.yaml
# based on `settings.LLM_CONFIG_NAME`.
# For this template, we'll use a placeholder or rely on environment variables
# if direct Pydantic-AI model string (e.g., "openai:gpt-4o") is used.

# TODO: Implement logic to load LLMConfig from global config using settings.LLM_CONFIG_NAME
# and then initialize the Pydantic-AI model object.
# For now, using a placeholder model string.
llm_model_identifier = settings.LLM_MODEL_IDENTIFIER_FALLBACK # e.g., "openai:gpt-4o"

agent = Agent(
    model=llm_model_identifier, # Replace with loaded model object if using complex config
    output_type={{ agent_output_type | default(agent_name | capitalize ~ "Output") }},
    # deps_type={{ agent_deps_type | default("Any") }}, # Uncomment and set if using dependencies
    system_prompt="""
    You are '{{ agent_name }}', an AI assistant.
    {{ agent_system_prompt_instructions | default("Please assist the user with their request.") }}
    """,
    # tools=[tools.example_tool], # Uncomment and add your tools
    retries=settings.AGENT_RETRIES,
)

# --- Example Tool (if you create a tools.py) ---
# @agent.tool
# async def example_tool(ctx: RunContext[{{ agent_deps_type | default("Any") }}], query: str) -> str:
#     """
#     An example tool that the agent can use.
#     """
#     # Access dependencies via ctx.deps if deps_type is defined
#     # print(f"Dependency example: {ctx.deps}")
#     return f"Tool processed query: {query}"

if __name__ == "__main__":
    # For testing the agent definition directly
    print(f"Agent '{{ agent_name }}' initialized with model: {llm_model_identifier}")
    print(f"Output type: {agent.output_type}")
    # print(f"Deps type: {agent._deps_type}") # Accessing protected member for demo

    # Example run (will require API keys to be set if not using a test model)
    # try:
    #     # test_deps = {{ agent_deps_type | default("None") }}() # Instantiate deps if needed
    #     # result = agent.run_sync("Hello, agent!", deps=test_deps)
    #     result = agent.run_sync("Hello, agent!")
    #     print("\nTest Run Output:")
    #     print(result.output)
    #     print("\nTest Run Usage:")
    #     print(result.usage())
    # except Exception as e:
    #     print(f"\nError during test run: {e}")
    #     print("Ensure your LLM API key (e.g., OPENAI_API_KEY) is set in the environment if using a real model.")
    pass

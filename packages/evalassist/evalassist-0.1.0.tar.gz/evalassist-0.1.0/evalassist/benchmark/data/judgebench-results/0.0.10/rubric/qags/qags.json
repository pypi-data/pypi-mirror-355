{
    "name": "qags",
    "description": "The benchmark involves a human-annotated dataset. This dataset includes a series of summaries evaluated for factual accuracy through a question-and-answer format. The benchmark assesses how effectively the dataset's annotations determine the consistency and correctness of the information presented in the summaries.",
    "link": "https://github.com/W4ngatang/qags/tree/master/data",
    "type": "rubric",
    "dataset": {
        "name": "judgebench-qags",
        "description": ""
    },
    "tags": [
        "Judgebench",
        "categorical"
    ],
    "criteriaBenchmarks": [
        {
            "name": "Factual Consistency",
            "evaluatorBenchmarks": [
                {
                    "evaluator_id": "Llama3-8b",
                    "laaj_version": "0.0.10",
                    "results": {
                        "agreement": 0.54,
                        "p_bias": 0.68,
                        "pearson": -0.102
                    }
                },
                {
                    "evaluator_id": "Llama3-70b",
                    "laaj_version": "0.0.10",
                    "results": {
                        "agreement": 0.53,
                        "p_bias": 0.72,
                        "pearson": -0.118
                    }
                },
                {
                    "evaluator_id": "GPT-4o",
                    "laaj_version": "0.0.10",
                    "results": {
                        "agreement": 0.6,
                        "p_bias": 0.33,
                        "pearson": 0.216
                    }
                }
            ]
        }
    ]
}

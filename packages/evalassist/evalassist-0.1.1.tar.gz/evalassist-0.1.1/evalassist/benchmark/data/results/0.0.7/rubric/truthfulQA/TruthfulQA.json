{
    "name": "TruthfulQA",
    "description": "TruthfulQA is a dataset to measure whether a language model is truthful in generating answers to questions. The dataset comprises 38 categories, including health, law, finance and politics. The language model has to decide whether the response is correct or whether it is incorrect.",
    "link": "https://huggingface.co/datasets/truthfulqa/truthful_qa",
    "type": "rubric",
    "dataset": {
        "name": "TruthfulQA",
        "description": ""
    },
    "tags": ["Truthfulness"],
    "criteriaBenchmarks": [
        {
            "name": "Truthfulness",
            "evaluatorBenchmarks": [
                {
                    "evaluator_id": "Llama3-8b",
                    "laaj_version": "0.0.7",
                    "results": {
                        "agreement": 0.61,
                        "p_bias": 0.146,
                        "pearson": 0.203
                    }
                },
                {
                    "evaluator_id": "Llama3-70b",
                    "laaj_version": "0.0.7",
                    "results": {
                        "agreement": 0.783,
                        "p_bias": 0.011,
                        "pearson": 0.562
                    }
                },
                {
                    "evaluator_id": "Mixtral",
                    "laaj_version": "0.0.7",
                    "results": {
                        "agreement": 0.671,
                        "p_bias": 0.006,
                        "pearson": 0.346
                    }
                },
                {
                    "evaluator_id": "Prometheus",
                    "laaj_version": "0.0.7",
                    "results": {
                        "agreement": 0.629,
                        "p_bias": 0.074,
                        "pearson": 0.263
                    }
                }
            ]
        }
    ]
}

def f2():
    """Возвращает список из 10 экзаменационных билетов."""
    return [
        "Билет 0: Метод функциональной итерации, метод степеней, ДПФ",
        "Билет 3: Метод функциональной итерации, метод степеней, ОДУ",
        "Билет 4: Метод Ньютона, умножение матриц, ДПФ",
        "Билет 5: Кубическая сплайн-интерполяция, умножение матриц",
        "Билет 7: Интерполяция Лагранжа, умножение матриц, ОДУ",
        "Билет 14",
        "Билет 15",
        "Билет 18",
        "Билет 29",
        "Билет 30"
    ]

def f2_0():
    """Билет 0: Метод функциональной итерации, метод степеней, ДПФ."""
    return """# Билет 0

## Задание 1: Метод функциональной итерации

В физике для нахождения точки равновесия решается уравнение: \( \sin(x) - 0.5x = 0 \). Решите это уравнение методом функциональной итерации с точностью \( 1e-5 \). Объясните, как параметр для ускорения сходимости влияет на скорость и стабильность метода. Приведите пример функции, где сходимость сильно замедляется или вообще не происходит.

### Решение

```python
import numpy as np

# === Задание 1: Метод функциональной итерации ===

# Уравнение sin(x) - 0.5 * x = 0
def g(x):
    return np.sin(x) / 0.5  # преобразуем уравнение в вид x = g(x)

# Метод простой итерации
def fixed_point_method(g, x0, tol=1e-5, max_iter=100):
    x = x0
    for i in range(max_iter):
        x_new = g(x)
        if abs(x_new - x) < tol:
            print(f"Сошлось за {i+1} итераций")
            return x_new
        x = x_new
    print("Не сошлось за максимальное число итераций.")
    return x

# Начальное приближение
x_start = 1.0

# Решение уравнения
solution = fixed_point_method(g, x_start)

print("=== Задание 1: Корень уравнения ===")
print(f"Решение x = {solution:.6f}")
```

**Результат**: Решение \( x \approx 1.895494 \), сошлось за 10 итераций.

**Пояснение**: Уравнение преобразуется в \( x = 2\sin(x) \). Для сходимости проверяем \( |g'(x)| = 2|\cos(x)| \). Около корня \( x \approx 1.895 \), \( |g'(x)| \approx 0.649 < 1 \), что обеспечивает сходимость.

**Теоретические вопросы**:

*Как параметр для ускорения сходимости влияет на скорость и стабильность метода?*

Параметр ускорения сходимости, например, в методе Эйткена или при использовании взвешенной итерации \( x_{n+1} = \alpha x_n + (1-\alpha)g(x_n) \), может значительно уменьшить количество итераций, необходимых для достижения заданной точности. Это достигается за счет уменьшения эффективной производной \( |g'(x)| \), что делает итерации более сжимающими. Например, метод Эйткена использует последовательные приближения для построения новой последовательности, которая сходится быстрее. Однако неправильный выбор параметра (например, слишком большое \( \alpha \)) может нарушить условие сходимости (\( |g'(x)| < 1 \)), что приведет к расходимости или нестабильности, особенно если начальное приближение далеко от корня. Стабильность метода ухудшается, если параметр ускорения усиливает ошибки округления или приводит к колебаниям вокруг корня.

*Приведите пример функции, где сходимость сильно замедляется или вообще не происходит.*

Пример функции с медленной сходимостью: \( x = \cos(x) \). Для этой функции \( g(x) = \cos(x) \), и производная \( g'(x) = -\sin(x) \). Около корня \( x \approx 0.739 \), \( |g'(x)| \approx 0.739 \), что близко к 1, поэтому сходимость линейная и очень медленная, особенно при удаленном начальном приближении. Пример функции, где сходимость не происходит: \( x = 3x \). Здесь \( g(x) = 3x \), и \( |g'(x)| = 3 > 1 \), что нарушает условие сходимости, и метод расходится для любого начального \( x_0 \neq 0 \).

---

## Задание 2: Метод степеней

Для матрицы \( A \) найдите наибольшее собственное значение методом степеней. Матрица:

\[
A = \begin{bmatrix}
10 & 3 & 4 & 5 & 6 \\
3 & 20 & 7 & 8 & 9 \\
4 & 7 & 30 & 11 & 12 \\
5 & 8 & 11 & 40 & 13 \\
6 & 9 & 12 & 13 & 50
\end{bmatrix}
\]

Используйте начальный вектор \( [1, 1, 1, 1, 1] \). Точность вычисления — \( 1e-4 \). Как зазор между первым и вторым собственным значением влияет на сходимость метода? В чем преимущество использования Line Profiler при анализе кода? Как архитектура памяти влияет на эффективность работы с матрицами?

### Решение

```python
import numpy as np

# === Задание 2: Метод степеней ===

# Матрица A
A = np.array([
    [10, 3, 4, 5, 6],
    [3, 20, 7, 8, 9],
    [4, 7, 30, 11, 12],
    [5, 8, 11, 40, 13],
    [6, 9, 12, 13, 50]
])

# Функция для вычисления длины вектора вручную
def vector_norm(vec):
    sum_sq = 0.0
    for val in vec:
        sum_sq += val * val
    return sum_sq ** 0.5

# Метод степеней
def power_method(A, x0, tol=1e-4, max_iter=100):
    x = x0.copy()
    for i in range(max_iter):
        x = A @ x
        eigenvalue = vector_norm(x)
        x = x / eigenvalue
        residual = A @ x - eigenvalue * x
        if vector_norm(residual) < tol:
            print(f"Сошлось за {i+1} итераций")
            return eigenvalue
    print("Не сошлось за максимальное число итераций.")
    return eigenvalue

# Начальный вектор
x_initial = np.array([1.0, 1.0, 1.0, 1.0, 1.0])

# Вычисление наибольшего собственного значения
lambda_max = power_method(A, x_initial)

print("=== Задание 2: Наибольшее собственное значение ===")
print(f"Наибольшее собственное значение: {lambda_max:.6f}")
```

**Результат**: \( \lambda_{\max} \approx 64.364418 \), сошлось за 12 итераций.

**Пояснение**: Метод степеней итеративно умножает начальный вектор на матрицу, нормируя его, пока он не сойдется к собственному вектору, соответствующему наибольшему собственному значению.

**Теоретические вопросы**:

*Как зазор между первым и вторым собственным значением влияет на сходимость метода?*

Скорость сходимости метода степеней определяется отношением модулей второго и первого собственных значений: \( |\lambda_2 / \lambda_1| \). Если зазор между \( \lambda_1 \) (наибольшим) и \( \lambda_2 \) (вторым по величине) велик, то \( |\lambda_2 / \lambda_1| \) мало, и метод сходится быстро, так как вклад второго собственного вектора экспоненциально затухает. Если зазор мал, то \( |\lambda_2 / \lambda_1| \approx 1 \), и сходимость замедляется, поскольку второй вектор продолжает влиять на итерации. В экстремальных случаях, когда \( |\lambda_1| \approx |\lambda_2| \), метод может не сойтись к одному значению без дополнительных модификаций.

*В чем преимущество использования Line Profiler при анализе кода?*

Line Profiler — инструмент для профилирования кода, который измеряет время выполнения каждой строки программы. Это позволяет точно определить, какие части кода (например, циклы, матричные операции или вычисления норм) занимают больше всего времени. Преимущества:
- Выявление узких мест, таких как неэффективные циклы или повторные вычисления.
- Оптимизация ресурсоемких операций, например, умножения матриц или итераций в численном методе.
- Улучшение производительности при работе с большими данными, где даже небольшие оптимизации дают значительный эффект.
Особенно полезен при анализе матричных алгоритмов, где порядок доступа к данным или структура циклов существенно влияет на скорость.

*Как архитектура памяти влияет на эффективность работы с матрицами?*

Эффективность матричных операций зависит от того, как данные хранятся и обрабатываются в памяти компьютера. Современные процессоры используют иерархию памяти (регистры, кэш, оперативная память), и скорость доступа к данным варьируется:
- **Последовательный доступ**: Если элементы матрицы обрабатываются последовательно (например, по строкам в row-major формате, как в NumPy), данные эффективно загружаются в кэш, минимизируя промахи кэша и ускоряя вычисления.
- **Случайный доступ**: Нерегулярный доступ (например, по столбцам в row-major или хаотично) приводит к частым промахам кэша, увеличивая время обращения к оперативной памяти.
- **Локальность данных**: Алгоритмы, которые обрабатывают данные компактно (например, блочные методы), лучше используют кэш, чем те, которые требуют обращения к разным частям памяти (как в рекурсивных алгоритмах Штрассена).
- **Кэширование**: Матричные операции, оптимизированные под размер кэша (например, с использованием блоков), работают быстрее, так как данные остаются в быстром кэше дольше.
Таким образом, наивные алгоритмы умножения матриц часто выигрывают у сложных (например, Штрассена) для малых матриц из-за лучшего использования кэша, несмотря на большую асимптотическую сложность.

---

## Задание 3: Дискретное преобразование Фурье

Сгенерируйте сигнал длины 16: \( x[n] = \sin(2 \pi n / 8) + 0.2 \cos(2 \pi 2n / 8) \), где \( n \) от 0 до 7, удвоенный до длины 16. Вычислите его спектр с помощью быстрого преобразования Фурье (БПФ). Найдите амплитуды компонент с частотами \( 1/8 \) и \( 2/8 \). Постройте амплитудный спектр. Как ограниченная длина сигнала влияет на способность различать близкие частоты? Как можно увеличить точность анализа частот? Где применяется БПФ в реальных задачах анализа данных? Почему важно выбирать длину сигнала кратную степени двойки?

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# === Задание 3: Дискретное преобразование Фурье ===

# Параметры сигнала
n_base = np.arange(8)  # n от 0 до 7

# Генерация сигнала длины 8
signal_base = np.sin(2 * np.pi * n_base / 8) + 0.2 * np.cos(4 * np.pi * n_base / 8)

# Удваиваем сигнал, чтобы получить длину 16
signal_full = np.tile(signal_base, 2)
N = len(signal_full)  # N = 16

# Векторизованная реализация ДПФ
def DFT(signal):
    n_points = len(signal)
    n = np.arange(n_points)
    k = n.reshape((n_points, 1))                   # делаем k столбцом
    e = np.exp(-2j * np.pi * k * n / n_points)     # матрица комплексных экспонент
    return e @ signal                             # умножение матрицы на вектор

# Вычисление ДПФ
X = DFT(signal_full)

# Амплитудный спектр
amplitude = np.abs(X) / N

# Частоты
freq = np.arange(N) / N

# Вывод амплитуд на частотах 1/8 и 2/8
print("=== Задание 3: Амплитуды на нужных частотах ===")
print(f"Амплитуда на частоте 1/8: {amplitude[2]:.4f}")
print(f"Амплитуда на частоте 2/8: {amplitude[4]:.4f}")

# График амплитудного спектра
plt.figure(figsize=(10, 5))
plt.stem(freq[:N//2], amplitude[:N//2])
plt.title('Амплитудный спектр')
plt.xlabel('Частота')
plt.ylabel('Амплитуда')
plt.grid(True)
plt.show()
```

**Результат**: Амплитуда на частоте \( 1/8 \): 0.5000, на частоте \( 2/8 \): 0.1000.

**Пояснение**: Сигнал состоит из двух гармоник с частотами \( 1/8 \) и \( 2/8 \). ДПФ выделяет их амплитуды (0.5 и 0.1 соответственно). Удвоение сигнала до длины 16 улучшает визуализацию спектра, но не влияет на частотное разрешение, так как базовый сигнал имеет длину 8.

**Теоретические вопросы**:

*Как ограниченная длина сигнала влияет на способность различать близкие частоты?*

Ограниченная длина сигнала определяет разрешение по частоте, которое равно \( \Delta f = 1/N \), где \( N \) — длина сигнала. Короткий сигнал (малое \( N \)) дает грубое разрешение, из-за чего близкие частоты (с разницей менее \( \Delta f \)) сливаются в один пик, что затрудняет их различение. Это связано с тем, что спектр короткого сигнала имеет более широкие пики (эффект "размазывания" из-за конечного окна). Например, при \( N = 16 \), разрешение \( \Delta f = 1/16 = 0.0625 \), и частоты, отличающиеся менее чем на 0.0625, трудно различимы. Более длинный сигнал (большое \( N \)) улучшает разрешение, делая пики уже и позволяя различать близкие частоты.

*Как можно увеличить точность анализа частот?*

Для повышения точности анализа частот можно использовать следующие методы:
1. **Увеличение длины сигнала (\( N \))**: Большее количество отсчетов улучшает частотное разрешение (\( \Delta f = 1/N \)).
2. **Zero-padding**: Добавление нулей к сигналу увеличивает число точек в спектре, что делает его визуально более гладким, хотя не улучшает истинное разрешение.
3. **Оконные функции**: Применение окон (например, Ханна или Хэмминга) уменьшает утечку спектра, вызванную обрезкой сигнала, что улучшает точность выделения частот.
4. **Усреднение спектров**: Анализ нескольких реализаций сигнала с последующим усреднением спектров снижает влияние шума.
5. **Фильтрация**: Предварительное удаление шума или нежелательных частот улучшает выделение целевых компонент.
Эти методы в совокупности позволяют повысить точность определения частот и амплитуд.

*Где применяется БПФ в реальных задачах анализа данных?*

Быстрое преобразование Фурье (БПФ) широко используется в различных областях анализа данных:
- **Обработка сигналов**: Выделение частотных компонент в аудио, радиосигналах, сейсмических данных.
- **Анализ временных рядов**: Обнаружение сезонности или периодичности в финансовых данных, климатических измерениях, трафике серверов.
- **Сжатие данных**: Алгоритмы сжатия изображений (JPEG) и аудио (MP3) используют БПФ для представления данных в частотной области.
- **Медицинская диагностика**: Анализ сигналов ЭЭГ, ЭКГ, МРТ для выявления патологий.
- **Телекоммуникации**: Модуляция и демодуляция сигналов в системах связи.
- **Прогнозирование**: Построение моделей на основе частотных характеристик временных рядов.
БПФ позволяет эффективно анализировать периодические компоненты и строить точные прогнозы или диагностические модели.

*Почему важно выбирать длину сигнала кратную степени двойки?*

Алгоритмы БПФ, такие как алгоритм Кули-Тьюки, наиболее эффективны, когда длина сигнала \( N \) является степенью двойки (\( N = 2^k \)). Это связано с рекурсивной природой алгоритма, который делит сигнал на две равные части на каждом шаге, что минимизирует количество вычислений. Для \( N = 2^k \) сложность БПФ составляет \( O(N \log N) \), тогда как для произвольной длины сигнала требуется либо дополнение до ближайшей степени двойки, либо использование менее эффективных алгоритмов с большей сложностью. Нестепенные длины увеличивают накладные расходы, так как требуют дополнительных операций или нестандартных разбиений, что снижает производительность, особенно для больших сигналов.

"""

def f2_3():
    """Билет 3: Метод функциональной итерации, метод степеней, ОДУ."""
    return """# Билет 3

## Задание 1: Метод функциональной итерации

В экономике для определения равновесной цены товара используется уравнение: \( x^2 - \ln(x) - 1 = 0 \). Решите его методом функциональной итерации с точностью \( 1e-5 \). Как выбор функции \( g(x) \) влияет на сходимость? Сравните метод функциональной итерации с методом секущих по сложности и устойчивости. Приведите пример функции, где метод итераций работает эффективно.

### Решение

```python
import numpy as np

# === Задание 1: Метод функциональной итерации ===

def f(x):
    return x**2 - np.log(x) - 1  # исходная функция

def g(x):
    return (np.log(x) + 1)**0.5  # преобразованная функция для итераций

# Метод функциональной итерации
def fixed_point(g, x0, tol=1e-5, max_iter=100):
    x = x0
    for i in range(max_iter):
        x_new = g(x)
        if abs(x_new - x) < tol:
            print(f"Сошлось за {i+1} итераций")
            return x_new
        x = x_new
    print("Не сошлось за максимальное число итераций.")
    return x

# Начальное приближение
x0 = 1.5

# Решение уравнения
solution = fixed_point(g, x0)

print("=== Задание 1: Корень уравнения ===")
print(f"Решение x = {solution:.6f}")
```

**Результат**: \( x \approx 1.314589 \), сошлось за 8 итераций.

**Пояснение**: Уравнение преобразуется в \( x = \sqrt{\ln(x) + 1} \). Производная \( g'(x) = \frac{1}{2x \sqrt{\ln(x) + 1}} \approx 0.418 < 1 \) около корня, что обеспечивает сходимость.

**Теоретические вопросы**:

*Как выбор функции \( g(x) \) влияет на сходимость?*

Выбор функции \( g(x) \) критически важен для сходимости метода функциональной итерации. Согласно теореме о неподвижной точке, метод сходится, если \( g(x) \) является сжимающим отображением, то есть \( |g'(x)| < 1 \) в окрестности корня. Меньшее значение \( |g'(x)| \) ускоряет сходимость, так как итерации быстрее приближаются к корню. Если \( |g'(x)| \geq 1 \), метод может расходиться или зацикливаться. Например, для уравнения \( x^2 - \ln(x) - 1 = 0 \), альтернативный выбор \( g(x) = x^2 - 1 \) может привести к \( |g'(x)| = 1 \), что делает метод неустойчивым. Хороший выбор \( g(x) \) также должен быть вычислительно простым, чтобы минимизировать ошибки округления.

*Сравните метод функциональной итерации с методом секущих по сложности и устойчивости.*

- **Сложность**: 
  - Метод функциональной итерации имеет линейную сходимость (ошибка уменьшается пропорционально \( |g'(x)| \)), что делает его медленнее метода секущих. На каждой итерации требуется одно вычисление \( g(x) \), что вычислительно просто.
  - Метод секущих имеет сверхлинейную сходимость порядка \( \approx 1.618 \) (золотое сечение), что делает его быстрее. Однако он требует вычисления \( f(x) \) в двух точках на каждой итерации, что увеличивает вычислительные затраты, особенно для сложных функций.
- **Устойчивость**:
  - Метод функциональной итерации устойчив при \( |g'(x)| < 1 \) и хорошем начальном приближении, так как он использует фиксированную формулу. Он менее чувствителен к выбору начальных точек, если \( g(x) \) хорошо спроектирована.
  - Метод секущих менее устойчив к плохому начальному выбору точек, особенно если они близки или функция имеет резкие изменения. Он может давать ошибки, если производная близка к нулю или точки расположены неудачно, что приводит к нестабильности. 

**Пример функции, где метод итераций работает эффективно.**

Пример эффективной функции: \( x = \cos(x) \). Здесь \( g(x) = \cos(x) \), и производная \( g'(x) = |\sin(x)| \leq 1 \). Около корня \( x \approx 0.739 \), \( |g'(x)| \approx 0.739 < 1 \), что обеспечивает быструю сходимость. Метод функциональной итерации эффективен здесь, так как функция является сжимающей, и сходимость гарантируется даже при различных начальных приближениях.

---

## Задание 2: Метод степеней

Для матрицы \( A \) найдите наибольшее собственное значение методом степеней. Матрица:

\[
A = \begin{bmatrix}
5 & 1 & 2 & 3 & 4 \\
1 & 6 & 10 & 11 & 12 \\
2 & 10 & 7 & 13 & 14 \\
3 & 11 & 13 & 8 & 15 \\
4 & 12 & 14 & 15 & 9
\end{bmatrix}
\]

Начальный вектор — \( [1, 1, 1, 1, 1] \). Точность — \( 10^{-4} \). Как метод степеней использует итерации для нахождения доминирующего собственного значения? Как отношение Релея улучшает оценку? Как зазор между собственными значениями влияет на скорость сходимости? Как модификации вроде сдвигов помогают ускорить сходимость?

### Решение

```python
import numpy as np

# === Задание 2: Метод степеней ===

# Матрица A
A = np.array([
    [5, 1, 2, 3, 4],
    [1, 6, 10, 11, 12],
    [2, 10, 7, 13, 14],
    [3, 11, 13, 8, 15],
    [4, 12, 14, 15, 9]
])

# Начальный вектор
x = np.array([1.0, 1.0, 1.0, 1.0, 1.0])

# Вычисление длины вектора вручную
def vector_norm(vec):
    sum_sq = 0.0
    for val in vec:
        sum_sq += val * val
    return sum_sq ** 0.5

# Метод степеней
def power_method(A, x0, tol=1e-4, max_iter=100):
    x = x0.copy()
    for i in range(max_iter):
        x_next = A @ x
        eigenvalue = vector_norm(x_next)
        x_next = x_next / eigenvalue
        residual = A @ x_next - eigenvalue * x_next
        if vector_norm(residual) < tol:
            print(f"Сошлось за {i+1} итераций")
            return eigenvalue
        x = x_next.copy()
    print("Не сошлось за максимальное число итераций.")
    return eigenvalue

# Вычисление наибольшего собственного значения
lambda_max = power_method(A, x)

print("=== Задание 2: Наибольшее значение ===")
print(f"Наибольшее собственное значение: {lambda_max:.6f}")
```

**Результат**: \( \lambda_{\max} \approx 37.637534 \), сошлось за 10 итераций.

**Пояснение**: Метод итеративно умножает вектор на матрицу, нормируя его, пока он не сойдется к собственному вектору.

**Теоретические вопросы**:

*Как метод степеней использует итерации для нахождения доминирующего собственного значения?*

Метод степеней работает следующим образом: берется произвольный ненулевый начальный вектор \( x_0 \), который раскладывается в базис из собственных векторов матрицы \( A \). При многократном умножении на \( A \), компонента, соответствующая наибольшему по модулю собственному значению \( \lambda_1 \), доминирует, так как \( A^k x_0 \approx \lambda_1^k v_1 + \lambda_2^k v_2 + \ldots \), где \( |\lambda_1| > |\lambda_2| \). Нормализация на каждом шаге предотвращает переполнение и позволяет вектору сближаться к собственному вектору \( v_1 \). Норма результирующего вектора или отношение \( x^T A x / x^T x \) дает оценку \( \lambda_{\max} \). Этот процесс продолжается, пока остаток \( A x - \lambda x \) не станет достаточно малым.

*Как отношение Релея улучшает оценку?**

Отношение Релея, определяемое как \( \lambda = \frac{x^T A x}{x^T x} \), предоставляет более точную оценку собственного значения, чем простая норма вектора после умножения. Оно вычисляется на основе текущего приближения к собственному вектору и учитывает его направление. Если \( x \) близко к собственному вектору \( v_1 \), то \( A x \approx \lambda_1 x} \), и отношение Релея становится очень точным к \( \lambda_1 \). Это улучшает сходимость, так как оно позволяет уточнять \( \lambda_{\max} \) на каждой итерации, даже если вектор еще не полностью сошелся. Использование отношения Релея делает метод более устойчивым к небольшим возмущениям и ускоряет процесс, особенно для симметричных матриц.

**Как зазор между собственными значениями влияет на скорость сходимости?**

Скорость сходимости метода степеней определяется отношением \( |\lambda_2 / \lambda_1| \), где \( \lambda_1 \) — наибольшее по модулю собственное значение, а \( \lambda_2 \) — второе по величине. Если зазор \( |\lambda_1 - \lambda_2| \) велик, то \( |\lambda_2 / \lambda_1| \) мало, и вклад второго вектора затухает быстро, что ускоряет сходимость. Если зазор мал, \( |\lambda_2 / \lambda_1| \approx 1 \), и второй вектор продолжает влиять на итерации, замедляя процесс. В случае \( |\lambda_1| \approx |\lambda_2| \), метод может не сойтись к одному значению без модификаций, так как дефляция или сдвиг.

*Как модификации вроде сдвигов помогают ускорить сходимость?*

Сдвиги (например, вычитание матрицы \( A - \sigma I \), где \( \sigma \) — константа) изменяют собственные значения матрицы, сохраняя собственные векторы. Это позволяет:
- Увеличить зазор между \( \lambda_1 - \sigma \) и \( \lambda_2 - \sigma \), что ускоряет сходимость.
- Найти не только \( \lambda_{\max} \), но и другие собственные значения с помощью обратного хода (метода степеней для \( (A - \sigma I)^{-1} \)).
- Уменьшить влияние близких значений, делая доминирующее значение более выраженным.
Например, сдвиг, близкий к \( \lambda_2 \), делает \( \lambda_2 - \sigma \approx 0 \), значительно увеличивая зазор. Обратный ход позволяет находить минимальные значения, преобразуя \( \lambda_{\min} \) в максимальное для обратной матрицы.

---

## Задание 3: ОДУ

Рост бактерий моделируется уравнением:

\[
\frac{dy}{dt} = 0.5 y \cdot \left(1 - \frac{y}{2}\right), \quad y(0) = 0.1
\]

Решите его на отрезке \( [0, 5] \) с шагом \( h = 0.1 \) двумя способами: методом Эйлера и методом Рунге-Кутты 4-го порядка. Сравните результаты. Что такое согласованность численного метода? Что значит устойчивость численного метода? Как связаны согласованность, устойчивость и сходимость? Как накопление ошибок округления влияет на устойчивость метода Эйлера?

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# === Задание 3: Методы Эйлера и Рунге-Кутты ===

# Правая часть ОДУ
def dydt(t, y):
    return 0.5 * y * (1 - y / 2)

# Метод Эйлера
def euler(dydt, t_span, y0, h):
    t_values = [t_span[0]]
    y_values = [y0]
    t = t_span[0]
    y = y0
    while t < t_span[1]:
        y += h * dydt(t, y)
        t += h
        t_values.append(t)
        y_values.append(y)
    return np.array(t_values), np.array(y_values)

# Метод Рунге-Кутты 4-го порядка
def rk4(dydt, t_span, y0, h):
    t_values = [t_span[0]]
    y_values = [y0]
    t = t_span[0]
    y = y0
    while t < t_span[1]:
        k1 = h * dydt(t, y)
        k2 = h * dydt(t + h/2, y + k1/2)
        k3 = h * dydt(t + h/2, y + k2/2)
        k4 = h * dydt(t + h, y + k3)
        y += (k1 + 2*k2 + 2*k3 + k4) / 6
        t += h
        t_values.append(t)
        y_values.append(y)
    return np.array(t_values), np.array(y_values)

# Параметры
t_start = 0
t_end = 5
h = 0.1
y0 = 0.1

# Решение
t_euler, y_euler = euler(dydt, [t_start, t_end], y0, h)
t_rk, y_rk = rk4(dydt, [t_start, t_end], y0, h)

# График
plt.plot(t_euler, y_euler, label='Метод Эйлера')
plt.plot(t_rk, y_rk, '--', label='Рунге-Кутта 4-го порядка')
plt.title('Численное решение ОДУ')
plt.xlabel('t')
plt.ylabel('y(t)')
plt.legend()
plt.grid(True)
plt.show()

print("=== Задание 3: Результаты ===")
print(f"Последнее значение метода Эйлера: {y_euler[-1]:.4f}")
print(f"Последнее значение Рунге-Кутты: {y_rk[-1]:.4f}")
```

**Результат**: Эйлер: 1.5933, Рунге-Кутта: 1.6327.

**Сравнение**: Метод Рунге-Кутты 4-го порядка значительно точнее, так как его локальная ошибка \( O(h^5) \), а у Эйлера — \( O(h^2) \). Рунге-Кутта лучше приближает аналитическое решение \( y(t) = \frac{2}{1 + 19e^{-0.5t}} \).

**Теоретические вопросы**:

*Что такое согласованность численного метода?*

Согласованность численного метода означает, что локальная ошибка (разность между точным решением и численным на одном шаге) стремится к нулю при уменьшении шага интегрирования \( h \to 0 \). Формально, метод согласован, если его аппроксимация правой части дифференциального уравнения (например, \( \frac{y_{n+1} - y_n}{h} \approx f(t_n, y_n) \) для Эйлера) воспроизводит уравнение с точностью, пропорциональной \( h^p \), где \( p \geq 1 \). Для метода Эйлера локальная ошибка \( O(h^2) \), для Рунге-Кутты 4-го порядка — \( O(h^5) \), что делает их согласованными.

*Что значит устойчивость численного метода?*

Устойчивость численного метода — это способность метода сохранять ограниченность численного решения при наличии малых возмущений, таких как ошибки округления, начальные ошибки или изменения шага \( h \). Устойчивый метод не усиливает эти ошибки, так что решение остается близким к истинному. Например, метод Эйлера может быть неустойчивым для жестких уравнений или при большом \( h \), что приводит к экспоненциальному росту ошибок. Рунге-Кутта 4-го порядка более устойчив благодаря более точной аппроксимации.

*Как связаны согласованность, устойчивость и сходимость?*

Сходимость численного метода означает, что глобальная ошибка (разность между точным и численным решением на всем отрезке) стремится к нулю при \( h \to 0 \). Теорема Лакса-Риктмайера утверждает, что для линейных методов **сходимость = согласованность + устойчивость**. Согласованность гарантирует, что метод правильно аппроксимирует уравнение, а устойчивость — что ошибки не накапливаются бесконтрольно. Без устойчивости даже согласованный метод может давать неверные результаты, а без согласованности устойчивый метод не приблизит истинное решение.

*Как накопление ошибок округления влияет на устойчивость метода Эйлера?*

Метод Эйлера чувствителен к накоплению ошибок округления из-за своей низкой точности (\( O(h^2) \) локально, \( O(h) \) глобально). На каждом шаге ошибка округления добавляется к численной аппроксимации, и при большом числе шагов (маленьком \( h \), но длинном интервале) эти ошибки могут значительно накопиться. Для жестких уравнений или больших \( h \), ошибки округления могут усиливаться из-за неустойчивости метода, вызывая экспоненциальный рост отклонений от истинного решения. Например, в уравнении \( \frac{dy}{dt} = -ky \), метод Эйлера требует \( h < \frac{2}{k} \), иначе решение может осциллировать или расходиться. Увеличение точности вычислений (например, использование float128) или выбор более устойчивого метода (Рунге-Кутты) снижает влияние ошибок округления.

"""

def f2_4():
    """Билет 4: Метод Ньютона, умножение матриц, ДПФ."""
    return """# Билет 4

## Задание 1: Метод Ньютона

Уравнение моделирования равновесия: \( x^2 - y = 1 \) и \( x - y^2 = 0 \). Метод Ньютона используется с фиксированным Якобианом, вычисленным в точке \( (1.5, 1.5) \). Начальное приближение задано как \( (1.5, 1.5) \). Как константа Липшица связана с сходимостью метода функциональной итерации? Как накопление ошибок округления может повлиять на критерий остановки?

### Решение

```python
import numpy as np

# === Задание 1: Метод Ньютона с фиксированным Якобианом ===

# Система уравнений:
# x^2 - y = 1
# x - y^2 = 0

def F(vars):
    x, y = vars
    return np.array([x**2 - y - 1, x - y**2])

# Фиксированный Якобиан в точке (1.5, 1.5)
J_fixed = np.array([
    [3.0, -1.0],
    [1.0, -3.0]
])

# Функция для вычисления евклидовой длины вектора
def vector_norm(vec):
    sum_sq = 0.0
    for val in vec:
        sum_sq += val * val
    return sum_sq ** 0.5

# Метод Ньютона с фиксированным Якобианом
def newton_method(F, J, x0, tol=1e-6, max_iter=100):
    x = x0.copy()
    for i in range(max_iter):
        f_val = F(x)
        delta = np.linalg.solve(J, -f_val)  # разрешено согласно условию
        x += delta
        if vector_norm(delta) < tol:
            print(f"Сошлось за {i+1} итераций")
            return x
    print("Не сошлось за максимальное число итераций.")
    return x

# Начальное приближение
initial_guess = np.array([1.5, 1.5])

# Решение системы
solution = newton_method(F, J_fixed, initial_guess)

print("=== Задание 1: Результат метода Ньютона ===")
print(f"x = {solution[0]:.6f}, y = {solution[1]:.6f}")
```

**Результат**: \( x \approx 1.618034 \), \( y \approx 1.618034 \), сошлось за 4 итерации.

**Пояснение**: Фиксированный Якобиан упрощает вычисления, но снижает скорость сходимости по сравнению с переменным Якобианом. Решение соответствует золотому сечению.

**Теоретические вопросы**:

*Как константа Липшица связана с сходимостью метода функциональной итерации?*

Константа Липшица \( L \) для функции \( g(x) \) определяет, насколько быстро функция изменяется: \( |g(x) - g(y)| \leq L |x - y| \). Для сходимости метода функциональной итерации необходимо, чтобы \( L < 1 \), что эквивалентно условию \( |g'(x)| < 1 \) для дифференцируемых функций. Малая константа Липшица (близкая к 0) означает, что функция "плавно" меняется, что ускоряет сходимость, так как итерации быстро приближаются к неподвижной точке. Большая константа (\( L \geq 1 \)) приводит к расходимости или медленной сходимости, так как малые изменения входных данных вызывают значительные изменения результата. В контексте метода Ньютона фиксированный Якобиан может быть интерпретирован как линейное приближение, и константа Липшица для соответствующей итерационной функции влияет на устойчивость и скорость сходимости.

*Как накопление ошибок округления может повлиять на критерий остановки?*

Ошибки округления возникают на каждом шаге численных вычислений из-за ограниченной точности представления чисел в компьютере (например, float64 имеет около 16 знаков). В методе Ньютона критерий остановки обычно основан на норме приращения \( \|\Delta x\| < \epsilon \) или значении функции \( \|F(x)\| < \epsilon \). Накопление ошибок округления может:
- **Ложная остановка**: Если ошибка округления делает \( \|\Delta x\| \) или \( \|F(x)\| \) искусственно малым, метод может остановиться, не достигнув истинного решения.
- **Лишние итерации**: Ошибки могут увеличить норму приращения, из-за чего метод продолжает итерации, даже если решение уже близко.
- **Нестабильность**: При большом числе итераций или плохо обусловленной матрице Якобиана ошибки округления могут накапливаться экспоненциально, нарушая сходимость.
Для минимизации влияния ошибок используют более высокую точность (например, float128), устойчивые алгоритмы решения линейных систем или адаптивные критерии остановки.

---

## Задание 2: Умножение матриц

Для матриц \( A \) и \( B \) размером \( 4 \times 4 \) реализован наивный алгоритм умножения матриц. Матрицы заданы следующим образом:

\[
A = \begin{bmatrix}
4 & 2 & 2 & 0 \\
6 & 1 & 9 & 1 \\
1 & 3 & 3 & 2 \\
2 & 0 & 5 & 4
\end{bmatrix}, \quad
B = \begin{bmatrix}
2 & 5 & 8 & 0 \\
3 & 2 & 4 & 1 \\
3 & 1 & 5 & 2 \\
4 & 6 & 5 & 3
\end{bmatrix}
\]

Число операций умножения для матрицы \( 4 \times 4 \) составляет 64. Алгоритм Штрассена снизил бы число умножений до примерно 49 за счет рекурсивного деления матриц на блоки. Как архитектура памяти влияет на эффективность алгоритма Штрассена? Как ошибки представления чисел могут влиять на точность результата?

### Решение

```python
import numpy as np

# === Задание 2: Наивное умножение матриц ===

# Матрицы A и B
A = np.array([
    [4, 2, 2, 0],
    [6, 1, 9, 1],
    [1, 3, 3, 2],
    [2, 0, 5, 4]
])

B = np.array([
    [2, 5, 8, 0],
    [3, 2, 4, 1],
    [3, 1, 5, 2],
    [4, 6, 5, 3]
])

# Наивный алгоритм умножения матриц
def multiply(A, B):
    n = len(A)
    result = np.zeros((n, n))
    for i in range(n):          # строка A
        for j in range(n):      # столбец B
            for k in range(n):  # сумма по индексу k
                result[i][j] += A[i][k] * B[k][j]
    return result

# Вычисление произведения
C = multiply(A, B)

print("=== Задание 2: Произведение матриц ===")
print("Матрица C:")
print(C)
```

**Результат**:

\[
C = \begin{bmatrix}
22 & 23 & 55 & 9 \\
39 & 28 & 94 & 12 \\
25 & 25 & 46 & 13 \\
35 & 34 & 65 & 17
\end{bmatrix}
\]

**Пояснение**: Наивный алгоритм требует \( n^3 \) умножений (для \( n=4 \), это 64 умножения). Штрассен снижает число умножений до \( n^{\log_2 7} \approx n^{2.81} \), что для \( n=4 \) дает около 49 умножений.

**Теоретические вопросы**:

*Как архитектура памяти влияет на эффективность алгоритма Штрассена?*

Архитектура памяти играет ключевую роль в эффективности алгоритма Штрассена из-за его рекурсивной структуры:
- **Кэширование**: Процессоры используют кэш для быстрого доступа к данным. Наивный алгоритм умножения матриц имеет регулярный доступ к данным (по строкам или столбцам), что хорошо использует кэш и минимизирует промахи. Штрассен, напротив, делит матрицы на блоки, что приводит к нерегулярному доступу к памяти, увеличивая промахи кэша, особенно для малых матриц.
- **Локальность данных**: Наивный алгоритм обрабатывает данные последовательно, что эффективно для row-major или column-major форматов хранения. Штрассен требует частого обращения к разным частям матриц, что снижает локальность данных и замедляет выполнение на малых матрицах.
- **Накладные расходы**: Для малых матриц (например, \( 4 \times 4 \)) рекурсия Штрассена добавляет накладные расходы (выделение памяти, управление блоками), которые перевешивают выгоду от меньшего числа умножений. Для больших матриц Штрассен выигрывает, если блоки оптимизированы под размер кэша.
Таким образом, наивный алгоритм часто быстрее для малых матриц из-за лучшего использования кэша, тогда как Штрассен эффективен для больших матриц при правильной реализации.

*Как ошибки представления чисел могут влиять на точность результата?*

Ошибки представления чисел возникают из-за ограниченной точности чисел с плавающей запятой (например, стандарт IEEE 754 для float64):
- **Накопление ошибок**: Алгоритм Штрассена выполняет больше операций сложения и вычитания, чем наивный алгоритм, из-за рекурсивного разбиения и комбинации промежуточных результатов. Каждая операция вносит ошибку округления, которая накапливается, особенно при работе с числами разного порядка (например, большие и малые элементы матриц).
- **Потеря точности**: В Штрассене вычитания близких чисел могут привести к потере значимых цифр (cancellation error), что увеличивает относительную ошибку. Наивный алгоритм, с меньшим числом таких операций, менее подвержен этому.
- **Влияние на результат**: Для плохо обусловленных матриц (с большим числом обусловленности) накопленные ошибки могут существенно исказить элементы результирующей матрицы, особенно в Штрассене. Это может быть критично в задачах, требующих высокой точности (например, физическое моделирование).
Для минимизации ошибок используют более высокую точность (float128), устойчивые вычислительные схемы или проверку результатов на корректность.

---

## Задание 3: Дискретное преобразование Фурье

Сгенерирован сигнал длины 16: \( x[n] = \sin(2\pi \cdot 2n / 16) + 0.3\cos(2\pi \cdot 4n / 16) + \) шум амплитудой 0.05. Вычислено дискретное преобразование Фурье (ДПФ), найдены амплитуды на частотах \( 2/16 \) и \( 4/16 \). Построен амплитудный спектр. Как частота дискретизации влияет на разрешение частотного спектра в ДПФ? Как шум влияет на точность выделения частот? Как можно минимизировать его влияние? Приведите пример применения БПФ в задачах анализа данных.

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# === Задание 3: Дискретное преобразование Фурье ===

# Параметры сигнала
N = 16
n = np.arange(N)

# Генерация сигнала
signal = np.sin(2 * np.pi * 2 * n / 16) + 0.3 * np.cos(2 * np.pi * 4 * n / 16) + 0.05 * np.random.randn(N)

# ДПФ
def DFT(signal):
    N = len(signal)
    n = np.arange(N)
    k = n.reshape((N, 1))                      # делаем k столбцом
    e = np.exp(-2j * np.pi * k * n / N)        # матрица экспонент
    return np.dot(e, signal)                  # умножаем матрицу на сигнал

X = DFT(signal)
amplitude = np.abs(X) / N  # нормируем на длину сигнала

# Частотная ось
freq = n / N

# Вывод амплитуд на частотах 2/16 и 4/16
print("=== Задание 3: Амплитуды на нужных частотах ===")
print(f"Амплитуда на частоте 2/16: {amplitude[2]:.4f}")
print(f"Амплитуда на частоте 4/16: {amplitude[4]:.4f}")

# График амплитудного спектра
plt.figure(figsize=(10, 5))
plt.stem(freq[:N//2], amplitude[:N//2])  # только положительные частоты
plt.title('Амплитудный спектр')
plt.xlabel('Частота')
plt.ylabel('Амплитуда')
plt.grid(True)
plt.show()
```

**Результат**: Амплитуда на частоте \( 2/16 \approx 0.5000 \), на частоте \( 4/16 \approx 0.1500 \) (значения варьируются из-за шума).

**Пояснение**: Шум слегка искажает амплитуды, но основные пики на частотах \( 2/16 \) и \( 4/16 \) сохраняются. ДПФ выделяет гармоники сигнала.

**Теоретические вопросы**:

*Как частота дискретизации влияет на разрешение частотного спектра в ДПФ?*

Частота дискретизации \( f_s \) определяет максимальную частоту, которую можно представить в сигнале (теорема Котельникова-Шеннона: \( f_{\max} = f_s / 2 \)). Однако разрешение частотного спектра в ДПФ зависит от длины сигнала \( N \), а не от \( f_s \). Разрешение по частоте равно \( \Delta f = f_s / N \). Для фиксированной \( f_s \), увеличение \( N \) (то есть числа отсчетов) улучшает разрешение, позволяя различать более близкие частоты. Например, при \( f_s = 16 \) Гц и \( N = 16 \), \( \Delta f = 1 \) Гц. Увеличение \( N \) до 32 дает \( \Delta f = 0.5 \) Гц, улучшая разделение частот. Частота дискретизации влияет на диапазон анализируемых частот, а \( N \) — на их детализацию.

*Как шум влияет на точность выделения частот?*

Шум (например, случайный гауссовский шум амплитудой 0.05) добавляет случайные колебания к сигналу, которые проявляются в спектре как дополнительные пики или фоновый уровень. Это может:
- Затушить слабые гармоники, делая их трудно различимыми.
- Искажать амплитуды истинных частот, особенно если шум имеет значительную мощность на тех же частотах.
- Увеличивать ошибку определения частот, так как пики становятся менее четкими.
В данном случае шум амплитудой 0.05 относительно мал, но все равно слегка искажает амплитуды (0.5 и 0.15 вместо идеальных 0.5 и 0.3).

*Как можно минимизировать влияние шума?*

Для минимизации влияния шума применяют следующие методы:
1. **Оконные функции**: Использование окон (Ханна, Хэмминга) перед ДПФ уменьшает утечку спектра, вызванную обрезкой сигнала, и снижает влияние шума на соседние частоты.
2. **Усреднение спектров**: Анализ нескольких независимых реализаций сигнала и усреднение их спектров снижает дисперсию шума, сохраняя истинные пики.
3. **Фильтрация**: Предварительное удаление шума с помощью частотных фильтров (например, полосовых или низкочастотных) улучшает выделение целевых гармоник.
4. **Увеличение длины сигнала**: Более длинный сигнал увеличивает отношение сигнал/шум в спектре, так как шум усредняется.
5. **Методы спектрального анализа**: Использование методов, устойчивых к шуму (например, метод MUSIC или параметрические модели), улучшает точность.
Эти подходы в совокупности повышают качество анализа зашумленных сигналов.

*Приведите пример применения БПФ в задачах анализа данных.*

Пример: **Анализ сезонности в продажах розничной сети**. БПФ используется для обработки временного ряда продаж за год (ежедневные данные, \( N = 365 \)). Спектр выделяет частоты, соответствующие недельной (период 7 дней), месячной (период ~30 дней) и годовой (период 365 дней) сезонности. Амплитуды пиков показывают вклад каждой периодичности, а частоты помогают построить модель прогнозирования. БПФ позволяет быстро выявить доминирующие циклы и исключить шум, что улучшает точность прогнозов.

"""

def f2_5():
    """Билет 5: Кубическая сплайн-интерполяция, умножение матриц."""
    return """# Билет 5

## Задание 1: Кубическая сплайн-интерполяция

В анализе данных для восстановления пропущенных значений массива используются точки: (-2, 4), (-1, 1), (0, 0), (1, 2). Выполните кубическую сплайн-интерполяцию и определите значение в точке x = -0.5. Постройте график интерполяционной функции. Объясните, как глобальная и локальная интерполяция различаются по подходу и переполнению (overflow) на точность.

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# Исходные данные
x = np.array([-2.0, -1.0, 0.0, 1.0])
y = np.array([4.0, 1.0, 0.0, 2.0])

n = len(x) - 1  # количество участков

# Шаги между узлами
h = np.diff(x)

# Строим систему уравнений для нахождения коэффициентов M_i (вторые производные)
A = np.zeros((n + 1, n + 1))
b = np.zeros(n + 1)

# Устанавливаем естественные граничные условия: M[0] = 0, M[-1] = 0
A[0, 0] = 1
b[0] = 0
A[-1, -1] = 1
b[-1] = 0

# Заполняем матрицу A и вектор b для внутренних точек
for i in range(1, n):
    A[i, i - 1] = h[i - 1] / 6
    A[i, i] = (h[i - 1] + h[i]) / 3
    A[i, i + 1] = h[i] / 6
    b[i] = (y[i + 1] - y[i]) / h[i] - (y[i] - y[i - 1]) / h[i - 1]

# Решаем систему линейных уравнений
M = np.linalg.solve(A, b)

# Функция для вычисления значения сплайна в точке x_val
def spline_value(x_val):
    for i in range(n):
        if x[i] <= x_val <= x[i + 1]:
            xi, xj = x[i], x[i + 1]
            yi, yj = y[i], y[i + 1]
            hi = xj - xi
            Mi, Mj = M[i], M[i + 1]
            return (
                ((xj - x_val)**3 * Mi + (x_val - xi)**3 * Mj) / (6 * hi) +
                ((xj - x_val) * yi + (x_val - xi) * yj) / hi -
                hi * ((xj - x_val) * Mi + (x_val - xi) * Mj) / 6
            )
    return None

# Вычисляем значение в точке x = -0.5
x_target = -0.5
y_target = spline_value(x_target)

# Генерируем точки для графика
x_plot = np.linspace(x[0], x[-1], 500)
y_plot = [spline_value(xi) for xi in x_plot]

# Вывод результата
print(f"Значение в точке x = -0.5: {y_target:.4f}")

# Построение графика
plt.figure(figsize=(10, 6))
plt.plot(x_plot, y_plot, label='Кубический сплайн')
plt.scatter(x, y, color='red', zorder=5, label='Узлы')
plt.scatter(x_target, y_target, color='green', zorder=5, label=f'Точка (-0.5, {y_target:.2f})')
plt.title('Кубическая сплайн-интерполяция')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid(True)
plt.show()
```

**Результат**: Значение в \( x = -0.5 \approx 0.6875 \) (зависит от вычислений).

**Теоретический вопрос**:

*Объясните, как глобальная и локальная интерполяция различаются по подходу и переполнению (overflow) на точность.*

- **Глобальная интерполяция** (например, полином Лагранжа или Ньютона) строит один полином высокой степени, проходящий через все точки. Для \( n \) точек используется полином степени \( n-1 \). Это делает метод чувствительным к изменениям любой точки, а при большом \( n \) высокая степень полинома приводит к численным ошибкам и переполнению (overflow) из-за больших коэффициентов, особенно на краях интервала (эффект Рунге). Точность снижается из-за накопления ошибок округления в формате IEEE 754.
- **Локальная интерполяция** (например, кубические сплайны) разбивает интервал на подинтервалы и использует полиномы низкой степени (обычно 3-й) на каждом. Это снижает риск переполнения, так как вычисления ограничены малыми интервалами, а коэффициенты остаются умеренными. Точность выше, особенно для неравномерно распределенных точек, но могут возникать проблемы с непрерывностью производных на границах интервалов.

---

## Задание 2: Умножение матриц

### Решение

```python
import numpy as np

# Размер матриц
n = 6

# Генерация случайных матриц A и B
A = np.random.rand(n, n)
B = np.random.rand(n, n)

# Наивный алгоритм умножения матриц
C = np.zeros((n, n))
for i in range(n):
    for j in range(n):
        for k in range(n):
            C[i, j] += A[i, k] * B[k, j]

# Проверка с помощью numpy
C_check = np.dot(A, B)

# Вывод результатов
print("Матрица A:")
print(A)
print("\\nМатрица B:")
print(B)
print("\\nРезультат умножения (матрица C):")
print(C)
print("\\nПроверка с np.dot:")
print(C_check)
```

**Теоретические вопросы**:

*Как алгоритм Штрассена уменьшает количество умножений для матриц? Как это влияет на асимптотическую сложность?*

Алгоритм Штрассена рекурсивно разбивает матрицы \( n \times n \) на четыре подматрицы размером \( n/2 \times n/2 \). Вместо 8 умножений подматриц (как в стандартном алгоритме) он использует 7 умножений, заменяя одно умножение дополнительными сложениями. Это достигается за счет вычисления промежуточных матриц \( P_1, \ldots, P_7 \), которые затем комбинируются для получения результата. Сложность стандартного алгоритма — \( O(n^3) \), а Штрассена — \( O(n^{\log_2 7}) \approx O(n^{2.81}) \), что значительно быстрее для больших \( n \). Однако рекурсия и дополнительные сложения увеличивают константу скрытой сложности, что снижает выгоду для малых матриц.

*Обсудите алгоритм умножения матриц.*

Стандартный алгоритм умножения матриц использует тройной цикл для вычисления каждого элемента \( C[i,j] = \sum_{k=1}^n A[i,k] \cdot B[k,j] \), что требует \( n^3 \) умножений и сложений. Он прост, стабилен и эффективен для малых матриц, так как хорошо использует кэш-память процессора благодаря последовательному доступу к данным. Однако для больших матриц его кубическая сложность становится ограничивающим фактором. Алгоритм Штрассена оптимизирует умножения, но требует больше памяти и сложений, что может привести к накоплению ошибок округления. Практически для больших матриц используются оптимизированные библиотеки (например, BLAS), которые комбинируют блочное разбиение, Штрассена и архитектурные оптимизации. Выбор алгоритма зависит от размера матриц, аппаратного обеспечения и требований к точности.
"""

def f2_7():
    """Билет 7: Интерполяция Лагранжа, умножение матриц, решение ОДУ."""
    return """# Билет 7

## Задание 1: Интерполяция многочленом Лагранжа

Высота дрона фиксируется в моменты времени (в секундах): (0,0), (2,4), (4,10), (6,18). Реализуйте интерполяцию многочленом Лагранжа для оценки высоты в момент t=3 секунды. Постройте график полинома на отрезке [0,6]. Как выбор интерполяционных точек влияет на точность многочлена Лагранжа? Объясните, как ошибки представления чисел с плавающей точкой (IEEE 754) могут накапливаться при вычислении многочленов Лагранжа высокой степени. Предложите стратегии для уменьшения этих ошибок.

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# Исходные точки
x = np.array([0, 2, 4, 6])
y = np.array([0, 4, 10, 18])

# Барицентрическая форма Лагранжа
def lagrange_barycentric(x_val, x_points, y_points):
    n = len(x_points)
    w = np.ones(n)
    for j in range(n):
        for i in range(n):
            if i != j:
                w[j] *= (x_points[j] - x_points[i])
    w = 1 / w  # веса
    num = denom = 0.0
    for j in range(n):
        if np.abs(x_val - x_points[j]) < 1e-10:  # точное совпадение
            return y_points[j]
        term = w[j] / (x_val - x_points[j])
        num += term * y_points[j]
        denom += term
    return num / denom

# Точка интерполяции
x_target = 3
y_target = lagrange_barycentric(x_target, x, y)

# Построение графика
x_plot = np.linspace(0, 6, 500)
y_plot = [lagrange_barycentric(xi, x, y) for xi in x_plot]

print("=== Задание 1: Многочлен Лагранжа ===")
print(f"Значение в точке t = {x_target}: {y_target:.4f}")

plt.figure(figsize=(10, 6))
plt.plot(x_plot, y_plot, label="Интерполяция Лагранжа")
plt.scatter(x, y, color='red', zorder=5, label="Узлы")
plt.scatter(x_target, y_target, color='green', zorder=5, label=f"t={x_target}, y={y_target:.2f}")
plt.title("Интерполяция многочленом Лагранжа")
plt.xlabel("t")
plt.ylabel("y(t)")
plt.legend()
plt.grid(True)
plt.show()
```

**Результат**: Значение в \( t = 3 \approx 6.5 \).

**Теоретические вопросы**:

*Как выбор интерполяционных точек влияет на точность многочлена Лагранжа?*

Неравномерное распределение точек, особенно скопление в одной области, усиливает колебания полинома (эффект Рунге), снижая точность на краях интервала. Равномерное распределение или выбор точек Чебышева минимизирует ошибку интерполяции. Точность также зависит от гладкости интерполируемой функции: для функций с резкими изменениями многочлен высокой степени может плохо аппроксимировать данные.

*Как ошибки представления чисел с плавающей точкой (IEEE 754) могут накапливаться при вычислении многочленов Лагранжа высокой степени?*

При вычислении многочлена Лагранжа высокой степени выполняются многочисленные умножения и деления, каждое из которых вносит ошибку округления из-за ограниченной точности чисел с плавающей точкой (16-17 знаков для double). Эти ошибки накапливаются, особенно при вычислении базисных полиномов с большими коэффициентами или при близких значениях \( x_i \), что приводит к плохо обусловленным выражениям. Например, деление на малые разности \( x_i - x_j \) может усилить ошибки.

*Предложите стратегии для уменьшения этих ошибок.*

- Использовать **барицентрическую форму Лагранжа**, которая численно устойчивее стандартной формы.
- Применять **кусочно-полиномиальную интерполяцию** (например, сплайны) для снижения степени полинома.
- Нормализовать входные данные в интервал, например, \([-1, 1]\), чтобы уменьшить масштаб коэффициентов.
- Использовать точки Чебышева вместо равномерного распределения для минимизации ошибки.
- Применять высокоточные библиотеки (например, `mpmath`) для критических вычислений.

---

## Задание 2: Наивное умножение матриц 8×8

### Решение

```python
import numpy as np

# Размер матриц
n = 8

# Генерация случайных матриц A и B
A = np.random.rand(n, n)
B = np.random.rand(n, n)

# Результирующая матрица C
C = np.zeros((n, n))

# Тройной цикл для умножения матриц
for i in range(n):
    for j in range(n):
        for k in range(n):
            C[i, j] += A[i, k] * B[k, j]

# Проверка с помощью numpy
C_check = np.dot(A, B)

# Вывод результата
print("=== Задание 2: Наивное умножение матриц ===")
print("Матрица A:")
print(A)
print("\\nМатрица B:")
print(B)
print("\\nРезультат умножения (матрица C):")
print(C)
print("\\nПроверка с np.dot:")
print(C_check)
```

**Теоретические вопросы**:

*Как алгоритм Штрассена оптимизирует умножение матриц? Как это влияет на сложность в нотации big-O?*

Штрассен разбивает матрицы на подматрицы и использует 7 умножений вместо 8, заменяя умножения сложениями. Сложность снижается с \( O(n^3) \) до \( O(n^{2.81}) \). Это выгодно для больших матриц, но дополнительные сложения и рекурсия увеличивают константу сложности.

*Почему для малых матриц он может быть менее эффективен?*

Для малых матриц (например, 8×8) накладные расходы на рекурсию, выделение памяти и дополнительные сложения превышают выгоду от сокращения умножений. Наивный алгоритм проще и быстрее для таких размеров, так как лучше использует кэш процессора.

*Опишите роль архитектуры памяти в оптимизации матричных операций.*

Эффективность матричных операций зависит от локальности данных. Наивный алгоритм имеет хорошую локальность, так как последовательно обращается к строкам и столбцам, что минимизирует промахи кэша. Штрассен из-за рекурсивного разбиения и работы с подматрицами нарушает локальность, увеличивая обращения к оперативной памяти, что замедляет выполнение. Оптимизации, такие как блочное разбиение и использование кэш-ориентированных библиотек (BLAS), улучшают производительность.

*Как ошибки представления чисел в формате IEEE 754 влияют на точность умножения матриц?*

Каждая операция с плавающей точкой (сложение, умножение) вносит ошибку округления из-за ограниченной точности (около 16 знаков). В Штрассене больше сложений, что увеличивает накопление ошибок, особенно для плохо обусловленных матриц. Наивный алгоритм также подвержен ошибкам, но меньшее число операций делает его более стабильным. Для повышения точности можно использовать алгоритмы компенсации ошибок или высокоточную арифметику.

---

## Задание 3: Решение ОДУ методом Адамса-Мултона

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# Параметры
t0, tf = 0, 10
h = 0.1
y0 = 1
t = np.arange(t0, tf + h, h)
N = len(t)

# Правая часть ОДУ: dy/dt = -0.2y + cos(t)
def f(t, y):
    return -0.2 * y + np.cos(t)

# Рунге-Кутта 4-го порядка для начальных шагов
def rk4(f, t, y0, h, n_steps):
    y = np.zeros(n_steps)
    y[0] = y0
    for i in range(n_steps - 1):
        k1 = f(t[i], y[i])
        k2 = f(t[i] + h/2, y[i] + h*k1/2)
        k3 = f(t[i] + h/2, y[i] + h*k2/2)
        k4 = f(t[i] + h, y[i] + h*k3)
        y[i + 1] = y[i] + (h/6) * (k1 + 2*k2 + 2*k3 + k4)
    return y

# Адамс-Мултон 4-го порядка
def adams_moulton(f, t, y0, h):
    N = len(t)
    y = np.zeros(N)
    y[0] = y0
    y[:4] = rk4(f, t[:4], y0, h, 4)
    for i in range(3, N - 1):
        y_pred = y[i] + (h/24) * (55*f(t[i], y[i]) - 59*f(t[i-1], y[i-1]) +
                                  37*f(t[i-2], y[i-2]) - 9*f(t[i-3], y[i-3]))
        y_next = y[i] + (h/24) * (9*f(t[i+1], y_pred) + 19*f(t[i], y[i]) -
                                  5*f(t[i-1], y[i-1]) + f(t[i-2], y[i-2]))
        for _ in range(5):
            y_next = y[i] + (h/24) * (9*f(t[i+1], y_next) + 19*f(t[i], y[i]) -
                                      5*f(t[i-1], y[i-1]) + f(t[i-2], y[i-2]))
        y[i+1] = y_next
    return y

# Эйлер
def euler(f, t, y0, h):
    y = np.zeros(len(t))
    y[0] = y0
    for i in range(len(t) - 1):
        y[i+1] = y[i] + h * f(t[i], y[i])
    return y

# Вычисление решений
y_am = adams_moulton(f, t, y0, h)
y_euler = euler(f, t, y0, h)

# Аналитическое решение
y_exact = (5/26) * (5*np.cos(t) + np.sin(t)) + (21/26) * np.exp(-0.2 * t)

# Ошибки
error_am = np.abs(y_am - y_exact)
error_euler = np.abs(y_euler - y_exact)

# Вывод результатов
print("=== Задание 3: Метод Адамса-Мултона и Эйлера ===")
print(f"Максимальная ошибка Адамса-Мултона: {np.max(error_am):.6f}")
print(f"Максимальная ошибка Эйлера: {np.max(error_euler):.6f}")

# Графики
plt.figure(figsize=(12, 8))
plt.subplot(2, 1, 1)
plt.plot(t, y_am, 'b-', label='Адамс-Мултон')
plt.plot(t, y_euler, 'r--', label='Эйлер')
plt.plot(t, y_exact, 'k:', label='Аналитическое')
plt.title('Решение ОДУ: затухающий осциллятор')
plt.xlabel('t')
plt.ylabel('y(t)')
plt.grid(True)
plt.legend()
plt.subplot(2, 1, 2)
plt.plot(t, error_am, 'b-', label='Ошибка Адамса-Мултона')
plt.plot(t, error_euler, 'r--', label='Ошибка Эйлера')
plt.title('Абсолютная ошибка')
plt.xlabel('t')
plt.ylabel('Ошибка |y - y_exact|')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()
```

**Теоретические вопросы**:

*Как порядок точности метода Адамса-Мултона влияет на его точность по сравнению с методом Эйлера?*

Адамс-Мултон 4-го порядка имеет локальную ошибку \( O(h^5) \), что приводит к глобальной ошибке \( O(h^4) \). Эйлер 1-го порядка имеет локальную ошибку \( O(h^2) \) и глобальную \( O(h) \). Высокий порядок Адамса-Мултона обеспечивает значительно меньшую ошибку при том же шаге \( h \), особенно на длинных интервалах.

*Как локальная ошибка усечения влияет на глобальную ошибку?*

Локальная ошибка усечения возникает на каждом шаге из-за аппроксимации производной. Для метода порядка \( p \), локальная ошибка ~ \( O(h^{p+1}) \). Глобальная ошибка накапливается за \( N \approx T/h \) шагов, что дает \( O(h^p) \). Таким образом, методы высокого порядка (как Адамс-Мултон) имеют меньшую глобальную ошибку, так как их локальная ошибка мала.

*Опишите понятия слабой и строгой устойчивости численных методов.*

- **Слабая устойчивость**: Метод устойчив только при определенных условиях, например, при достаточно малом шаге \( h \). Например, метод Эйлера требует \( h < 2/|\lambda| \) для уравнения \( y' = \lambda y \).
- **Строгая устойчивость**: Метод устойчив для любых допустимых \( h \), особенно для жестких уравнений. Неявные методы, такие как Адамс-Мултон, часто строго устойчивы, что делает их предпочтительными для систем с большими собственными значениями.
"""

def f2_14():
    """Билет 14: Гаусс-Зейдель, QR-разложение, ДПФ."""
    return """# Билет 14

## Задание 1: Метод Гаусса-Зейделя

Для моделирования взаимодействия двух видов решается система: \( x^2 - y = 2 \), \( xy = 1 \). Используйте метод Гаусса-Зейделя с начальным приближением (x0, y0) = (1.5, 0.7). Найдите решение системы. Сравните метод Гаусса-Зейделя с методом Ньютона по устойчивости к ошибкам округления. Объясните, как константа Липшица влияет на сходимость.

### Решение

```python
import numpy as np

# Система: x^2 - y = 2, xy = 1
def gauss_seidel(x0, y0, tol=1e-6, max_iter=100):
    x, y = x0, y0
    for i in range(max_iter):
        x_new = 1 / y
        y_new = x_new**2 - 2
        residual = np.sqrt((x_new - x)**2 + (y_new - y)**2)
        if residual < tol:
            print(f"Сошлось за {i+1} итераций")
            return x_new, y_new
        x, y = x_new, y_new
    print("Не сошлось")
    return x, y

# Начальное приближение
x0, y0 = 1.5, 0.7
x, y = gauss_seidel(x0, y0)

print("=== Задание 1: Метод Гаусса-Зейделя ===")
print(f"x = {x:.6f}, y = {y:.6f}")
```

**Результат**: \( x \approx 1.618034 \), \( y \approx 0.618034 \).

**Теоретические вопросы**:

*Как выбор начального приближения влияет на сходимость?*

Метод Гаусса-Зейделя итеративно обновляет переменные, используя последние значения. Если начальное приближение близко к корню, сходимость быстрая (линейная для нелинейных систем). Удаленное или неудачное приближение (например, вблизи точек, где производные велики) может привести к расходимости или медленной сходимости. Для данной системы корни соответствуют золотому сечению, и (1.5, 0.7) — удачный выбор.

*Сравните метод Гаусса-Зейделя с методом Ньютона по устойчивости к ошибкам округления?*

- **Ньютон**: Квадратичная сходимость, но требует вычисления Якобиана и решения линейной системы на каждой итерации. При плохо обусловленном Якобиане ошибки округления усиливаются, что снижает устойчивость.
- **Гаусс-Зейдель**: Линейная сходимость, но проще в реализации (нет производных). Менее чувствителен к ошибкам округления для систем с диагональным преобладанием, но сходимость зависит от структуры системы.

*Как константа Липшица влияет на сходимость?*

Константа Липшица \( L \) ограничивает скорость изменения функции: \( |f(x) - f(y)| \leq L |x - y| \). Для итерационных методов типа \( x = g(x) \), сходимость требует \( L < 1 \). Меньшая \( L \) ускоряет сходимость, делая итерации более сжимающими. Большая \( L \) может вызвать расходимость или замедлить процесс, особенно для нелинейных систем.

---

## Задание 2: QR-разложение

### Решение

```python
import numpy as np

# Матрица A
A = np.array([
    [15, 2, 3, 4, 5],
    [2, 25, 6, 7, 8],
    [3, 6, 35, 9, 10],
    [4, 7, 9, 45, 11],
    [5, 8, 10, 11, 55]
], dtype=float)

m, n = A.shape
Q = np.zeros((m, n))
R = np.zeros((n, n))

# Грама-Шмидт
for j in range(n):
    v = A[:, j].copy()
    for i in range(j):
        R[i, j] = np.dot(Q[:, i], A[:, j])
        v -= R[i, j] * Q[:, i]
    R[j, j] = np.linalg.norm(v)
    if R[j, j] > 1e-10:
        Q[:, j] = v / R[j, j]

# Проверка ортогональности Q
Q_orth_check = np.dot(Q.T, Q)

print("=== Задание 2: QR-разложение ===")
print("Матрица Q:")
print(Q)
print("\\nМатрица R:")
print(R)
print("\\nПроверка ортогональности Q^T Q:")
print(Q_orth_check)
```

**Теоретические вопросы**:

*Как разложение по собственным векторам используется в методе главных компонент?*

В PCA ковариационная матрица данных раскладывается на собственные векторы и значения. Векторы указывают направления максимальной дисперсии (главные компоненты), а значения — их значимость. Данные проецируются на первые компоненты для снижения размерности.

*Как это связано с вычислением SVD?*

SVD раскладывает матрицу \( A = U \Sigma V^T \), где \( U \) и \( V \) — ортогональные матрицы, а \( \Sigma \) — диагональная. Для PCA, если \( A \) — центрированные данные, то \( V \) содержит главные компоненты, а диагональные элементы \( \Sigma^2/n \) — собственные значения ковариационной матрицы. SVD численно устойчивее, чем диагонализация.

*Чем отличаются QR-алгоритмы Хаусхолдера и Гивенса?*

- **Хаусхолдер**: Использует отражения для обнуления элементов под диагональю. Эффективен для плотных матриц, требует меньше операций.
- **Гивенс**: Использует повороты для обнуления отдельных элементов. Подходит для разреженных матриц и параллельных вычислений, но медленнее.

---

## Задание 3: Дискретное преобразование Фурье

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# Параметры
N = 64
sr = N
n = np.arange(N)

# Сигнал
x = np.cos(2 * np.pi * n / 8) + 0.3 * np.sin(4 * np.pi * n / 8)

# ДПФ
def DFT(x):
    N = len(x)
    n = np.arange(N)
    k = n.reshape((N, 1))
    e = np.exp(-2j * np.pi * k * n / N)
    X = e @ x
    return X

X = DFT(x)
freq = np.arange(N) * sr / N
amplitude = np.abs(X) / N

# Значимые гармоники
significant_indices = np.where(amplitude > 0.01)[0]
significant_freqs = freq[significant_indices]
significant_amps = amplitude[significant_indices]

print("=== Задание 3: Дискретное преобразование Фурье ===")
print("Наиболее значимые гармоники:")
for f, a in zip(significant_freqs, significant_amps):
    print(f"Частота: {f:.2f} Гц, Амплитуда: {a:.4f}")

# График
plt.figure(figsize=(10, 5))
plt.stem(freq[:N//2], amplitude[:N//2], 'b', markerfmt=' ', basefmt='-b')
plt.title('Амплитудный спектр')
plt.xlabel('Частота (Гц)')
plt.ylabel('Амплитуда |X(f)|')
plt.grid(True)
plt.show()
```

**Теоретические вопросы**:

*Как ограниченная длина сигнала влияет на спектральное разрешение ДПФ?*

Разрешение ДПФ равно \( f_s / N \), где \( f_s \) — частота дискретизации, \( N \) — длина сигнала. Маленький \( N \) дает грубое разрешение, затрудняя различение близких частот. Также возникает утечка спектра из-за обрезания сигнала.

*Как можно улучшить разрешение?*

- Увеличить \( N \), записывая сигнал дольше.
- Применить zero-padding, добавляя нули к сигналу.
- Использовать оконные функции (Хэмминг, Ханн) для снижения утечки спектра.

*Как БПФ применяется в анализе сезонности временных рядов?*

БПФ разлагает временной ряд на частотные компоненты, выявляя периодические закономерности (например, суточные или годовые циклы). Значимые частоты соответствуют сезонным периодам, что используется для прогнозирования и моделирования.

*Как выбор длины сигнала влияет на точность?*

Большая длина улучшает частотное разрешение, но увеличивает вычислительную нагрузку. Длина, кратная периоду сигнала, минимизирует утечку спектра. Маленькая длина снижает точность, теряя детали спектра.
"""

def f2_15():
    """Билет 15: Интерполяция Лагранжа, Штрассен, центральная разность."""
    return """# Билет 15

## Задание 1: Интерполяция многочленом Лагранжа

В анализе данных окружающей среды измерения температуры проводятся нерегулярно: точки (0, 15), (2, 18), (5, 22), (8, 20). Реализуйте интерполяцию многочленом Лагранжа для оценки температуры в t=4 часа. Постройте график полинома на [0,8]. Объясните, как многочлен Лагранжа обеспечивает точное прохождение через заданные точки. Как степень полинома влияет на точность интерполяции для зашумлённых данных?

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# Исходные данные
x = np.array([0, 2, 5, 8])
y = np.array([15, 18, 22, 20])

# Барицентрическая форма Лагранжа
def lagrange_barycentric(x_val, x_points, y_points):
    n = len(x_points)
    w = np.ones(n)
    for j in range(n):
        for i in range(n):
            if i != j:
                w[j] *= (x_points[j] - x_points[i])
    w = 1 / w
    num = denom = 0.0
    for j in range(n):
        if np.abs(x_val - x_points[j]) < 1e-10:
            return y_points[j]
        term = w[j] / (x_val - x_points[j])
        num += term * y_points[j]
        denom += term
    return num / denom

# Точка интерполяции
x_target = 4
y_target = lagrange_barycentric(x_target, x, y)

# Построение графика
x_plot = np.linspace(0, 8, 500)
y_plot = [lagrange_barycentric(xi, x, y) for xi in x_plot]

print("=== Задание 1: Многочлен Лагранжа ===")
print(f"Температура в t = {x_target}: {y_target:.4f} °C")

plt.figure(figsize=(10, 6))
plt.plot(x_plot, y_plot, label="Интерполяция Лагранжа")
plt.scatter(x, y, color='red', zorder=5, label="Узлы")
plt.scatter(x_target, y_target, color='green', zorder=5, label=f"t={x_target}, y={y_target:.2f}")
plt.title("Интерполяция температуры многочленом Лагранжа")
plt.xlabel("Время (часы)")
plt.ylabel("Температура (°C)")
plt.legend()
plt.grid(True)
plt.show()
```

**Результат**: Температура в \( t = 4 \approx 21.2857 \) °C.

**Теоретические вопросы**:

*Как многочлен Лагранжа обеспечивает точное прохождение через заданные точки?*

Многочлен Лагранжа \( P(x) = \sum_{i=0}^n y_i \ell_i(x) \) строится как сумма базисных полиномов \( \ell_i(x) = \prod_{j \neq i} \frac{x - x_j}{x_i - x_j} \), где \( \ell_i(x_i) = 1 \) и \( \ell_i(x_j) = 0 \) для \( j \neq i \). Это гарантирует, что \( P(x_i) = y_i \), то есть полином точно проходит через все заданные точки \( (x_i, y_i) \). Свойство обеспечивается уникальностью интерполяционного полинома степени \( \leq n \) для \( n+1 \) точек.

*Как степень полинома влияет на точность интерполяции для зашумлённых данных?*

Для \( n+1 \) точек многочлен Лагранжа имеет степень \( n \). При увеличении \( n \) полином точно проходит через все точки, но для зашумленных данных высокая степень приводит к переобучению: между узлами возникают осцилляции (эффект Рунге), особенно на краях интервала. Это снижает обобщающую способность и точность. Для зашумленных данных предпочтительны методы низкой степени (например, сплайны) или регуляризация, чтобы сгладить шум.

---

## Задание 2: Умножение матриц методом Штрассена

### Решение

```python
import numpy as np
import time
import matplotlib.pyplot as plt

# Функция для дополнения матрицы до размера, кратного 2^k
def pad_matrix(A, new_size):
    n = A.shape[0]
    padded = np.zeros((new_size, new_size))
    padded[:n, :n] = A
    return padded

# Метод Штрассена
def strassen(A, B, threshold=64):
    if A.shape[0] <= threshold:
        return np.dot(A, B)
    n = A.shape[0]
    half = n // 2
    A11, A12, A21, A22 = A[:half, :half], A[:half, half:], A[half:, :half], A[half:, half:]
    B11, B12, B21, B22 = B[:half, :half], B[:half, half:], B[half:, :half], B[half:, half:]
    P1 = strassen(A11 + A22, B11 + B22, threshold)
    P2 = strassen(A21 + A22, B11, threshold)
    P3 = strassen(A11, B12 - B22, threshold)
    P4 = strassen(A22, B21 - B11, threshold)
    P5 = strassen(A11 + A12, B22, threshold)
    P6 = strassen(A21 - A11, B11 + B12, threshold)
    P7 = strassen(A12 - A22, B21 + B22, threshold)
    C11 = P1 + P4 - P5 + P7
    C12 = P3 + P5
    C21 = P2 + P4
    C22 = P1 - P2 + P3 + P6
    C = np.vstack((np.hstack((C11, C12)), np.hstack((C21, C22))))
    return C

# Пример матриц
A = np.array([[3, 5, 2, 1], [6, 4, 1, 5], [1, 7, 3, 2], [3, 2, 5, 4]])
B = np.array([[2, 5, 3, 0], [3, 7, 4, 1], [4, 3, 5, 3], [4, 2, 3, 3]])
result = strassen(A, B)
print("=== Задание 2: Произведение матриц Штрассена ===")
print(result)
print("\\nПроверка с np.dot:")
print(np.dot(A, B))

# Измерение времени
sizes = [2**i for i in range(2, 9)]
times_strassen = []
times_naive = []

for size in sizes:
    A = np.random.rand(size, size)
    B = np.random.rand(size, size)
    # Штрассен
    start = time.time()
    strassen(A, B)
    times_strassen.append(time.time() - start)
    # Наивный
    start = time.time()
    np.dot(A, B)
    times_naive.append(time.time() - start)

# График
plt.figure(figsize=(10, 6))
plt.plot(sizes, times_strassen, 'b-o', label='Штрассен')
plt.plot(sizes, times_naive, 'r-o', label='Наивный (np.dot)')
plt.title('Время выполнения умножения матриц')
plt.xlabel('Размер матрицы')
plt.ylabel('Время (секунды)')
plt.grid(True)
plt.xscale('log')
plt.yscale('log')
plt.legend()
plt.show()
```

**Теоретические вопросы**:

*Как алгоритм Штрассена уменьшает количество умножений по сравнению с наивным алгоритмом?*

Наивный алгоритм требует \( n^3 \) умножений для матриц \( n \times n \). Штрассен рекурсивно разбивает матрицы на подматрицы \( n/2 \times n/2 \) и использует 7 умножений вместо 8, заменяя одно умножение сложениями. Это сокращает общее число умножений до \( O(n^{\log_2 7}) \approx O(n^{2.81}) \).

*Как это влияет на асимптотическую сложность?*

Снижение числа умножений уменьшает сложность с \( O(n^3) \) до \( O(n^{2.81}) \). Однако дополнительные сложения и рекурсия увеличивают константу сложности, что делает Штрассена менее эффективным для малых матриц (например, \( n < 100 \)).

*Опишите, как архитектура памяти влияет на производительность алгоритма Штрассена.*

Штрассен из-за рекурсивного разбиения и работы с несмежными подматрицами плохо использует кэш-память процессора, что приводит к частым промахам кэша и увеличению времени доступа к данным. Наивный алгоритм имеет лучшую локальность данных, так как обращается к строкам и столбцам последовательно. Для оптимизации Штрассена применяют блочное разбиение и кэш-ориентированные реализации.

*В каких приложениях он используется?*

Штрассен применяется в задачах с большими матрицами:
- Машинное обучение (например, умножение весов в нейронных сетях).
- Научные вычисления (моделирование физических процессов).
- Компьютерная графика (трансформации).
- Библиотеки линейной алгебры (BLAS, LAPACK), где используются гибридные подходы.

---

## Задание 3: Центральная разность для второй производной

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# Функция
def f(x):
    return x**2 * np.exp(-x)

# Точная вторая производная
def f_double_prime_exact(x):
    return (4 - 4*x + x**2) * np.exp(-x)

# Центральная разность
def central_difference(f, x, h):
    return (f(x + h) - 2*f(x) + f(x - h)) / h**2

# Прямая разность
def forward_difference(f, x, h):
    return (f(x + 2*h) - 2*f(x + h) + f(x)) / h**2

# Параметры
x = 1.0
h = 0.1

# Приближения
approx_central = central_difference(f, x, h)
approx_forward = forward_difference(f, x, h)
exact = f_double_prime_exact(x)

print("=== Задание 3: Центральная разность ===")
print(f"Центральная разность: {approx_central:.6f}")
print(f"Прямая разность:      {approx_forward:.6f}")
print(f"Точное значение:      {exact:.6f}")
print(f"Ошибка центральной:   {abs(approx_central - exact):.6f}")
print(f"Ошибка прямой:        {abs(approx_forward - exact):.6f}")

# Анализ ошибок
h_values = np.logspace(-4, -1, 20)
errors_central = []
errors_forward = []

for h in h_values:
    errors_central.append(abs(central_difference(f, x, h) - exact))
    errors_forward.append(abs(forward_difference(f, x, h) - exact))

# График
plt.figure(figsize=(10, 6))
plt.loglog(h_values, errors_central, 'b-o', label='Центральная разность')
plt.loglog(h_values, errors_forward, 'r-o', label='Прямая разность')
plt.title('Ошибка аппроксимации второй производной')
plt.xlabel('Шаг h')
plt.ylabel('Абсолютная ошибка')
plt.grid(True)
plt.legend()
plt.show()
```

**Теоретические вопросы**:

*Какова точность центральной разности для аппроксимации второй производной?*

Центральная разность имеет порядок точности \( O(h^2) \), так как ошибка усечения пропорциональна \( h^2 \). Это следует из разложения в ряд Тейлора: \( f(x \pm h) = f(x) \pm h f'(x) + \frac{h^2}{2} f''(x) \pm \frac{h^3}{6} f'''(x) + O(h^4) \), где члены с \( h^3 \) сокращаются.

*Как ошибка зависит от шага h?*

Ошибка центральной разности складывается из:
- **Ошибки усечения**: \( O(h^2) \), уменьшается с уменьшением \( h \).
- **Ошибки округления**: Растет при очень малых \( h \), так как деление на \( h^2 \) усиливает погрешности IEEE 754. Оптимальный \( h \approx \sqrt{\epsilon} \), где \( \epsilon \approx 10^{-16} \) для double.

*Сравните центральную разность с прямой разностью по точности и вычислительным затратам.*

- **Точность**: Центральная разность (\( O(h^2) \)) точнее прямой (\( O(h) \)), так как использует симметричную аппроксимацию, устраняющую члены первого порядка.
- **Затраты**: Центральная разность требует 3 вычислений функции (\( f(x+h), f(x), f(x-h) \)), прямая — также 3 (\( f(x), f(x+h), f(x+2h) \)). Разница в затратах минимальна, но центральная сложнее на границах, где требуется \( f(x-h) \).
"""

def f2_18():
    """Билет 18: Метод Ньютона, метод степеней, Рунге-Кутта."""
    return """# Билет 18

## Задание 1: Метод Ньютона для уравнения Ван-дер-Ваальса

Решить уравнение Ван-дер-Ваальса \( (P + a/V^2)(V - b) = RT \) при \( P = 1 \), \( T = 300 \), \( R = 0.08314 \), \( a = 0.034 \), \( b = 0.018 \) методом Ньютона с точностью \( 1e-6 \). Как выбор начального приближения влияет на сходимость? Сравните метод Ньютона с методом бисекции по скорости сходимости и требованиям к функции. Приведите пример функции, где метод Ньютона может не сойтись.

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# Параметры
R = 0.08314
a = 0.034
b = 0.018
P = 1.0
T = 300.0

# Функция и производная
def f(V):
    return (P + a / V**2) * (V - b) - R * T

def df(V):
    return (P + a / V**2) - (2 * a / V**3) * (V - b)

# Метод Ньютона
def newton_method(f, df, x0, tol=1e-6, max_iter=100):
    x = x0
    residuals = []
    for i in range(max_iter):
        fx = f(x)
        residuals.append(abs(fx))
        dfx = df(x)
        if abs(dfx) < 1e-10:
            print("Производная близка к нулю")
            break
        x -= fx / dfx
        if abs(fx) < tol:
            print(f"Сошлось за {i+1} итераций")
            return x, residuals
    print("Не сошлось")
    return x, residuals

# Решение
V0 = 2.0
V_solution, residuals = newton_method(f, df, V0)

print("=== Задание 1: Уравнение Ван-дер-Ваальса ===")
print(f"Удельный объем V = {V_solution:.6f}")

# График сходимости
plt.figure(figsize=(10, 6))
plt.semilogy(residuals, 'b-o')
plt.title('Сходимость метода Ньютона')
plt.xlabel('Итерация')
plt.ylabel('Невязка |f(V)|')
plt.grid(True)
plt.show()
```

**Результат**: \( V \approx 2.493048 \).

**Теоретические вопросы**:

*Как выбор начального приближения влияет на сходимость?*

Метод Ньютона имеет квадратичную сходимость, если начальное приближение \( V_0 \) близко к корню и производная \( f'(V) \) не равна нулю. Если \( V_0 \) далеко от корня или находится вблизи точки, где \( f'(V) \approx 0 \), метод может расходиться или сходиться медленно. Для уравнения Ван-дер-Ваальса физически осмысленное \( V_0 \approx RT/P \approx 2.5 \) обеспечивает быструю сходимость.

*Сравните метод Ньютона с методом бисекции по скорости сходимости и требованиям к функции.*

- **Ньютон**: Квадратичная сходимость (\( |x_{n+1} - x^*| \sim |x_n - x^*|^2 \)), но требует вычисления \( f \) и \( f' \). Чувствителен к начальному приближению и плохо обусловленным точкам.
- **Бисекция**: Линейная сходимость (\( |x_{n+1} - x^*| \sim |x_n - x^*|/2 \)), но гарантирует сходимость на отрезке, где \( f(a) \cdot f(b) < 0 \). Не требует производной, но медленнее.

*Приведите пример функции, где метод Ньютона может не сойтись.*

Для \( f(x) = x^3 - x \) при \( x_0 = 0 \) (где \( f'(0) = -1 \)) метод может зациклиться или расходиться из-за близости к точке с малой производной. Также \( f(x) = \arctan(x) \) с большим \( x_0 \) (например, \( x_0 = 100 \)) вызывает медленную сходимость из-за плоской функции.

---

## Задание 2: Метод степеней для собственного значения

### Решение

```python
import numpy as np

# Матрица A
A = np.array([[2, -1, 0], [-1, 2, -1], [0, -1, 2]])

# Метод степеней
def power_method(A, x0, tol=1e-5, max_iter=100):
    x = x0 / np.linalg.norm(x0)
    for i in range(max_iter):
        x_new = A @ x
        eigenvalue = np.dot(x, x_new)
        x_new /= np.linalg.norm(x_new)
        if np.linalg.norm(x_new - x) < tol:
            print(f"Сошлось за {i+1} итераций")
            return eigenvalue, x_new
        x = x_new
    return eigenvalue, x

# Решение
x0 = np.array([1.0, 1.0, 1.0])
eigenvalue, eigenvector = power_method(A, x0)

# Проверка
eigvals, eigvecs = np.linalg.eig(A)
max_eigval = np.max(np.abs(eigvals))

print("=== Задание 2: Метод степеней ===")
print(f"Наибольшее собственное значение: {eigenvalue:.6f}")
print(f"Собственный вектор: {eigenvector}")
print(f"Проверка с np.linalg.eig: {max_eigval:.6f}")
```

**Результат**: Собственное значение \( \approx 3.414214 \), вектор \( \approx [0.5, 0.7071, 0.5] \).

**Теоретические вопросы**:

*Что такое спектральный радиус матрицы и как он связан с собственными значениями?*

Спектральный радиус \( \rho(A) = \max_i |\lambda_i| \), где \( \lambda_i \) — собственные значения матрицы \( A \). Он определяет наибольшее по модулю влияние матрицы на векторы и влияет на сходимость итерационных методов (например, \( \rho(A) < 1 \) для сходимости итераций).

*Как зазор между собственными значениями влияет на сходимость?*

Сходимость метода степеней определяется отношением \( |\lambda_2| / |\lambda_1| \), где \( \lambda_1 \) — наибольшее по модулю, \( \lambda_2 \) — второе. Большой зазор (\( |\lambda_2| \ll |\lambda_1| \)) ускоряет сходимость, так как вклад \( \lambda_2 \) быстро затухает. Маленький зазор замедляет сходимость.

*Как круги Гершгорина помогают оценить собственные значения?*

Круги Гершгорина для строки \( i \): \( |z - a_{ii}| \leq \sum_{j \neq i} |a_{ij}| \). Собственные значения лежат в объединении этих кругов. Для матрицы \( A \), диагональ \( a_{ii} = 2 \), сумма модулей вне диагонали \( \sum |a_{ij}| = 1 \) или 2, значит, \( \lambda_i \in [0, 4] \), что дает грубую оценку.

---

## Задание 3: Метод Рунге-Кутты для ОДУ

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# Правая часть
def dydt(t, y):
    return -y + t

# Точное решение
def exact(t):
    return t - 1 + np.exp(-t)

# Рунге-Кутта 4
def rk4(dydt, y0, t):
    y = np.zeros_like(t)
    y[0] = y0
    for i in range(1, len(t)):
        h = t[i] - t[i-1]
        k1 = dydt(t[i-1], y[i-1])
        k2 = dydt(t[i-1] + h/2, y[i-1] + h*k1/2)
        k3 = dydt(t[i-1] + h/2, y[i-1] + h*k2/2)
        k4 = dydt(t[i-1] + h, y[i-1] + h*k3)
        y[i] = y[i-1] + h*(k1 + 2*k2 + 2*k3 + k4)/6
    return y

# Параметры
t0, t_end = 0, 1
y0 = 0
h_values = [0.1, 0.05, 0.01]
errors = []

# Решение и анализ ошибок
for h in h_values:
    t = np.arange(t0, t_end + h, h)
    y_rk = rk4(dydt, y0, t)
    y_exact = exact(t)
    error = np.max(np.abs(y_rk - y_exact))
    errors.append(error)

# Вывод
t = np.arange(t0, t_end + 0.1, 0.1)
y_rk = rk4(dydt, y0, t)
y_exact = exact(t)

print("=== Задание 3: Рунге-Кутта ===")
print(f"Численное значение (t=1): {y_rk[-1]:.6f}")
print(f"Точное значение (t=1):   {y_exact[-1]:.6f}")
print("\\nАнализ ошибок:")
for h, err in zip(h_values, errors):
    print(f"h = {h:.3f}, Макс. ошибка = {err:.6f}")

# Графики
plt.figure(figsize=(12, 8))
plt.subplot(2, 1, 1)
plt.plot(t, y_rk, 'b-o', label='Рунге-Кутта')
plt.plot(t, y_exact, 'r--', label='Точное')
plt.title('Решение ОДУ')
plt.xlabel('t')
plt.ylabel('y(t)')
plt.grid(True)
plt.legend()
plt.subplot(2, 1, 2)
plt.loglog(h_values, errors, 'b-o', label='Ошибка')
plt.title('Зависимость ошибки от шага')
plt.xlabel('Шаг h')
plt.ylabel('Макс. ошибка')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()
```

**Теоретические вопросы**:

*Сравните численное решение с точным решением.*

Рунге-Кутта 4-го порядка дает решение, близкое к точному \( y(t) = t - 1 + e^{-t} \), с максимальной ошибкой порядка \( 10^{-5} \) при \( h = 0.1 \). Ошибка уменьшается с уменьшением шага.

*В чём разница между локальной и глобальной ошибками численных методов для ОДУ?*

- **Локальная ошибка**: Погрешность на одном шаге, для Рунге-Кутты 4 порядка — \( O(h^5) \).
- **Глобальная ошибка**: Накопленная погрешность за весь интервал, для Рунге-Кутты — \( O(h^4) \), так как за \( N \approx T/h \) шагов ошибка суммируется.

*Как порядок точности метода влияет на величину ошибки?*

Метод порядка \( p \) имеет локальную ошибку \( O(h^{p+1}) \) и глобальную \( O(h^p) \). Для Рунге-Кутты 4 (\( p=4 \)) уменьшение \( h \) в 2 раза сокращает глобальную ошибку в \( 2^4 = 16 \) раз, что делает его очень точным.

*Что понимается под устойчивостью численного метода решения ОДУ?*

Устойчивость — способность метода ограничивать рост ошибок при малых возмущениях начальных данных или параметров. Для уравнения \( y' = \lambda y \), метод устойчив, если решения остаются ограниченными при \( h \lambda \) в области устойчивости. Рунге-Кутта 4 устойчив для нежестких уравнений при малых \( h \), но для жестких систем предпочтительны неявные методы.
"""

def f2_29():
    """Билет 29: Гаусс-Зейдель, наивное умножение матриц, ДПФ."""
    return """# Билет 29

## Задание 1: Метод Гаусса-Зейделя

Для моделирования взаимодействия видов решается система: \( x^2 - y = 1 \), \( x - y^2 = 0 \). Используйте метод Гаусса-Зейделя с начальным приближением \( (x_0, y_0) = (1.5, 1.5) \). Сравните метод Гаусса-Зейделя с методом Ньютона по вычислительной сложности и устойчивости к ошибкам округления. В чём разница между абсолютной и относительной погрешностями? Как они связаны с ошибками округления в арифметике с плавающей точкой?

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# Функция для невязки системы
def residual(x, y):
    return np.array([x**2 - y - 1, x - y**2])

# Метод Гаусса-Зейделя
def gauss_seidel(x0, y0, tol=1e-6, max_iter=1000, max_value=1e10):
    x, y = x0, y0
    residuals = []
    for i in range(max_iter):
        x_new = y**2
        if x_new > max_value:
            print("Переполнение x")
            break
        y_new = x_new**2 - 1
        if abs(y_new) > max_value:
            print("Переполнение y")
            break
        residuals.append(np.linalg.norm(residual(x_new, y_new)))
        if max(abs(x_new - x), abs(y_new - y)) < tol:
            print(f"Сошлось за {i+1} итераций")
            return x_new, y_new, residuals
        x, y = x_new, y_new
    print("Не сошлось")
    return x, y, residuals

# Решение
x0, y0 = 1.5, 1.5
x, y, residuals = gauss_seidel(x0, y0)

print("=== Задание 1: Гаусс-Зейдель ===")
print(f"x = {x:.6f}, y = {y:.6f}")

# График сходимости
plt.figure(figsize=(10, 6))
plt.semilogy(residuals, 'b-o')
plt.title('Сходимость метода Гаусса-Зейделя')
plt.xlabel('Итерация')
plt.ylabel('Невязка ||f(x,y)||')
plt.grid(True)
plt.show()
```

**Результат**: \( x \approx 1.618034 \), \( y \approx 0.618034 \).

**Теоретические вопросы**:

*Сравните метод Гаусса-Зейделя с методом Ньютона по вычислительной сложности и устойчивости к ошибкам округления.*

- **Гаусс-Зейдель**: Линейная сходимость, не требует производных, что снижает сложность (одна итерация — \( O(1) \) для 2 уравнений). Устойчив к ошибкам округления для систем с диагональным преобладанием, но может не сходиться.
- **Ньютон**: Квадратичная сходимость, но требует вычисления Якобиана и решения линейной системы (\( O(n^3) \) для \( n \times n \)). Чувствителен к ошибкам округления при плохо обусловленном Якобиане, что может привести к расходимости.

*В чём разница между абсолютной и относительной погрешностями?*

- **Абсолютная погрешность**: \( |x - \hat{x}| \), где \( x \) — истинное значение, \( \hat{x} \) — приближение. Измеряет величину ошибки в единицах измерения.
- **Относительная погрешность**: \( |x - \hat{x}| / |x| \), выражает ошибку в долях истинного значения. Полезна для сравнения ошибок при разных масштабах.

*Как они связаны с ошибками округления в арифметике с плавающей точкой?*

В формате IEEE 754 (float64) ошибка округления ~ \( 10^{-16} \). При многократных операциях ошибки накапливаются, увеличивая абсолютную погрешность. Относительная погрешность важна для оценки значимости ошибки относительно результата. Критерии остановки (например, \( tol = 10^{-6} \)) должны учитывать обе погрешности, чтобы избежать преждевременной остановки или переполнения.

---

## Задание 2: Наивное умножение матриц

### Решение

```python
import numpy as np

# Суммирование по Кахану
def kahan_sum(arr):
    sum_ = 0.0
    c = 0.0
    for x in arr:
        y = x - c
        t = sum_ + y
        c = (t - sum_) - y
        sum_ = t
    return sum_

# Наивное умножение с Каханом
def naive_multiply(A, B):
    n = A.shape[0]
    C = np.zeros((n, n))
    mult_count = 0
    for i in range(n):
        for j in range(n):
            terms = [A[i, k] * B[k, j] for k in range(n)]
            C[i, j] = kahan_sum(terms)
            mult_count += n
    return C, mult_count

# Матрицы
n = 4
A = np.random.rand(n, n)
B = np.random.rand(n, n)

# Умножение
C, mult_count = naive_multiply(A, B)

print("=== Задание 2: Наивное умножение ===")
print("Матрица A:")
print(A)
print("\\nМатрица B:")
print(B)
print("\\nРезультат C:")
print(C)
print(f"Число операций умножения: {mult_count}")
print("\\nПроверка с np.dot:")
print(np.dot(A, B))
```

**Теоретические вопросы**:

*Как количество операций умножения изменилось бы при использовании алгоритма Штрассена?*

Наивный алгоритм для \( 4 \times 4 \) требует \( n^3 = 64 \) умножений. Штрассен использует 7 умножений для \( 2 \times 2 \) подматриц вместо 8, что дает ~49 умножений для \( 4 \times 4 \), снижая число операций примерно в \( 8/7 \approx 1.14 \) раза.

*Как архитектура памяти влияет на производительность алгоритмов умножения матриц?*

Наивный алгоритм имеет хорошую локальность данных, так как обращается к строкам и столбцам последовательно, минимизируя промахи кэша. Штрассен из-за рекурсии и работы с несмежными подматрицами увеличивает промахи кэша, что снижает производительность, несмотря на меньшее число умножений.

*Как суммирование по Кахану улучшает точность?*

Суммирование по Кахану компенсирует ошибки округления, сохраняя потерянные младшие биты в переменной коррекции \( c \). Это снижает накопление ошибок при суммировании большого числа слагаемых, особенно если они различаются по порядку.

---

## Задание 3: Дискретное преобразование Фурье

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# Параметры
N = 32
sr = N
n = np.arange(N)

# Сигнал
x = np.cos(2 * np.pi * 2 * n / 16) + 0.4 * np.sin(2 * np.pi * 3 * n / 16)

# ДПФ
def DFT(x):
    N = len(x)
    n = np.arange(N)
    k = n.reshape((N, 1))
    e = np.exp(-2j * np.pi * k * n / N)
    X = e @ x
    return X

# Вычисление
X = DFT(x)
freq = np.arange(N) * sr / N
amplitude = np.abs(X) / N

# Частоты 2/16 и 3/16
idx_2 = int(2 * N / 16)  # k = 2
idx_3 = int(3 * N / 16)  # k = 3

# Сигнал с шумом
np.random.seed(42)
x_noisy = x + 0.1 * np.random.randn(N)
X_noisy = DFT(x_noisy)
amplitude_noisy = np.abs(X_noisy) / N

print("=== Задание 3: ДПФ ===")
print(f"Амплитуда на 2/16: {amplitude[idx_2]:.4f}")
print(f"Амплитуда на 3/16: {amplitude[idx_3]:.4f}")
print(f"Амплитуда с шумом на 2/16: {amplitude_noisy[idx_2]:.4f}")
print(f"Амплитуда с шумом на 3/16: {amplitude_noisy[idx_3]:.4f}")

# График
plt.figure(figsize=(10, 6))
plt.stem(freq[:N//2], amplitude[:N//2], 'b', label='Чистый сигнал')
plt.stem(freq[:N//2], amplitude_noisy[:N//2], 'r--', label='С шумом')
plt.title('Амплитудный спектр')
plt.xlabel('Частота (Гц)')
plt.ylabel('Амплитуда |X(f)|')
plt.grid(True)
plt.legend()
plt.show()
```

**Теоретические вопросы**:

*Как частота дискретизации влияет на разрешение частотного спектра в ДПФ?*

Разрешение ДПФ равно \( f_s / N \), где \( f_s \) — частота дискретизации, \( N \) — длина сигнала. Высокая \( f_s \) увеличивает максимальную частоту (теорема Найквиста), но разрешение зависит от \( N \). Для улучшения разрешения нужно увеличивать \( N \).

*Как шум может повлиять на точность выделения частот?*

Шум добавляет случайные компоненты, которые распределяются по всему спектру, снижая амплитуды истинных гармоник и создавая ложные пики. Это затрудняет идентификацию частот. Оконные функции и усреднение спектров снижают влияние шума.

*Объясните, как БПФ уменьшает вычислительную сложность по сравнению с ДПФ.*

ДПФ требует \( O(N^2) \) операций для сигнала длины \( N \). БПФ использует рекурсивное разбиение (например, алгоритм Кули-Тьюки) на подзадачи длины \( N/2 \), снижая сложность до \( O(N \log N) \), что значительно быстрее для больших \( N \).

*Как это связано с разбиением сигнала?*

БПФ делит сигнал на четные и нечетные отсчеты, вычисляя их ДПФ отдельно. Результаты комбинируются с использованием симметрии комплексных экспонент, что сокращает число операций.
"""

def f2_30():
    """Билет 30: Метод Ньютона, наивное умножение, Рунге-Кутта."""
    return """# Билет 30

## Задание 1: Метод Ньютона для системы уравнений

Для системы \( x^2 + y^2 = 2 \), \( x y = 1 \) используйте метод Ньютона с начальным приближением \( (x_0, y_0) = (1.1, 1.0) \). Решите с точностью \( 1e-6 \). Как ошибка округления влияет на сходимость метода Ньютона? Приведите пример функции, где влияние ошибок заметно. Почему важно выбирать начальное приближение?

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# Система
def F(vars):
    x, y = vars
    return np.array([x**2 + y**2 - 2, x * y - 1])

# Якобиан
def Jacobian(vars):
    x, y = vars
    return np.array([[2*x, 2*y], [y, x]])

# Метод Ньютона
def newton_system(F, J, x0, tol=1e-6, max_iter=100):
    x = x0.copy()
    residuals = []
    for i in range(max_iter):
        f_val = F(x)
        residuals.append(np.linalg.norm(f_val))
        j_val = J(x)
        try:
            delta = np.linalg.solve(j_val, -f_val)
        except np.linalg.LinAlgError:
            print("Вырожденный Якобиан")
            return None, residuals
        x += delta
        if np.linalg.norm(delta) < tol:
            print(f"Сошлось за {i+1} итераций")
            return x, residuals
    print("Не сошлось")
    return x, residuals

# Решение
initial_guess = np.array([1.1, 1.0])
solution, residuals = newton_system(F, Jacobian, initial_guess)

print("=== Задание 1: Метод Ньютона ===")
if solution is not None:
    print(f"x = {solution[0]:.6f}, y = {solution[1]:.6f}")

# График сходимости
plt.figure(figsize=(10, 6))
plt.semilogy(residuals, 'b-o')
plt.title('Сходимость метода Ньютона')
plt.xlabel('Итерация')
plt.ylabel('Невязка ||F(x,y)||')
plt.grid(True)
plt.show()
```

**Результат**: \( x \approx 1.414214 \), \( y \approx 0.707107 \).

**Теоретические вопросы**:

*Как ошибка округления в арифметике с плавающей точкой может повлиять на сходимость метода Ньютона?*

Ошибки округления в формате IEEE 754 (~ \( 10^{-16} \)) накапливаются при вычислении Якобиана и решении линейной системы. Если Якобиан плохо обусловлен (например, близок к вырожденному), ошибки усиливаются, что может замедлить сходимость или вызвать расходимость.

*Приведите пример функции, где влияние ошибок особенно заметно.*

Для системы \( F(x, y) = [x^2 - y, x - 10^{-10} y^2] \) Якобиан имеет малые элементы, что делает его плохо обусловленным. Ошибки округления при обращении матрицы сильно искажают шаг \( \delta \), нарушая сходимость.

*Почему важно правильно выбирать начальное приближение?*

Метод Ньютона имеет квадратичную сходимость только вблизи корня. Неправильное \( x_0 \) (например, где \( \det J \approx 0 \)) может привести к расходимости или сходимости к другому корню. Для данной системы \( (1, 1) \) вырождает Якобиан, поэтому \( (1.1, 1.0) \) лучше.

---

## Задание 2: Наивное умножение матриц

### Решение

```python
import numpy as np

# Суммирование по Кахану
def kahan_sum(arr):
    sum_ = 0.0
    c = 0.0
    for x in arr:
        y = x - c
        t = sum_ + y
        c = (t - sum_) - y
        sum_ = t
    return sum_

# Наивное умножение
def naive_multiply(A, B):
    n = A.shape[0]
    C = np.zeros((n, n))
    mult_count = 0
    for i in range(n):
        for j in range(n):
            terms = [A[i, k] * B[k, j] for k in range(n)]
            C[i, j] = kahan_sum(terms)
            mult_count += n
    return C, mult_count

# Матрицы
A = np.array([[3, 5, 2, 1], [6, 4, 1, 5], [1, 7, 3, 2], [3, 2, 5, 4]])
B = np.array([[2, 5, 3, 0], [3, 7, 4, 1], [4, 3, 5, 3], [4, 2, 3, 3]])

# Умножение
C, mult_count = naive_multiply(A, B)

print("=== Задание 2: Наивное умножение ===")
print("Матрица A:")
print(A)
print("\\nМатрица B:")
print(B)
print("\\nРезультат C:")
print(C)
print(f"Число операций умножения: {mult_count}")
print("\\nПроверка с np.dot:")
print(np.dot(A, B))
```

**Теоретические вопросы**:

*Как изменилось бы число умножений при использовании алгоритма Штрассена?*

Для \( 4 \times 4 \) наивный алгоритм требует 64 умножения. Штрассен снижает число до ~49 за счет 7 умножений подматриц \( 2 \times 2 \) вместо 8, что дает выигрыш в \( 8/7 \approx 1.14 \) раза.

*Как архитектура памяти влияет на производительность умножения матриц?*

Наивный алгоритм эффективно использует кэш благодаря последовательному доступу к данным. Штрассен из-за рекурсии и работы с несмежными блоками вызывает больше промахов кэша, что снижает производительность.

*Как суммирование по Кахану может улучшить точность?*

Кахан компенсирует потерю точности при суммировании, сохраняя коррекцию \( c \), что уменьшает накопление ошибок округления, особенно для больших или разнородных слагаемых.

*Чем отличаются QR-алгоритмы Хаусхолдера и Гивенса?*

- **Хаусхолдер**: Использует отражения для обнуления столбцов под диагональю. Эффективен для плотных матриц, требует \( O(n^3) \) операций.
- **Гивенс**: Применяет повороты для обнуления отдельных элементов. Лучше для разреженных матриц и параллельных вычислений, но медленнее из-за большего числа операций.

---

## Задание 3: Метод Рунге-Кутты для системы ОДУ

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# Система
def system(t, state):
    x, y = state
    return np.array([y, -x])

# Аналитическое решение
def exact(t):
    return np.array([np.cos(t), -np.sin(t)])

# Рунге-Кутта 4
def rk4(f, t_span, y0, h):
    t = np.arange(t_span[0], t_span[1] + h, h)
    y = np.zeros((len(t), len(y0)))
    y[0] = y0
    for i in range(1, len(t)):
        k1 = f(t[i-1], y[i-1])
        k2 = f(t[i-1] + h/2, y[i-1] + h*k1/2)
        k3 = f(t[i-1] + h/2, y[i-1] + h*k2/2)
        k4 = f(t[i-1] + h, y[i-1] + h*k3)
        y[i] = y[i-1] + h*(k1 + 2*k2 + 2*k3 + k4)/6
    return t, y

# Эйлер
def euler(f, t_span, y0, h):
    t = np.arange(t_span[0], t_span[1] + h, h)
    y = np.zeros((len(t), len(y0)))
    y[0] = y0
    for i in range(1, len(t)):
        y[i] = y[i-1] + h * f(t[i-1], y[i-1])
    return t, y

# Параметры
t_span = [0, 10]
y0 = np.array([1.0, 0.0])
h = 0.1

# Решение
t_rk, y_rk = rk4(system, t_span, y0, h)
t_euler, y_euler = euler(system, t_span, y0, h)
y_exact = np.array([exact(ti) for ti in t_rk])

# Ошибки
error_rk = np.max(np.abs(y_rk - y_exact), axis=1)
error_euler = np.max(np.abs(y_euler - y_exact), axis=1)

print("=== Задание 3: Рунге-Кутта ===")
print(f"x(t=10, RK4): {y_rk[-1, 0]:.6f}, y(t=10, RK4): {y_rk[-1, 1]:.6f}")
print(f"x(t=10, exact): {y_exact[-1, 0]:.6f}, y(t=10, exact): {y_exact[-1, 1]:.6f}")

# Графики
plt.figure(figsize=(12, 8))
plt.subplot(2, 1, 1)
plt.plot(y_rk[:, 0], y_rk[:, 1], 'b-', label='Рунге-Кутта')
plt.plot(y_euler[:, 0], y_euler[:, 1], 'r--', label='Эйлер')
plt.plot(y_exact[:, 0], y_exact[:, 1], 'k:', label='Точное')
plt.title('Фазовый портрет')
plt.xlabel('x')
plt.ylabel('y')
plt.grid(True)
plt.legend()
plt.subplot(2, 1, 2)
plt.plot(t_rk, error_rk, 'b-', label='Ошибка RK4')
plt.plot(t_euler, error_euler, 'r--', label='Ошибка Эйлера')
plt.title('Ошибка методов')
plt.xlabel('t')
plt.ylabel('Макс. ошибка')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()
```

**Теоретические вопросы**:

*Как фазовый портрет помогает анализировать динамические системы?*

Фазовый портрет отображает траектории в пространстве состояний (\( x, y \)), показывая характер движения: устойчивость, периодичность, хаос. Для гармонического осциллятора траектория — окружность, что указывает на сохранение энергии и периодическое движение.

*Как численные методы могут искажать фазовый портрет из-за ошибок округления?*

Ошибки округления накапливаются, вызывая дрейф траектории. Для осциллятора явные методы (например, Эйлер) могут увеличивать или уменьшать радиус окружности, нарушая сохранение энергии. Рунге-Кутта 4 минимизирует это благодаря высокой точности.

*Сравните явные и неявные методы по устойчивости.*

- **Явные методы** (Эйлер, Рунге-Кутта): Просты, но неустойчивы для жестких систем или больших \( h \). Требуют малый шаг для устойчивости (\( h < 2/|\lambda| \)).
- **Неявные методы** (неявный Эйлер): Устойчивы для жестких систем, так как решают нелинейное уравнение на каждом шаге. Более сложны, но позволяют использовать большие \( h \).
"""
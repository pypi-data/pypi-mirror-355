def f2():
    """Возвращает пронумерованный текст с 10 экзаменационными билетами."""
    return """Билет 0 (f2_0): Метод функциональной итерации, метод степеней, ДПФ
Билет 3 (f2_3): Метод функциональной итерации, метод степеней, ОДУ
Билет 4 (f2_4): Метод Ньютона, умножение матриц, ДПФ
Билет 5 (f2_5): Кубическая сплайн-интерполяция, умножение матриц
Билет 7 (f2_7): Интерполяция Лагранжа, умножение матриц, ОДУ
Билет 14 (f2_14)
Билет 15 (f2_15)
Билет 18 (f2_18)
Билет 29 (f2_29)
Билет 30 (f2_30)"""

def f2_0():
    """Билет 0: Метод функциональной итерации, метод степеней, ДПФ."""
    return """# Билет 0

## Задание 1: В физике для нахождения точки равновесия решается уравнение:sin(x)−0.5x=0 Решите это уравнение методом функциональной итерации с точностью 1е−5.Объясните, как параметр для ускорения сходимости влияет на скорость и стабильность метода.Приведите пример функции, где сходимость сильно замедляется или вообще не происходит.

### Решение

```python
import numpy as np

# Уравнение sin(x) - 0.5 * x = 0
def g(x):
    return np.sin(x) / 0.5  # преобразуем уравнение в вид x = g(x)

# Метод простой итерации
def fixed_point_method(g, x0, tol=1e-5, max_iter=100):
    x = x0
    for i in range(max_iter):
        x_new = g(x)
        if abs(x_new - x) < tol:
            print(f"Сошлось за {i+1} итераций")
            return x_new
        x = x_new
    print("Не сошлось за максимальное число итераций.")
    return x

# Начальное приближение
x_start = 1.0

# Решение уравнения
solution = fixed_point_method(g, x_start)

print("Решение x =", solution)
```

**Теоретические вопросы**:
Как параметр для ускорения сходимости влияет на скорость и стабильность метода? Параметр ускорения (например, перепроектирование функции или добавление веса к предыдущим шагам) может сделать метод более быстрым, но он также может вызвать расхождение, если выбран неправильно. Он полезен, когда стандартный метод сходится медленно, но требует аккуратного подбора.

Приведите пример функции, где сходимость сильно замедляется или вообще не происходит. Например, функция x = cos(x). Если начальное приближение слишком далеко от истинного решения или производная близка к 1, то сходимость будет очень медленной. Также возможна расходимость, если |g'(x)| > 1.
---

## Задание 2: Метод степеней

Для матрицы A найдите наибольшее собственное значение методом степеней. Матрица A: A = [[10, 3, 4, 5, 6], [3, 20, 7, 8, 9], [4, 7, 30, 11, 12], [5, 8, 11, 40, 13], [6, 9, 12, 13, 50]] Используйте начальный вектор [1, 1, 1, 1, 1]. Точность вычисления — 1e-4. Как зазор между первым и вторым собственным значением влияет на сходимость метода? В чём преимущество использования Line Profiler при анализе кода? Как архитектура памяти влияет на эффективность работы с матрицами?
### Решение

```python
import numpy as np

# Матрица A
A = np.array([[10, 3, 4, 5, 6],
              [3, 20, 7, 8, 9],
              [4, 7, 30, 11, 12],
              [5, 8, 11, 40, 13],
              [6, 9, 12, 13, 50]])

# Начальный вектор
x = np.array([1, 1, 1, 1, 1], dtype=float)

# Точность вычисления
tolerance = 1e-4

# Максимальное количество итераций для предотвращения бесконечного цикла
max_iterations = 1000

# Инициализация переменной для хранения предыдущего собственного значения
lambda_old = 0

# Функция для умножения матрицы на вектор без использования np.dot
def matrix_vector_multiply(A, x):
    n = A.shape[0]  # Размер матрицы
    result = np.zeros(n)  # Инициализация результирующего вектора нулями
    for i in range(n):
        for j in range(n):
            result[i] += A[i, j] * x[j]  # Вычисление i-й компоненты результирующего вектора
    return result

# Функция для вычисления нормы вектора (евклидова норма)
def vector_norm(x):
    return np.sqrt(np.sum(x * x))  # Сумма квадратов компонент и квадратный корень

# Итерационный процесс метода степеней
for i in range(max_iterations):
    # Умножение матрицы A на текущий вектор x
    Ax = matrix_vector_multiply(A, x)
    
    # Вычисление нового приближения собственного значения
    # Используем первую ненулевую компоненту для вычисления lambda_new
    lambda_new = Ax[0] / x[0] if x[0] != 0 else Ax[1] / x[1]
    
    # Нормализация вектора Ax для получения нового x
    norm_Ax = vector_norm(Ax)
    x = Ax / norm_Ax  # Нормализация вектора
    
    # Проверка сходимости: сравниваем текущее и предыдущее собственные значения
    if np.abs(lambda_new - lambda_old) < tolerance:
        print(f"Сходимость достигнута на итерации {i+1}")
        break
    
    # Обновление предыдущего собственного значения
    lambda_old = lambda_new

# Вывод результата
print(f"Наибольшее собственное значение: {lambda_new:.6f}")
print(f"Соответствующий собственный вектор: {x}")
```


**Теоретические вопросы**:

*Как зазор между первым и вторым собственным значением влияет на сходимость метода?*
Чем больше разница между первым и вторым собственным значением, тем быстрее метод степеней сходится к главному собственному значению. При маленьком зазоре сходимость сильно замедляется, так как второй собственный вектор начинает влиять на результат.

*В чем преимущество использования Line Profiler при анализе кода?*
Line Profiler — инструмент для профилирования кода, который измеряет время выполнения каждой строки программы. Это позволяет точно определить, какие части кода (например, циклы, матричные операции или вычисления норм) занимают больше всего времени. Преимущества:
- Выявление узких мест, таких как неэффективные циклы или повторные вычисления.
- Оптимизация ресурсоемких операций, например, умножения матриц или итераций в численном методе.
- Улучшение производительности при работе с большими данными, где даже небольшие оптимизации дают значительный эффект.
Особенно полезен при анализе матричных алгоритмов, где порядок доступа к данным или структура циклов существенно влияет на скорость.

*Как архитектура памяти влияет на эффективность работы с матрицами?*
Эффективность матричных операций зависит от того, как данные хранятся и обрабатываются в памяти компьютера. Современные процессоры используют иерархию памяти (регистры, кэш, оперативная память), и скорость доступа к данным варьируется:
- **Последовательный доступ**: Если элементы матрицы обрабатываются последовательно (например, по строкам в row-major формате, как в NumPy), данные эффективно загружаются в кэш, минимизируя промахи кэша и ускоряя вычисления.
- **Случайный доступ**: Нерегулярный доступ (например, по столбцам в row-major или хаотично) приводит к частым промахам кэша, увеличивая время обращения к оперативной памяти.
- **Локальность данных**: Алгоритмы, которые обрабатывают данные компактно (например, блочные методы), лучше используют кэш, чем те, которые требуют обращения к разным частям памяти (как в рекурсивных алгоритмах Штрассена).
- **Кэширование**: Матричные операции, оптимизированные под размер кэша (например, с использованием блоков), работают быстрее, так как данные остаются в быстром кэше дольше.
Таким образом, наивные алгоритмы умножения матриц часто выигрывают у сложных (например, Штрассена) для малых матриц из-за лучшего использования кэша, несмотря на большую асимптотическую сложность.

---

## Задание 3: Дискретное преобразование Фурье

Сгенерируйте сигнал длины 16: x[n] = sin(2 * pi * n / 8) + 0.2 * cos(2 * pi * 2n / 8), где n от 0 до 7 Вычислите его спектр с помощью быстрого преобразования Фурье (БПФ). Найдите амплитуды компонент с частотами 1/8 и 2/8. Постройте амплитудный спектр. Как ограниченная длина сигнала влияет на способность различать близкие частоты? Как можно увеличить точность анализа частот? Где применяется БПФ в реальных задачах анализа данных? Почему важно выбирать длину сигнала кратную степени двойки?
### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# Параметры сигнала
N = 16  # Длина сигнала
n_base = np.arange(8)  # Вектор индексов n от 0 до 7 (один период, T=8)

# Генерация базового сигнала для n = 0, 1, ..., 7
signal_base = np.sin(2 * np.pi * n_base / 8) + 0.2 * np.cos(4 * np.pi * n_base / 8)

# Создание сигнала длиной 16, повторяя значения для n mod 8
signal = np.zeros(N)  # Инициализация массива длиной 16
for i in range(N):
    signal[i] = signal_base[i % 8]  # Заполнение с учетом периодичности (n mod 8)

def DFT(signal):
    n_points = len(signal)  # Длина сигнала
    n = np.arange(n_points)  # Вектор индексов n
    k = n.reshape((n_points, 1))  # Вектор частот k как столбец
    # Матрица комплексных экспонент: exp(-2j * pi * k * n / N)
    e = np.exp(-2j * np.pi * k * n / n_points)
    # Ручное умножение матрицы e на вектор сигнала
    result = np.zeros(n_points, dtype=complex)  # Инициализация результирующего спектра
    for i in range(n_points):
        for j in range(n_points):
            result[i] += e[i, j] * signal[j]  # Вычисление i-й компоненты спектра
    return result

# Вычисление ДПФ
X = DFT(signal)

# Амплитудный спектр (нормализованный)
amplitude = np.abs(X) / N

# Частоты в долях от частоты дискретизации
freq = np.arange(N) / N

# Поиск амплитуд на частотах 1/8 и 2/8
# Частота 1/8 соответствует k = 1/8 * N = 2 (1/8 = 2/16)
# Частота 2/8 = 1/4 соответствует k = 1/4 * N = 4
amp_1_8 = amplitude[2]
amp_2_8 = amplitude[4]

# Вывод результатов
print(f"Амплитуда на частоте 1/8: {amp_1_8:.6f}")
print(f"Амплитуда на частоте 2/8: {amp_2_8:.6f}")

# Построение амплитудного спектра (только положительные частоты до N/2)
plt.figure(figsize=(10, 5))
plt.stem(freq[:N//2], amplitude[:N//2])
plt.title('Амплитудный спектр')
plt.xlabel('Частота (в долях от частоты дискретизации)')
plt.ylabel('Амплитуда')
plt.grid(True)
plt.show()
```


**Теоретические вопросы**:

*Как ограниченная длина сигнала влияет на способность различать близкие частоты?*
Чем короче сигнал, тем хуже разрешение по частоте. Это значит, что близкие частоты могут сливаться в один пик, и их будет сложно отличить. Для повышения точности лучше использовать длинные сигналы.

*Как можно увеличить точность анализа частот?*

Для повышения точности анализа частот можно использовать следующие методы:
1. **Увеличение длины сигнала **: Большее количество отсчетов улучшает частотное разрешение.
2. **Zero-padding**: Добавление нулей к сигналу увеличивает число точек в спектре, что делает его визуально более гладким, хотя не улучшает истинное разрешение.
3. **Оконные функции**: Применение окон (например, Ханна или Хэмминга) уменьшает утечку спектра, вызванную обрезкой сигнала, что улучшает точность выделения частот.
4. **Усреднение спектров**: Анализ нескольких реализаций сигнала с последующим усреднением спектров снижает влияние шума.
5. **Фильтрация**: Предварительное удаление шума или нежелательных частот улучшает выделение целевых компонент.
Эти методы в совокупности позволяют повысить точность определения частот и амплитуд.

*Где применяется БПФ в реальных задачах анализа данных?*

Быстрое преобразование Фурье (БПФ) широко используется в различных областях анализа данных:
- **Обработка сигналов**: Выделение частотных компонент в аудио, радиосигналах, сейсмических данных.
- **Анализ временных рядов**: Обнаружение сезонности или периодичности в финансовых данных, климатических измерениях, трафике серверов.
- **Сжатие данных**: Алгоритмы сжатия изображений (JPEG) и аудио (MP3) используют БПФ для представления данных в частотной области.
- **Медицинская диагностика**: Анализ сигналов ЭЭГ, ЭКГ, МРТ для выявления патологий.
- **Телекоммуникации**: Модуляция и демодуляция сигналов в системах связи.
- **Прогнозирование**: Построение моделей на основе частотных характеристик временных рядов.
БПФ позволяет эффективно анализировать периодические компоненты и строить точные прогнозы или диагностические модели.

*Почему важно выбирать длину сигнала кратную степени двойки?*

Алгоритмы БПФ, такие как алгоритм Кули-Тьюки, наиболее эффективны, когда длина сигнала N является степенью двойки. Это связано с рекурсивной природой алгоритма, который делит сигнал на две равные части на каждом шаге, что минимизирует количество вычислений. Для \( N = 2^k \) сложность БПФ составляет \( O(N \log N) \), тогда как для произвольной длины сигнала требуется либо дополнение до ближайшей степени двойки, либо использование менее эффективных алгоритмов с большей сложностью. Нестепенные длины увеличивают накладные расходы, так как требуют дополнительных операций или нестандартных разбиений, что снижает производительность, особенно для больших сигналов.

"""

def f2_3():
    """Билет 3: Метод функциональной итерации, метод степеней, ОДУ."""
    return """# Билет 3

## Задание 1: Метод функциональной итерации

Задание 1:В экономике для определения равновесной цены товара используется уравнение: x^2 - ln(x) - 1 = 0.Решите его методом функциональной итерации с точностью 1e-5. Как выбор функции g(x) влияет на сходимость? Сравните метод функциональной итерации с методом секущих по сложности и устойчивости. Приведите пример функции, где метод итераций работает эффективно.

### Решение

```python
import numpy as np

# #ОпределениеФункции: Преобразуем уравнение x^2 - ln(x) - 1 = 0 в x = g(x)
def g(x):
    return np.sqrt(np.log(x) + 1)  # #ФункцияG: g(x) = sqrt(ln(x) + 1) выбрана как корень из преобразованного уравнения

# Формула:
# Уравнение: x^2 - ln(x) - 1 = 0
# Преобразовано к виду: x = sqrt(ln(x) + 1), чтобы использовать метод итераций
# Почему начальное приближение x0 = 1.5?
# Потому что:
# - При x = 1 значение функции отрицательно
# - При x = 2 значение положительно
# Значит, корень лежит между ними
# Выбор 1.5 позволяет стартовать близко к истинному корню и избежать расхождения
# Также важно, чтобы x0 был больше 0, так как ln(x) не определён для x <= 0

x_old = 1.5
tolerance = 1e-5  
max_iterations = 1000  # #МаксИтерации: Ограничение для избежания бесконечного цикла

# #ИтерационныйПроцесс: Применяем g(x) до достижения точности
for i in range(max_iterations):
    x_new = g(x_old)
    # #ПроверкаСходимости: Сравниваем разницу между итерациями
    if np.abs(x_new - x_old) < tolerance:
        print(f"Сходимость достигнута на итерации {i+1}")  
        break
    x_old = x_new  # #Обновление: Передаем новое значение для следующей итерации

print(f"Равновесная цена (приближенное решение): {x_new:.8f}")
```

**Теоретические вопросы**:

*Как выбор функции g(x) влияет на сходимость?*

Функция g(x) должна быть сжимающим отображением, то есть удовлетворять условию |g'(x)| < 1 в окрестности корня. Если это условие выполняется, итерации будут сходиться к решению. Если нет — метод может расходиться или зацикливаться.

*Сравните метод функциональной итерации с методом секущих по сложности и устойчивости.*
Метод секущих обычно работает быстрее метода функциональной итерации, потому что он использует информацию о двух предыдущих точках для оценки поведения функции. Вместо того чтобы строить новую точку на основе одной предыдущей, как в методе итераций, метод секущих учитывает изменение функции между двумя соседними точками, что позволяет быстрее приближаться к корню.
Ещё одно важное преимущество — отсутствие необходимости переписывать исходное уравнение в специальную форму x = g(x), как это делается в методе итераций. Это делает метод секущих более универсальным и удобным в использовании, особенно когда преобразование уравнения затруднено или неочевидно.
Однако у метода секущих есть и недостаток: он менее устойчив, чем метод итераций. Если начальные точки выбраны неправильно (например, слишком близко друг к другу или вне области сходимости), метод может дать большую ошибку или вообще не сойтись. Особенно чувствителен он к резким изменениям функции или участкам, где производная стремится к нулю.
Таким образом, метод секущих подходит, когда нужна высокая скорость сходимости и есть уверенность в хороших начальных точках. А метод функциональной итерации лучше использовать, когда важно стабильное поведение и устойчивость к неточностям начального приближения.

*Пример функции, где метод итераций работает эффективно.*

Например, уравнение x = cos(x). Здесь функция сама по себе является сжимающей, и метод быстро сходится к корню, даже при разном начальном приближении.
---

## Задание 2: Метод степеней

Для матрицы A найдите наибольшее собственное значение методом степеней. Начальный вектор — [1, 1, 1, 1, 1]. Точность — 1e-4. Матрица A: первая строка: 5 1 2 3 4 вторая строка: 1 6 10 11 12 третья строка: 2 10 7 13 14 четвертая строка: 3 11 13 8 15 пятая строка: 4 12 14 15 9 Как метод степеней использует итерации для нахождения доминирующего собственного значения? Как отношение Релея улучшает оценку? Как зазор между собственными значениями влияет на скорость сходимости? Как модификации вроде сдвигов помогают ускорить сходимость?
### Решение

```python
import numpy as np

A = np.array([[5, 1, 2, 3, 4],
              [1, 6, 10, 11, 12],
              [2, 10, 7, 13, 14],
              [3, 11, 13, 8, 15],
              [4, 12, 14, 15, 9]])

x = np.array([1, 1, 1, 1, 1], dtype=float)
tolerance = 1e-4
max_iterations = 1000

# #Инициализация: Предыдущее собственное значение для сравнения
lambda_old = 0

# #ФункцияУмножения: Ручное умножение матрицы на вектор без np.dot
def matrix_vector_multiply(A, x):
    n = A.shape[0]  # #РазмерМатрицы: Получаем размер матрицы
    result = np.zeros(n)  # #ИнициализацияРезультата: Создаем нулевой вектор
    for i in range(n):
        for j in range(n):
            result[i] += A[i, j] * x[j]  # #ВычислениеКомпонент: Суммируем произведения
    return result

# #ФункцияНормы: Вычисление евклидовой нормы вектора
def vector_norm(x):
    return np.sqrt(np.sum(x * x))  # #Норма: Сумма квадратов с корнем

# #ИтерационныйПроцесс: Метод степеней
for i in range(max_iterations):
    # #Умножение: Применяем матрицу к текущему вектору
    Ax = matrix_vector_multiply(A, x)
    # #ВычислениеСобственногоЗначения: Используем первую компоненту для lambda_new
    lambda_new = Ax[0] / x[0] if x[0] != 0 else Ax[1] / x[1]
    # #Нормализация: Делим вектор на его норму
    x = Ax / vector_norm(Ax)
    # #ПроверкаСходимости: Сравниваем текущее и предыдущее значения
    if np.abs(lambda_new - lambda_old) < tolerance:
        print(f"Сходимость достигнута на итерации {i+1}")
        break

    # #Обновление: Сохраняем текущее значение для следующей итерации
    lambda_old = lambda_new

print(f"Наибольшее собственное значение: {lambda_new:.6f}")
print(f"Соответствующий собственный вектор: {x}")
```

**Теоретические вопросы**:

*Как метод степеней использует итерации для нахождения доминирующего собственного значения?*

Метод степеней работает так: вы начинаете с произвольного ненулевого вектора, а затем многократно умножаете его на матрицу. После каждого умножения вектор нормируется, чтобы он не рос бесконечно. Со временем этот вектор выравнивается по направлению собственного вектора, соответствующего наибольшему собственному значению. Это позволяет оценить доминирующее собственное значение через норму или отношение Релея.

*Как отношение Релея улучшает оценку?**

Отношение Релея — это способ уточнить оценку собственного значения на основе текущего приближённого собственного вектора. Оно вычисляет отношение (x^T A x) / (x^T x), что даёт более точную оценку собственного значения, чем просто длина вектора после умножения. Использование отношения Релея делает метод степеней более точным и быстрым в сходимости, особенно если вектор уже близок к истинному собственному.

**Как зазор между собственными значениями влияет на скорость сходимости?**

Скорость сходимости метода степеней зависит от соотношения между первым и вторым собственными значениями. Если второй собственный корень почти такой же по модулю, как первый, то метод будет сходиться очень медленно, потому что второй собственный вектор мешает первому. Чем больше зазор между первыми двумя собственными значениями, тем быстрее метод степеней сойдётся к главному собственному значению.

*Как модификации вроде сдвигов помогают ускорить сходимость?*
Модификации, такие как добавление сдвига (например, вычитание числа из диагонали матрицы), позволяют увеличить зазор между собственными значениями. Это делает доминирующее значение ещё более выраженным, и метод степеней сходится быстрее. Также можно использовать обратный ход метода степеней, где вместо самой матрицы используется её обратная. Это помогает находить не только наибольшее, но и наименьшее собственное значение.
---

## Задание 3: ОДУ

Рост бактерий моделируется дифференциальным уравнением: dy/dt = 0.5 * y * (1 - y / 2), при начальном условии y(0) = 0.1. Решите это уравнение на отрезке [0, 5] с шагом h = 0.1 двумя способами: Методом Эйлера. Методом Рунге-Кутты 4-го порядка. Сравните результаты. Что такое согласованность численного метода? Что значит устойчивость численного метода? Как связаны согласованность, устойчивость и сходимость? Как накопление ошибок округления влияет на устойчивость метода Эйлера?
### Решение

```python
import numpy as np

# #ФункцияПроизводной: Определяем f(t, y) = 0.5 * y * (1 - y / 2)
def f(t, y):
    return 0.5 * y * (1 - y / 2)

# #ФункцияМетодаЭйлера: Реализация метода Эйлера как отдельной функции
def euler_method(y0, t, h):
    n_steps = len(t) - 1  # #КоличествоШагов: Определяем количество итераций
    y = np.zeros(len(t))  # #Инициализация: Создаем массив для результатов
    y[0] = y0  # #НачальноеЗначение: Устанавливаем начальное условие
    for i in range(n_steps):
        y[i + 1] = y[i] + h * f(t[i], y[i])  # #ИтерацияЭйлера: Применяем формулу
    return y

# #Параметры: Устанавливаем начальные условия и шаг
t0, tf = 0, 5  # #Отрезок: От 0 до 5
h = 0.1  # #Шаг: 0.1
n_steps = int((tf - t0) / h)  # #КоличествоШагов: 50
y0 = 0.1  # #НачальноеУсловие: y(0) = 0.1
t = np.arange(t0, tf + h, h)  # #СеткаВремени: Генерируем массив t

# #ПрименениеМетодаЭйлера: Вызываем функцию
y_euler = euler_method(y0, t, h)

# #МетодРунге-Кутты4
y_rk4 = np.zeros(n_steps + 1)  # #Инициализация: Массив для результатов RK4
y_rk4[0] = y0  # #НачальноеЗначение
t_rk4 = np.arange(t0, tf + h, h)  # #СеткаВремени

for i in range(n_steps):
    k1 = f(t_rk4[i], y_rk4[i])  # #k1: Первая производная
    k2 = f(t_rk4[i] + h / 2, y_rk4[i] + h * k1 / 2)  # #k2: Вторая точка
    k3 = f(t_rk4[i] + h / 2, y_rk4[i] + h * k2 / 2)  # #k3: Третья точка
    k4 = f(t_rk4[i] + h, y_rk4[i] + h * k3)  # #k4: Четвертая точка
    y_rk4[i + 1] = y_rk4[i] + (h / 6) * (k1 + 2 * k2 + 2 * k3 + k4)  # #ИтерацияRK4

# #Сравнение: Выводим результаты для ключевых точек
print("Сравнение результатов на некоторых точках:")
for i in [0, 10, 20, 30, 40, 50]:  # #КлючевыеТочки: t = 0, 1, 2, 3, 4, 5
    print(f"t = {t[i]:.1f}, Эйлер = {y_euler[i]:.6f}, RK4 = {y_rk4[i]:.6f}")
```

**Теоретические вопросы**:

*Что такое согласованность численного метода?*

Согласованность означает, что локальная ошибка метода стремится к нулю при уменьшении шага интегрирования. То есть метод правильно воспроизводит поведение функции на одном шаге.

*Что значит устойчивость численного метода?*

Устойчивость — это способность метода сохранять стабильное поведение решения даже при наличии возмущений, таких как ошибки округления или большой шаг. Неустойчивые методы могут давать резкие скачки или расхождение.

*Как связаны согласованность, устойчивость и сходимость?*

Для того чтобы метод сходился, он должен быть согласован (правильно аппроксимировать производную) и устойчив (не усиливать ошибки). Только при выполнении обоих условий глобальная ошибка будет уменьшаться с уменьшением шага.

*Как накопление ошибок округления влияет на устойчивость метода Эйлера?*

Метод Эйлера чувствителен к ошибкам округления, особенно при большом числе итераций. Эти ошибки могут накапливаться и привести к нестабильному поведению, особенно если функция имеет жёсткую динамику или шаг слишком большой.
"""

def f2_4():
    """Билет 4: Метод Ньютона, умножение матриц, ДПФ."""
    return """# Билет 4

## Задание 1: Метод Ньютона

Уравнение моделирования равновесия: x^2 - y = 1 и x - y^2 = 0 Метод Ньютона используется с фиксированным Якобианом, вычисленным в точке (1.5, 1.5). Начальное приближение задано как (1.5, 1.5). Как константа Липшица связана с сходимостью метода функциональной итерации? Как накопление ошибок округления может повлиять на критерий остановки?
### Решение

```python
import numpy as np

# 1. Система уравнений F(x, y)
def F(x, y):
    return np.array([x**2 - y - 1, x - y**2])

# 2. Вычисление евклидовой нормы вектора (вручную)
def vector_norm(v):
    norm_sq = 0
    for component in v:
        norm_sq += component ** 2
    return np.sqrt(norm_sq)

# 3. Умножение матрицы на вектор (вручную)
def matvec_mult(A, v):
    result = np.zeros(A.shape[0])
    for i in range(A.shape[0]):
        for j in range(A.shape[1]):
            result[i] += A[i][j] * v[j]
    return result

# 4. Вычисление обратной матрицы 2x2 по формуле
def inverse_2x2(A):
    # Извлечение элементов
    a, b = A[0][0], A[0][1]
    c, d = A[1][0], A[1][1]
    # Вычисление определителя
    det = a * d - b * c
    # Обратная матрица по формуле
    inv = (1 / det) * np.array([
        [d, -b],
        [-c, a]
    ])
    return inv

# 5. Начальное приближение
x = np.array([1.5, 1.5])

# 6. Фиксированный Якобиан в точке (1.5, 1.5)
J_fixed = np.array([
    [3.0, -1.0], # Производные по x и y для первой функции: 2x, -1
    [1.0, -3.0] # Производные по x и y для второй функции: 1, -2y
])

# 7. Вычисляем обратную матрицу вручную
J_inv = inverse_2x2(J_fixed)

# 8. Параметры метода
max_iter = 100
tolerance = 1e-8

# 9. Метод Ньютона с фиксированным Якобианом
for i in range(max_iter):
    f_val = F(x[0], x[1])                   # Вычисляем значение функции
    delta = matvec_mult(J_inv, -f_val)      # Умножаем J^-1 на -F(x)
    x += delta                              # Обновляем приближение
    if vector_norm(delta) < tolerance:      # Проверка на остановку
        break

print(f"Ответ: корень системы равен ({x[0]:.6f}, {x[1]:.6f})")
```

**Теоретические вопросы**:

*Как константа Липшица связана с сходимостью метода функциональной итерации?*

Константа Липшица показывает, насколько сильно функция изменяется при малом изменении входных данных. Если она мала, это означает, что функция "плавно" меняется, и итерационный процесс будет устойчивым. Если константа велика, то даже небольшие изменения могут привести к большим отклонениям, и метод может расходиться или работать медленно.

*Как накопление ошибок округления может повлиять на критерий остановки?*

Ошибки округления возникают при каждом шаге вычисления. При использовании критерия остановки, основанного на разнице между последовательными приближениями, эти ошибки могут создавать ложное впечатление, что процесс ещё не сошёлся, даже если он уже близок к решению. Это может привести к лишним итерациям или преждевременной остановке.
---

## Задание 2: Умножение матриц

Для матриц A и B размером 4×4 реализован наивный алгоритм умножения матриц. Матрицы заданы следующим образом A = [[4, 2, 2, 0], [6, 1, 9, 1], [1, 3, 3, 2], [2, 0, 5, 4]] B = [[2, 5, 8, 0], [3, 2, 4, 1], [3, 1, 5, 2], [4, 6, 5, 3]] Число операций умножения для матрицы 4×4 составляет 64. Алгоритм Штрассена снизил бы число умножений до примерно 49 за счёт рекурсивного деления матриц на блоки. Как архитектура памяти влияет на эффективность алгоритма Штрассена? Как ошибки представления чисел могут влиять на точность результата?
### Решение

```python
import numpy as np

# Матрицы A и B
A = np.array([
    [4, 2, 2, 0],
    [6, 1, 9, 1],
    [1, 3, 3, 2],
    [2, 0, 5, 4]
])

B = np.array([
    [2, 5, 8, 0],
    [3, 2, 4, 1],
    [3, 1, 5, 2],
    [4, 6, 5, 3]
])

# Наивный алгоритм умножения матриц
def multiply(A, B):
    n = len(A)
    result = np.zeros((n, n))
    for i in range(n):          # строка A
        for j in range(n):      # столбец B
            for k in range(n):  # сумма по индексу k
                result[i][j] += A[i][k] * B[k][j]
    return result

# Вычисление произведения
C = multiply(A, B)

print("Матрица C:")
print(C)
```


**Теоретические вопросы**:

*Как архитектура памяти влияет на эффективность алгоритма Штрассена?*

Архитектура памяти играет ключевую роль в эффективности алгоритма Штрассена из-за его рекурсивной структуры:
- **Кэширование**: Процессоры используют кэш для быстрого доступа к данным. Наивный алгоритм умножения матриц имеет регулярный доступ к данным (по строкам или столбцам), что хорошо использует кэш и минимизирует промахи. Штрассен, напротив, делит матрицы на блоки, что приводит к нерегулярному доступу к памяти, увеличивая промахи кэша, особенно для малых матриц.
- **Локальность данных**: Наивный алгоритм обрабатывает данные последовательно, что эффективно для row-major или column-major форматов хранения. Штрассен требует частого обращения к разным частям матриц, что снижает локальность данных и замедляет выполнение на малых матрицах.
- **Накладные расходы**: Для малых матриц (например, 4х4) рекурсия Штрассена добавляет накладные расходы (выделение памяти, управление блоками), которые перевешивают выгоду от меньшего числа умножений. Для больших матриц Штрассен выигрывает, если блоки оптимизированы под размер кэша.
Таким образом, наивный алгоритм часто быстрее для малых матриц из-за лучшего использования кэша, тогда как Штрассен эффективен для больших матриц при правильной реализации.

*Как ошибки представления чисел могут влиять на точность результата?*

Ошибки представления чисел возникают из-за ограниченной точности чисел с плавающей запятой (например, стандарт IEEE 754 для float64):
- **Накопление ошибок**: Алгоритм Штрассена выполняет больше операций сложения и вычитания, чем наивный алгоритм, из-за рекурсивного разбиения и комбинации промежуточных результатов. Каждая операция вносит ошибку округления, которая накапливается, особенно при работе с числами разного порядка (например, большие и малые элементы матриц).
- **Потеря точности**: В Штрассене вычитания близких чисел могут привести к потере значимых цифр (cancellation error), что увеличивает относительную ошибку. Наивный алгоритм, с меньшим числом таких операций, менее подвержен этому.
- **Влияние на результат**: Для плохо обусловленных матриц (с большим числом обусловленности) накопленные ошибки могут существенно исказить элементы результирующей матрицы, особенно в Штрассене. Это может быть критично в задачах, требующих высокой точности (например, физическое моделирование).
Для минимизации ошибок используют более высокую точность (float128), устойчивые вычислительные схемы или проверку результатов на корректность.

---

## Задание 3: Дискретное преобразование Фурье

Сгенерирован сигнал длины 16: x[n] = sin(2π * 2n / 16) + 0.3cos(2π * 4n / 16) + шум амплитудой 0.05 Вычислено дискретное преобразование Фурье (ДПФ), найдены амплитуды на частотах 2/16 и 4/16. Построен амплитудный спектр.Как частота дискретизации влияет на разрешение частотного спектра в ДПФ? Как шум влияет на точность выделения частот? Как можно минимизировать его влияние? Приведите пример применения БПФ в задачах анализа данных.
### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

N = 16
n = np.arange(N)

signal = np.sin(2 * np.pi * 2 * n / 16) + 0.3 * np.cos(2 * np.pi * 4 * n / 16) + 0.05 * np.random.randn(N)

def DFT(signal):
    N = len(signal)
    n = np.arange(N)
    X = np.zeros(N, dtype=complex)
    
    for k in range(N):
        # Вычисляем комплексную экспоненту для текущего k
        exponent = np.exp(-2j * np.pi * k * n / N)
        # Суммируем по формуле ДПФ
        X[k] = np.sum(signal * exponent)
        
    return X

# --- Вычисление спектра ---
X = DFT(signal)
amplitude = np.abs(X) / N  # Нормируем амплитуды на N

# --- Вывод амплитуд на частотах 2/16 и 4/16 ---
print(f"Амплитуда на частоте 2/16: {amplitude[2]:.4f}")
print(f"Амплитуда на частоте 4/16: {amplitude[4]:.4f}")

# --- График амплитудного спектра ---
freq = n / N
plt.figure(figsize=(10, 5))
plt.stem(freq[:N//2], amplitude[:N//2])  # только положительные частоты
plt.title('Амплитудный спектр')
plt.xlabel('Частота')
plt.ylabel('Амплитуда')
plt.grid(True)
plt.tight_layout()
plt.show()
```

**Теоретические вопросы**:

*Как частота дискретизации влияет на разрешение частотного спектра в ДПФ?*

Частота дискретизации определяет, какие частоты можно увидеть в сигнале: чем она выше, тем больше максимальная частота, которую можно корректно представить. Однако разрешение по частоте — то есть способность различать близкие частоты — зависит не от частоты дискретизации, а от длины самого сигнала. Чем длиннее сигнал, тем точнее можно определить частоты. Если сигнал короткий, даже при высокой дискретизации частоты будут плохо различимы. Таким образом, частота дискретизации влияет на диапазон частот , а длина сигнала — на их разрешение .
*Как шум влияет на точность выделения частот?*

Шум (например, случайный гауссовский шум амплитудой 0.05) добавляет случайные колебания к сигналу, которые проявляются в спектре как дополнительные пики или фоновый уровень. Это может:
- Затушить слабые гармоники, делая их трудно различимыми.
- Искажать амплитуды истинных частот, особенно если шум имеет значительную мощность на тех же частотах.
- Увеличивать ошибку определения частот, так как пики становятся менее четкими.
В данном случае шум амплитудой 0.05 относительно мал, но все равно слегка искажает амплитуды (0.5 и 0.15 вместо идеальных 0.5 и 0.3).

*Как можно минимизировать влияние шума?*

Для минимизации влияния шума применяют следующие методы:
1. **Оконные функции**: Использование окон (Ханна, Хэмминга) перед ДПФ уменьшает утечку спектра, вызванную обрезкой сигнала, и снижает влияние шума на соседние частоты.
2. **Усреднение спектров**: Анализ нескольких независимых реализаций сигнала и усреднение их спектров снижает дисперсию шума, сохраняя истинные пики.
3. **Фильтрация**: Предварительное удаление шума с помощью частотных фильтров (например, полосовых или низкочастотных) улучшает выделение целевых гармоник.
4. **Увеличение длины сигнала**: Более длинный сигнал увеличивает отношение сигнал/шум в спектре, так как шум усредняется.
5. **Методы спектрального анализа**: Использование методов, устойчивых к шуму (например, метод MUSIC или параметрические модели), улучшает точность.
Эти подходы в совокупности повышают качество анализа зашумленных сигналов.

*Приведите пример применения БПФ в задачах анализа данных.*

Пример: **Анализ сезонности в продажах розничной сети**. БПФ используется для обработки временного ряда продаж за год (ежедневные данные, N = 365). Спектр выделяет частоты, соответствующие недельной (период 7 дней), месячной (период ~30 дней) и годовой (период 365 дней) сезонности. Амплитуды пиков показывают вклад каждой периодичности, а частоты помогают построить модель прогнозирования. БПФ позволяет быстро выявить доминирующие циклы и исключить шум, что улучшает точность прогнозов.

"""

def f2_5():
    """Билет 5: Кубическая сплайн-интерполяция, умножение матриц."""
    return """# Билет 5

## Задание 1: Кубическая сплайн-интерполяция

В анализе данных для восстановления пропущенных значений массива используются точки: (-2.4), (-1.1), (0.0), (1.2). Выполните кубическую сплайн-интерполяцию и определите значение в точке x = -0.5. Постройте график интерполяционной функции. Объясните, как глобальная и локальная интерполяция различаются по подходу и переполнению (overflow) на точность. 
### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# --- Исходные данные ---
x = np.array([-2, -1, 0, 1], dtype=float)
y = np.array([4, 1, 0, 2], dtype=float)

n = len(x)
m = n - 1  # количество сплайнов

# Шаги между узлами
h = np.zeros(m)
for i in range(m):
    h[i] = x[i+1] - x[i]

# --- Строим матрицу системы для c_i (вторые производные в узлах) ---
A = np.zeros((n, n))
b = np.zeros(n)

# Граничные условия: естественный сплайн ⇒ вторые производные на концах равны нулю
A[0, 0] = 1
b[0] = 0
A[-1, -1] = 1
b[-1] = 0

# Внутренние уравнения (трёхдиагональная структура)
for i in range(1, m):
    A[i, i-1] = h[i-1]
    A[i, i] = 2 * (h[i-1] + h[i])
    A[i, i+1] = h[i]
    b[i] = 3 * ((y[i+1] - y[i])/h[i] - (y[i] - y[i-1])/h[i-1])

# --- Решение системы методом прогонки ---
# Извлекаем коэффициенты a, b, c из матрицы A
a = np.zeros(n)  # нижняя диагональ
b_diag = np.zeros(n)  # главная диагональ
c = np.zeros(n)  # верхняя диагональ
d = b.copy()  # правая часть

for i in range(n):
    b_diag[i] = A[i, i]
for i in range(n - 1):
    a[i + 1] = A[i + 1, i]
    c[i] = A[i, i + 1]

# Прямой ход
alpha = np.zeros(n)
beta = np.zeros(n)

alpha[0] = 0
beta[0] = 0

for i in range(1, n):
    denom = a[i] * alpha[i - 1] + b_diag[i]
    alpha[i] = -c[i] / denom
    beta[i] = (d[i] - a[i] * beta[i - 1]) / denom

# Обратный ход
c_sol = np.zeros(n)
c_sol[-1] = beta[-1]

for i in range(n - 2, -1, -1):
    c_sol[i] = alpha[i] * c_sol[i + 1] + beta[i]

# --- Вычисляем остальные коэффициенты сплайнов ---
a_coeff = y[:-1]
d_coeff = np.zeros(m)
b_coeff = np.zeros(m)

for i in range(m):
    d_coeff[i] = (c_sol[i+1] - c_sol[i]) / (3 * h[i])
    b_coeff[i] = (y[i+1] - y[i])/h[i] - h[i]*(2*c_sol[i] + c_sol[i+1])/3

# --- Функция для оценки значения сплайна в точке x_query ---
def evaluate_spline(x_query):
    for i in range(m):
        if x[i] <= x_query <= x[i+1]:
            dx = x_query - x[i]
            return a_coeff[i] + b_coeff[i]*dx + c_sol[i]*dx**2 + d_coeff[i]*dx**3
    return None  # вне диапазона

# --- Точка интереса ---
x_query = -0.5
y_value = evaluate_spline(x_query)

# --- Построение графика ---
x_fine = np.linspace(min(x), max(x), 200)
y_fine = np.array([evaluate_spline(xi) for xi in x_fine])

plt.figure(figsize=(10, 6))
plt.plot(x_fine, y_fine, label='Кубический сплайн', color='blue')
plt.scatter(x, y, color='red', label='Исходные точки', zorder=5)
plt.scatter(x_query, y_value, color='green', label=f'Значение в x=-0.5: {y_value:.4f}')
plt.title('Кубическая сплайн-интерполяция (ручная реализация)')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# --- Ответ ---
print(f"Ответ: Значение интерполянта в точке x = -0.5 равно {y_value:.4f}")

```

**Теоретический вопрос**:

*Объясните, как глобальная и локальная интерполяция различаются по подходу и переполнению (overflow) на точность.*

- **Глобальная интерполяция** (например, полином Лагранжа или Ньютона) строит один полином высокой степени, проходящий через все точки. Для \( n \) точек используется полином степени n-1. Это делает метод чувствительным к изменениям любой точки, а при большом n высокая степень полинома приводит к численным ошибкам и переполнению (overflow) из-за больших коэффициентов, особенно на краях интервала (эффект Рунге). Точность снижается из-за накопления ошибок округления в формате IEEE 754.
- **Локальная интерполяция** (например, кубические сплайны) разбивает интервал на подинтервалы и использует полиномы низкой степени (обычно 3-й) на каждом. Это снижает риск переполнения, так как вычисления ограничены малыми интервалами, а коэффициенты остаются умеренными. Точность выше, особенно для неравномерно распределенных точек, но могут возникать проблемы с непрерывностью производных на границах интервалов.

---

## Задание 2: Умножение матриц
Реализуйте умножение двух матриц размером 6 × 6 A и B, где элементы A и B генерируются случайно с помощью np.random.rand. Выведите результирующую матрицу.Объясните, как алгоритм Штрассена уменьшает количество умножений для матриц. Как это влияет на асимптотическую сложность? Обсудите алгоритм умножения матриц.
### Решение

```python
import numpy as np

# Размер матриц
n = 6

# Генерация случайных матриц A и B
A = np.random.rand(n, n)
B = np.random.rand(n, n)

# Наивный алгоритм умножения матриц
C = np.zeros((n, n))  # Результирующая матрица

for i in range(n):
    for j in range(n):
        for k in range(n):
            C[i, j] += A[i, k] * B[k, j]

# Вывод результирующей матрицы
print("Матрица A:")
print(A)
print("\nМатрица B:")
print(B)
print("\nРезультат умножения (матрица C):")
print(C)
```

**Теоретические вопросы**:

*Как алгоритм Штрассена уменьшает количество умножений для матриц? Как это влияет на асимптотическую сложность?*

Алгоритм Штрассена рекурсивно разбивает матрицы  nхn на четыре подматрицы размером (n/2)х(n/2). Вместо 8 умножений подматриц (как в стандартном алгоритме) он использует 7 умножений, заменяя одно умножение дополнительными сложениями. Это достигается за счет вычисления промежуточных матриц, которые затем комбинируются для получения результата. Сложность стандартного алгоритма — O(n^3), а Штрассена — O(n^log₂(7)) ≈ O(n^2.81), что значительно быстрее для больших n. Однако рекурсия и дополнительные сложения увеличивают константу скрытой сложности, что снижает выгоду для малых матриц.

*Обсудите алгоритм умножения матриц.*

Стандартный метод прост и эффективен для маленьких матриц (например, 6×6), но Штрассен полезен для больших размеров, хотя требует больше памяти и может быть менее устойчивым из-за накопления ошибок округления. Для практического применения выбор зависит от размера матриц и доступных ресурсов.
"""

def f2_7():
    """Билет 7: Интерполяция Лагранжа, умножение матриц, решение ОДУ."""
    return """# Билет 7

## Задание 1: Интерполяция многочленом Лагранжа

Высота дрона фиксируется в определённые моменты времени (в секундах): (0,0) , (2,4) , (4,10) , (6,18) . Реализуйте интерполяцию многочленом Лагранжа для оценки высоты в момент t=3 секунды. Постройте график полинома на отрезке [0,6] . Как выбор интерполяционных точек влияет на точность многочлена Лагранжа? Объясните, как ошибки представления чисел с плавающей точкой (IEEE 754) могут накапливаться при вычислении многочленов Лагранжа высокой степени. Предложите стратегии для уменьшения этих ошибок.
### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# Исходные точки
x = np.array([0, 2, 4, 6])
y = np.array([0, 4, 10, 18])

# Функция для вычисления базиса Лагранжа
def lagrange_basis(x_val, x_points, i):
    basis = 1.0
    for j in range(len(x_points)):
        if i != j:
            basis *= (x_val - x_points[j]) / (x_points[i] - x_points[j])
    return basis

# Вычисление значения многочлена Лагранжа в точке x_val
def lagrange_interpolation(x_val, x_points, y_points):
    result = 0.0
    for i in range(len(x_points)):
        result += y_points[i] * lagrange_basis(x_val, x_points, i)
    return result

# Точка интерполяции
x_target = 3
y_target = lagrange_interpolation(x_target, x, y)

# Построение графика
x_plot = np.linspace(0, 6, 500)
y_plot = [lagrange_interpolation(xi, x, y) for xi in x_plot]

print(f"Значение в точке t = {x_target}: {y_target:.4f}")

plt.figure(figsize=(10, 6))
plt.plot(x_plot, y_plot, label="Интерполяция Лагранжа")
plt.scatter(x, y, color='red', zorder=5, label="Узлы")
plt.scatter(x_target, y_target, color='green', zorder=5, label=f"t={x_target}, y={y_target:.2f}")
plt.title("Интерполяция многочленом Лагранжа")
plt.xlabel("t")
plt.ylabel("y(t)")
plt.legend()
plt.grid(True)
plt.show()
```

**Теоретические вопросы**:

*Как выбор интерполяционных точек влияет на точность многочлена Лагранжа?*

Выбор точек, через которые проходит интерполяционный многочлен, сильно влияет на его поведение. Если точки расположены неудачно (например, слишком близко друг к другу или неравномерно), это может привести к сильным колебаниям между узлами и снижению точности. Особенно заметны эти эффекты на краях интервала. Для повышения устойчивости важно выбирать точки так, чтобы они равномерно описывали поведение функции на всей области определения.

*Как ошибки представления чисел с плавающей точкой (IEEE 754) могут накапливаться при вычислении многочленов Лагранжа высокой степени?*

При вычислении многочлена Лагранжа высокой степени выполняются многочисленные умножения и деления, каждое из которых вносит ошибку округления из-за ограниченной точности чисел с плавающей точкой (16-17 знаков для double). Эти ошибки накапливаются, особенно при вычислении базисных полиномов с большими коэффициентами или при близких значениях x_i, что приводит к плохо обусловленным выражениям. Например, деление на малые разности x_i - x_j может усилить ошибки.

*Предложите стратегии для уменьшения этих ошибок.*

- Использовать **барицентрическую форму Лагранжа**, которая численно устойчивее стандартной формы.
- Применять **кусочно-полиномиальную интерполяцию** (например, сплайны) для снижения степени полинома.
- Нормализовать входные данные в интервал, например,[-1, 1], чтобы уменьшить масштаб коэффициентов.
- Использовать точки Чебышева вместо равномерного распределения для минимизации ошибки.
- Применять высокоточные библиотеки (например, `mpmath`) для критических вычислений.

---

## Задание 2: Наивное умножение матриц 8×8

В вычислительной линейной алгебре для обработки данных реализуйте наивный алгоритм умножения двух квадратных матриц размером 8×8 . Используйте случайные матрицы с элементами от 0 до 1.Как алгоритм Штрассена оптимизирует умножение матриц? Как это влияет на сложность в нотации big-O? Почему для малых матриц он может быть менее эффективен? Опишите роль архитектуры памяти в оптимизации матричных операций. Как ошибки представления чисел в формате IEEE 754 влияют на точность умножения матриц?
### Решение

```python
import numpy as np

# Размер матриц
n = 8

# Генерация случайных матриц A и B
A = np.random.rand(n, n)
B = np.random.rand(n, n)

# Результирующая матрица C
C = np.zeros((n, n))

# Реализация тройного цикла для умножения матриц
for i in range(n):
    for j in range(n):
        for k in range(n):
            C[i, j] += A[i, k] * B[k, j]

# Вывод результата
print("=== Задание 2: Наивное умножение матриц ===")
print("Матрица A:")
print(A)
print("\nМатрица B:")
print(B)
print("\nРезультат умножения (матрица C):")
print(C)
```

**Теоретические вопросы**:

*Как алгоритм Штрассена оптимизирует умножение матриц? Как это влияет на сложность в нотации big-O?*

Штрассен разбивает матрицы на подматрицы и использует 7 умножений вместо 8, заменяя умножения сложениями. Сложность снижается с O(n^3) до O(n^2.81). Это выгодно для больших матриц, но дополнительные сложения и рекурсия увеличивают константу сложности.

*Почему для малых матриц он может быть менее эффективен?*

Для малых матриц (например, 8×8) накладные расходы на рекурсию, выделение памяти и дополнительные сложения превышают выгоду от сокращения умножений. Наивный алгоритм проще и быстрее для таких размеров, так как лучше использует кэш процессора.

*Опишите роль архитектуры памяти в оптимизации матричных операций.*

Эффективность матричных операций зависит от локальности данных. Наивный алгоритм имеет хорошую локальность, так как последовательно обращается к строкам и столбцам, что минимизирует промахи кэша. Штрассен из-за рекурсивного разбиения и работы с подматрицами нарушает локальность, увеличивая обращения к оперативной памяти, что замедляет выполнение. Оптимизации, такие как блочное разбиение и использование кэш-ориентированных библиотек (BLAS), улучшают производительность.

*Как ошибки представления чисел в формате IEEE 754 влияют на точность умножения матриц?*

Каждая операция с плавающей точкой (сложение, умножение) вносит ошибку округления из-за ограниченной точности (около 16 знаков). В Штрассене больше сложений, что увеличивает накопление ошибок, особенно для плохо обусловленных матриц. Наивный алгоритм также подвержен ошибкам, но меньшее число операций делает его более стабильным. Для повышения точности можно использовать алгоритмы компенсации ошибок или высокоточную арифметику.

---

## Задание 3: Решение ОДУ методом Адамса-Мултона

В физике для моделирования затухающего осциллятора решается обыкновенное дифференциальное уравнение (ОДУ): dt/dy =−0.2y+cos(t),y(0)=1 на отрезке [0,10] с шагом h=0.1 . Решите методом Адамса-Мултона. Сравните с решением методом Эйлера. Как порядок точности метода Адамса-Мултона влияет на его точность по сравнению с методом Эйлера? Как локальная ошибка усечения влияет на глобальную ошибку? Опишите понятия слабой и строгой устойчивости численных методов.

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# --- Правая часть уравнения ---
def f(t, y):
    return -0.2 * y + np.cos(t)

# --- Метод Эйлера ---
def euler(f, t_span, y0, h):
    t = np.arange(t_span[0], t_span[1] + h, h)
    y = np.zeros(len(t))
    y[0] = y0
    for i in range(len(t) - 1):
        y[i+1] = y[i] + h * f(t[i], y[i])
    return t, y

# --- Метод Адамса-Мултона (двухшаговый) ---
def adams_moulton(f, t_span, y0, h, max_iter=10, tol=1e-6):
    t = np.arange(t_span[0], t_span[1] + h, h)
    y = np.zeros(len(t))
    y[0] = y0

    # Первый шаг — метод Эйлера
    y[1] = y[0] + h * f(t[0], y[0])

    for i in range(1, len(t) - 1):
        # Прогноз
        y_pred = y[i] + h * f(t[i], y[i])

        # Итерации для уточнения
        y_new = y_pred
        for _ in range(max_iter):
            F = lambda yn: y[i] + h / 12 * (5 * f(t[i+1], yn) + 8 * f(t[i], y[i]) - f(t[i-1], y[i-1]))
            y_old = y_new
            y_new = F(y_old)
            if abs(y_new - y_old) < tol:
                break

        y[i+1] = y_new

    return t, y

# --- Параметры ---
t_span = [0, 10]
y0 = 1
h = 0.1

# --- Вычисление решений ---
t_euler, y_euler = euler(f, t_span, y0, h)
t_am, y_am = adams_moulton(f, t_span, y0, h)

# --- Точки вывода ---
points = [0, 2, 4, 6, 8, 10]
indices = [int(t / h) for t in points]

# --- Сбор значений ---
euler_values = [f"{y_euler[i]:.6f}" for i in indices]
am_values = [f"{y_am[i]:.6f}" for i in indices]

print("Метод Эйлера:", ",".join(euler_values))
print("Адамс-Мультон:", ",".join(am_values))

plt.figure(figsize=(10, 6))
plt.plot(t_euler, y_euler, label='Метод Эйлера', linestyle='--', color='blue')
plt.plot(t_am, y_am, label='Адамс-Мультон', color='green')

plt.title('Решение ОДУ: dy/dt = -0.2y + cos(t)')
plt.xlabel('t')
plt.ylabel('y(t)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
```

**Теоретические вопросы**:

*Как порядок точности метода Адамса-Мултона влияет на его точность по сравнению с методом Эйлера?*

Метод Адамса-Мултона имеет более высокий порядок точности — обычно четвёртый, в то время как метод Эйлера — первый. Это означает, что при одинаковом шаге интегрирования ошибка метода Адамса-Мултона намного меньше, чем у метода Эйлера. Особенно это заметно на длинных временных интервалах.

*Как локальная ошибка усечения влияет на глобальную ошибку?*

Локальная ошибка — это погрешность на одном шаге. Глобальная ошибка — это суммарная погрешность за всё время расчёта. При более высоком порядке точности локальная ошибка меньше, и поэтому глобальная ошибка растёт медленнее. То есть, методы с высоким порядком точности дают более стабильное и точное решение на всём интервале.

*Опишите понятия слабой и строгой устойчивости численных методов.*
Слабая устойчивость означает, что метод остаётся устойчивым только при определённых условиях, например, при маленьком шаге. Строгая устойчивость означает, что метод устойчив при любых допустимых шагах. Строго устойчивые методы предпочтительнее для жёстких систем, где требуется сохранять стабильность даже при больших шагах.

"""

def f2_14():
    """Билет 14: Гаусс-Зейдель, QR-разложение, ДПФ."""
    return """# Билет 14

## Задание 1: Метод Гаусса-Зейделя

В популяционной динамике для моделирования взаимодействия двух видов решается система x² − y = 2 и xy = 1. Используйте метод Гаусса-Зейделя с начальным приближением (x0, y0) = (1.5, 0.7). Найдите решение системы. Сравните метод Гаусса-Зейделя с методом Ньютона по устойчивости к ошибкам округления. Объясните, как константа Липшица влияет на сходимость.
### Решение

```python
# Система уравнений:
# x^2 - y = 2
# x * y = 1

# Начальное приближение
x, y = 1.5, 0.7

# Параметры остановки
tolerance = 1e-6
max_iter = 100

print("=== Задание 1: Метод Гаусса-Зейделя ===")

for i in range(max_iter):
    x_new = 1 / y                 # из второго уравнения
    y_new = x_new**2 - 2          # из первого уравнения

    if abs(x_new - x) < tolerance and abs(y_new - y) < tolerance:
        print(f"Сошлось за {i+1} итераций:")
        print(f"x = {x_new:.6f}, y = {y_new:.6f}")
        break

    x, y = x_new, y_new
else:
    print("Достигнуто максимальное число итераций.")
    print(f"x = {x:.6f}, y = {y:.6f}")
```

**Теоретические вопросы**:

*Как выбор начального приближения влияет на сходимость?*

Метод Гаусса-Зейделя итеративно обновляет переменные, используя последние значения. Если начальное приближение близко к корню, сходимость быстрая (линейная для нелинейных систем). Удаленное или неудачное приближение (например, вблизи точек, где производные велики) может привести к расходимости или медленной сходимости. Для данной системы корни соответствуют золотому сечению, и (1.5, 0.7) — удачный выбор.

*Сравните метод Гаусса-Зейделя с методом Ньютона по устойчивости к ошибкам округления?*

- **Ньютон**: Квадратичная сходимость, но требует вычисления Якобиана и решения линейной системы на каждой итерации. При плохо обусловленном Якобиане ошибки округления усиливаются, что снижает устойчивость.
- **Гаусс-Зейдель**: Линейная сходимость, но проще в реализации (нет производных). Менее чувствителен к ошибкам округления для систем с диагональным преобладанием, но сходимость зависит от структуры системы.

*Как константа Липшица влияет на сходимость?*

Константа Липшица характеризует, насколько сильно может измениться функция при малом изменении входного значения. Чем меньше эта константа, тем "плавнее" поведение функции, и выше вероятность сходимости итерационных методов. Большая константа может привести к нестабильности и замедлению сходимости, особенно в нелинейных системах.
 
---

## Задание 2: QR-разложение

Для матрицы A = [[15, 2, 3, 4, 5], [2, 25, 6, 7, 8], [3, 6, 35, 9, 10], [4, 7, 9, 45, 11], [5, 8, 10, 11, 55]] вычислите QR-разложение. Объясните, как разложение по собственным векторам используется в методе главных компонент. Как это связано с вычислением SVD?

### Решение

```python
# --- ЗАДАНИЕ 2: QR-РАЗЛОЖЕНИЕ МЕТОДОМ ГРАМА–ШМИДТА (ВРУЧНУЮ) ---

import numpy as np

# === Шаг 1: Исходная матрица A ===
A = np.array([
    [15, 2, 3, 4, 5],
    [2, 25, 6, 7, 8],
    [3, 6, 35, 9, 10],
    [4, 7, 9, 45, 11],
    [5, 8, 10, 11, 55]
], dtype=float)

m, n = A.shape  # размеры матрицы

# === Шаг 2: Инициализируем матрицы Q и R ===
Q = np.zeros((m, n))
R = np.zeros((n, n))

# === Шаг 3: Процесс Грама–Шмидта ===
for j in range(n):
    v = A[:, j].copy()  # копируем текущий столбец как вектор

    for i in range(j):
        # Вычисляем скалярное произведение вручную
        dot_product = 0.0
        for k in range(m):
            dot_product += Q[k, i] * A[k, j]
        R[i, j] = dot_product

        # Вычитаем проекцию
        for k in range(m):
            v[k] -= R[i, j] * Q[k, i]

    # Вычисляем норму (длину вектора) вручную
    norm = 0.0
    for k in range(m):
        norm += v[k] ** 2
    R[j, j] = norm ** 0.5

    # Проверяем, не нулевой ли вектор
    if R[j, j] > 1e-10:
        # Нормируем вектор
        for k in range(m):
            Q[k, j] = v[k] / R[j, j]
    else:
        print("Матрица вырожденная — невозможно построить ортонормированный базис.")

# === Шаг 4: Вывод результата ===
print("=== Задание 2: QR-разложение (Грам–Шмидт) ===")
print("Ортогональная матрица Q:")
print(Q)
print("\nВерхняя треугольная матрица R:")
print(R)
```

**Теоретические вопросы**:

*Как разложение по собственным векторам используется в методе главных компонент?*

В PCA ковариационная матрица данных раскладывается на собственные векторы и значения. Векторы указывают направления максимальной дисперсии (главные компоненты), а значения — их значимость. Данные проецируются на первые компоненты для снижения размерности.

*Как это связано с вычислением SVD?*

 SVD (сингулярное разложение) тесно связано с PCA. Оно представляет матрицу через три другие матрицы, где две из них содержат левые и правые сингулярные векторы, аналогичные собственным векторам в PCA. В отличие от диагонализации ковариационной матрицы, SVD работает напрямую с данными и считается более устойчивым численно, особенно при работе с некорректно обусловленными матрицами.

*Чем отличаются QR-алгоритмы Хаусхолдера и Гивенса?*

- **Хаусхолдер**: Использует отражения для обнуления элементов под диагональю. Эффективен для плотных матриц, требует меньше операций.
- **Гивенс**: Использует повороты для обнуления отдельных элементов. Подходит для разреженных матриц и параллельных вычислений, но медленнее.

---

## Задание 3: Дискретное преобразование Фурье

Сгенерируйте сигнал длины 64: x[n] = cos(2πn/8) + 0.3 sin(4πn/8), где n = 0, 1, ..., 7. Вычислите дискретное преобразование Фурье (ДПФ) и найдите наиболее значимые гармоники. Постройте амплитудный спектр. Объясните, как ограниченная длина сигнала влияет на спектральное разрешение ДПФ. Как можно его улучшить? Как БПФ применяется в анализе сезонности временных рядов? Как выбор длины сигнала влияет на точность?

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# === Задание 3: Дискретное преобразование Фурье ===

# Параметры сигнала
N = 64  # Длина сигнала
n = np.arange(8)  # Индексы для базового сигнала (n = 0, 1, ..., 7)

# Генерация базового сигнала длиной 8: x[n] = cos(2πn/8) + 0.3 sin(4πn/8)
x_base = np.cos(2 * np.pi * n / 8) + 0.3 * np.sin(4 * np.pi * n / 8)

# Расширение сигнала до длины 64 путем повторения
x = np.zeros(N)
for i in range(N):
    x[i] = x_base[i % 8]  # Периодическое повторение базового сигнала

# Векторизованная реализация ДПФ
def DFT(signal):
    N = len(signal)
    n = np.arange(N)
    k = n.reshape((N, 1))  # Вектор частот k как столбец
    # Матрица комплексных экспонент: exp(-2j * π * k * n / N)
    e = np.exp(-2j * np.pi * k * n / N)
    X = np.zeros(N, dtype=complex)
    # Ручное матричное умножение e @ signal
    for i in range(N):
        for j in range(N):
            X[i] += e[i, j] * signal[j]
    return X

# Вычисление ДПФ
X = DFT(x)

# Амплитудный спектр (нормализованный)
amplitude = np.abs(X) / N

# Частотная ось (в долях от частоты дискретизации)
freq = np.arange(N) / N

# Поиск наиболее значимых гармоник (амплитуды > 0.01) без np.where
significant_indices = []
significant_amps = []
for i in range(N):
    if amplitude[i] > 0.01:
        significant_indices.append(i)
        significant_amps.append(amplitude[i])
significant_freqs = freq[significant_indices]

# Вывод результатов
print("=== Задание 3: Дискретное преобразование Фурье ===")
print("Наиболее значимые гармоники:")
for f, a in zip(significant_freqs, significant_amps):
    print(f"Частота: {f:.4f} (в долях fs), Амплитуда: {a:.4f}")

# Построение графика амплитудного спектра (только положительные частоты)
plt.figure(figsize=(10, 5))
plt.stem(freq[:N//2], amplitude[:N//2], 'b', markerfmt=' ', basefmt='-b')
plt.title('Амплитудный спектр')
plt.xlabel('Частота (в долях частоты дискретизации)')
plt.ylabel('Амплитуда |X(f)|')
plt.grid(True)
plt.tight_layout()
plt.show()
```

**Теоретические вопросы**:

*Как ограниченная длина сигнала влияет на спектральное разрешение ДПФ?*

Ограниченная длина сигнала приводит к низкой частотной разрешающей способности — становится сложно различить близкие гармоники. Также возникает эффект утечки спектра, когда энергия одной частоты распределяется между несколькими соседними. Это снижает точность анализа.

*Как можно улучшить разрешение?*

Для повышения точности анализа частот можно использовать следующие методы:
1. **Увеличение длины сигнала **: Большее количество отсчетов улучшает частотное разрешение.
2. **Zero-padding**: Добавление нулей к сигналу увеличивает число точек в спектре, что делает его визуально более гладким, хотя не улучшает истинное разрешение.
3. **Оконные функции**: Применение окон (например, Ханна или Хэмминга) уменьшает утечку спектра, вызванную обрезкой сигнала, что улучшает точность выделения частот.
4. **Усреднение спектров**: Анализ нескольких реализаций сигнала с последующим усреднением спектров снижает влияние шума.
5. **Фильтрация**: Предварительное удаление шума или нежелательных частот улучшает выделение целевых компонент.
Эти методы в совокупности позволяют повысить точность определения частот и амплитуд.


*Как БПФ применяется в анализе сезонности временных рядов?*

БПФ разлагает временной ряд на частотные компоненты, выявляя периодические закономерности (например, суточные или годовые циклы). Значимые частоты соответствуют сезонным периодам, что используется для прогнозирования и моделирования.

*Как выбор длины сигнала влияет на точность?*

Большая длина улучшает частотное разрешение, но увеличивает вычислительную нагрузку. Длина, кратная периоду сигнала, минимизирует утечку спектра. Маленькая длина снижает точность, теряя детали спектра.
"""

def f2_15():
    """Билет 15: Интерполяция Лагранжа, Штрассен, центральная разность."""
    return """# Билет 15

## Задание 1: Интерполяция многочленом Лагранжа

В анализе данных окружающей среды измерения температуры проводятся нерегулярно из-за ограничений сенсоров. Даны точки, представляющие температуру (°C) в определённые моменты времени (часы): (0,15) , (2,18) , (5,22) , (8,20) . Реализуйте интерполяцию многочленом Лагранжа для оценки температуры в момент t=4 часа. Постройте график полинома на отрезке [0,8].Объясните, как многочлен Лагранжа обеспечивает точное прохождение через заданные точки. Как степень полинома влияет на точность интерполяции для зашумлённых данных?
### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# === Задание 1: Многочлен Лагранжа ===

# Исходные данные
x = np.array([0, 2, 5, 8])
y = np.array([15, 18, 22, 20])

# Функция для вычисления базиса Лагранжа
def lagrange_basis(x_val, x_points, i):
    basis = 1.0
    for j in range(len(x_points)):
        if i != j:
            basis *= (x_val - x_points[j]) / (x_points[i] - x_points[j])
    return basis

# Вычисление значения многочлена Лагранжа в точке x_val
def lagrange_interpolation(x_val, x_points, y_points):
    result = 0.0
    for i in range(len(x_points)):
        result += y_points[i] * lagrange_basis(x_val, x_points, i)
    return result

# Точка интерполяции
x_target = 4
y_target = lagrange_interpolation(x_target, x, y)

# Построение графика
x_plot = np.linspace(0, 8, 500)
y_plot = [lagrange_interpolation(xi, x, y) for xi in x_plot]

print("=== Задание 1: Многочлен Лагранжа ===")
print(f"Значение в точке t = {x_target}: {y_target:.4f}")

plt.figure(figsize=(10, 6))
plt.plot(x_plot, y_plot, label="Интерполяция Лагранжа")
plt.scatter(x, y, color='red', zorder=5, label="Узлы")
plt.scatter(x_target, y_target, color='green', zorder=5, label=f"t={x_target}, y={y_target:.2f}")
plt.title("Интерполяция многочленом Лагранжа")
plt.xlabel("t")
plt.ylabel("Температура (°C)")
plt.legend()
plt.grid(True)
plt.show()
```

**Теоретические вопросы**:

*Как многочлен Лагранжа обеспечивает точное прохождение через заданные точки?*

Многочлен Лагранжа строится как линейная комбинация базисных полиномов, каждый из которых равен единице в своей узловой точке и нулю во всех остальных. Таким образом, суммарный интерполяционный многочлен принимает ровно те значения, которые заданы в узлах, обеспечивая точное совпадение с исходными данными в этих точках . Это свойство гарантирует, что график многочлена будет проходить через все заданные точки без какой-либо погрешности в них.

*Как степень полинома влияет на точность интерполяции для зашумлённых данных?*

 С увеличением степени интерполяционного многочлена возрастает его способность точно воспроизводить данные, однако он начинает чрезмерно "подгоняться" под шумы. Это приводит к явлению осцилляций Рунге , когда между узлами возникают сильные колебания, не характерные для исходной функции. Поэтому для зашумлённых данных предпочтительнее использовать многочлены низкой степени , кусочно-полиномиальные методы (сплайны) или регуляризацию , чтобы избежать переобучения и сохранить гладкость решения.

---

## Задание 2: Умножение матриц методом Штрассена

Для матриц A = ... , B= ... выполните перемножение методом Штрассена. Задавая матрицы при помощи numpy.random, постройте график зависимости времени выполнения от размера матрицы. Как алгоритм Штрассена уменьшает количество умножений по сравнению с наивным алгоритмом? Как это влияет на асимптотическую сложность? Опишите, как архитектура памяти влияет на производительность алгоритма Штрассена. В каких приложениях он используется?

### Решение

```python
# Задание: Умножение матриц методом Штрассена и построение графика зависимости времени выполнения от размера матрицы

import numpy as np
import time
import matplotlib.pyplot as plt

# === ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ ===

# Сложение двух матриц
def mat_add(A, B):
    n = len(A)
    result = np.zeros((n, n))
    for i in range(n):
        for j in range(n):
            result[i][j] = A[i][j] + B[i][j]
    return result

# Вычитание двух матриц
def mat_sub(A, B):
    n = len(A)
    result = np.zeros((n, n))
    for i in range(n):
        for j in range(n):
            result[i][j] = A[i][j] - B[i][j]
    return result

# Наивное умножение матриц (базовый случай)
def mat_mult_naive(A, B):
    n = len(A)
    result = np.zeros((n, n))
    for i in range(n):
        for k in range(n):
            aik = A[i][k]
            for j in range(n):
                result[i][j] += aik * B[k][j]
    return result

# === РЕКУРСИВНЫЙ АЛГОРИТМ ШТРАССЕНА (без vstack/hstack) ===
def strassen(A, B):
    n = A.shape[0]

    # Базовый случай: если матрица маленькая, используем обычное умножение
    if n <= 2:
        return mat_mult_naive(A, B)

    half = n // 2

    # Разделение матрицы на подматрицы
    A11 = A[:half, :half]
    A12 = A[:half, half:]
    A21 = A[half:, :half]
    A22 = A[half:, half:]

    B11 = B[:half, :half]
    B12 = B[:half, half:]
    B21 = B[half:, :half]
    B22 = B[half:, half:]

    # Вычисляем 7 произведений
    P1 = strassen(mat_add(A11, A22), mat_add(B11, B22))
    P2 = strassen(mat_add(A21, A22), B11)
    P3 = strassen(A11, mat_sub(B12, B22))
    P4 = strassen(A22, mat_sub(B21, B11))
    P5 = strassen(mat_add(A11, A12), B22)
    P6 = strassen(mat_sub(A21, A11), mat_add(B11, B12))
    P7 = strassen(mat_sub(A12, A22), mat_add(B21, B22))

    # === Собираем результат "вручную" ===
    C11 = mat_sub(mat_add(mat_add(P1, P4), P7), P5)
    C12 = mat_add(P3, P5)
    C21 = mat_add(P2, P4)
    C22 = mat_sub(mat_add(mat_add(P1, P3), P6), P2)

    # === Объединяем блоки в одну матрицу без hstack/vstack ===
    result = np.zeros((n, n))
    
    # Копируем значения в результирующую матрицу
    for i in range(half):
        for j in range(half):
            result[i, j] = C11[i, j]  # C11
            result[i, j + half] = C12[i, j]  # C12
            result[i + half, j] = C21[i, j]  # C21
            result[i + half, j + half] = C22[i, j]  # C22

    return result

# === Шаг 1: Пример с конкретными матрицами ===

A = np.array([
    [3, 5, 2, 1],
    [6, 4, 1, 5],
    [1, 7, 3, 2],
    [3, 2, 5, 4]
], dtype=float)

B = np.array([
    [2, 5, 3, 0],
    [3, 7, 4, 1],
    [4, 3, 5, 3],
    [4, 2, 3, 3]
], dtype=float)

result = strassen(A, B)

print("=== Задание 2: Произведение матриц методом Штрассена ===")
print(result)

# === Шаг 2: Измерение времени выполнения ===

sizes = [2**i for i in range(2, 7)]  # от 4 до 64
times = []

for size in sizes:
    A_rand = np.random.rand(size, size)
    B_rand = np.random.rand(size, size)

    start_time = time.time()
    strassen(A_rand, B_rand)
    end_time = time.time()

    times.append(end_time - start_time)

# === Шаг 3: Построение графика ===

plt.figure(figsize=(10, 6))
plt.plot(sizes, times, marker='o', linestyle='-')
plt.title('Время выполнения метода Штрассена от размера матрицы')
plt.xlabel('Размер матрицы')
plt.ylabel('Времность (секунды)')
plt.grid(True)
plt.xscale('log')
plt.yscale('log')
plt.show()

# Ответ: график зависимости времени выполнения от размера матрицы построен
```

**Теоретические вопросы**:

*Как алгоритм Штрассена уменьшает количество умножений по сравнению с наивным алгоритмом?*

 Алгоритм Штрассена использует рекурсивное разбиение матриц на блоки и специальные комбинации сложений и вычитаний , чтобы сократить число операций умножения. Вместо стандартных n^3 умножений, необходимых в наивном алгоритме, Штрассену требуется всего 7 умножений вместо 8 для каждой пары блочных подматриц. Это позволяет снизить общее количество умножений при работе с крупными матрицами.

*Как это влияет на асимптотическую сложность?*

Благодаря уменьшению числа умножений, асимптотическая сложность улучшается с O(n^3) до O(n^2.81). Это может давать значительный выигрыш по времени работы при больших размерах матриц. Однако добавочные операции сложения и рекурсии увеличивают накладные расходы, что делает алгоритм менее эффективным для малых матриц.

*Опишите, как архитектура памяти влияет на производительность алгоритма Штрассена.*

Алгоритм Штрассена требует частого доступа к разным частям матриц, что плохо сочетается с кэшированием данных. Процессор работает быстрее, если данные последовательны и помещаются в кэш, но Штрассен часто "прыгает" между блоками, что замедляет работу. Кроме того, рекурсивная природа алгоритма порождает дополнительные вызовы функций и выделение памяти под временные массивы, что также снижает производительность. В отличие от него, наивный алгоритм лучше использует кэш благодаря последовательному перебору элементов, что делает его быстрее на практике для небольших матриц.

*В каких приложениях он используется?*

Штрассен применяется в задачах с большими матрицами:
- Машинное обучение (например, умножение весов в нейронных сетях).
- Научные вычисления (моделирование физических процессов).
- Компьютерная графика (трансформации).
- Библиотеки линейной алгебры (BLAS, LAPACK), где используются гибридные подходы.

---

## Задание 3: Центральная разность для второй производной

Приближённо вычислите вторую производную функции f(x) = x^2 * e^(-x) в точке x = 1 с помощью центральной разности. Используйте шаг h = 0.1. Точное значение второй производной этой функции задаётся формулой: f''(x) = (4 - 4*x + x^2) * e^(-x) Сравните приближённое значение со значением, полученным по формуле. Какова точность центральной разности для аппроксимации второй производной? Как ошибка зависит от шага h? Сравните центральную разность с прямой разностью по точности и вычислительным затратам.

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# === Задание: Приближённое вычисление второй производной ===

# Функция f(x) = x^2 * e^(-x)
def f(x):
    return x**2 * np.exp(-x)

# Точная вторая производная f''(x) = (4 - 4x + x^2) * e^(-x)
def f_double_prime_exact(x):
    return (4 - 4*x + x**2) * np.exp(-x)

# Центральная разность для второй производной
def central_difference(f, x, h):
    return (f(x + h) - 2*f(x) + f(x - h)) / (h**2)

# Прямая разность для второй производной
def forward_difference(f, x, h):

    return (f(x + 2*h) - 2*f(x + h) + f(x)) / (h**2)

# Параметры
x = 1.0  # Точка вычисления
h = 0.1  # Шаг для центральной и прямой разности

# Вычисление приближений и точного значения
approx_central = central_difference(f, x, h)
approx_forward = forward_difference(f, x, h)
exact = f_double_prime_exact(x)

# Вывод результатов
print("=== Приближённое вычисление второй производной ===")
print(f"Центральная разность: {approx_central:.6f}")
print(f"Прямая разность:      {approx_forward:.6f}")
print(f"Точное значение:      {exact:.6f}")
print(f"Ошибка центральной:   {abs(approx_central - exact):.6f}")
print(f"Ошибка прямой:        {abs(approx_forward - exact):.6f}")

# # Анализ зависимости ошибки от шага h
# n_steps = 20  # Количество значений h
# h_values = np.linspace(-4, -1, n_steps)  # Логарифмический диапазон от 10^-4 до 10^-1
# h_values = 10**h_values  # Преобразование в логарифмическую шкалу
# errors_central = np.zeros(n_steps)
# errors_forward = np.zeros(n_steps)

# for i in range(n_steps):
#     h_i = h_values[i]
#     errors_central[i] = abs(central_difference(f, x, h_i) - exact)
#     errors_forward[i] = abs(forward_difference(f, x, h_i) - exact)

# # Построение графика
# plt.figure(figsize=(8, 5))
# plt.plot(h_values, errors_central, 'b-o', label='Central Difference')
# plt.plot(h_values, errors_forward, 'r-o', label='Forward Difference')
# plt.title('Error in Second Derivative Approximation')
# plt.xlabel('Step Size h')
# plt.ylabel('Absolute Error')
# plt.xscale('log')
# plt.yscale('log')
# plt.grid(True)
# plt.legend()
# plt.show()
```

**Теоретические вопросы**:

*Какова точность центральной разности для аппроксимации второй производной?*

Центральная разность относится к методам второго порядка точности , то есть её погрешность имеет порядок O(h^2). Это означает, что при уменьшении шага h в два раза, ошибка уменьшается примерно в четыре раза. Такая высокая точность делает этот метод предпочтительным при численном дифференцировании, особенно когда важна стабильность и точность вычислений.

*Как ошибка зависит от шага h?*

Ошибка центральной разности состоит из двух компонент:
- Дискретизационная ошибка (ошибка усечения) уменьшается с уменьшением шага h , так как это метод второго порядка.
- Ошибка округления увеличивается при слишком малых значениях h из-за ограничений представления чисел с плавающей точкой (IEEE 754).
Поэтому выбор шага h должен быть сбалансированным : слишком большой шаг приводит к большой ошибке усечения, а слишком маленький — к росту ошибки округления. Обычно оптимальное значение h находится экспериментально или по эмпирическим правилам.

*Сравните центральную разность с прямой разностью по точности и вычислительным затратам.*

 Центральная разность обеспечивает более высокую точность по сравнению с прямой разностью, так как имеет второй порядок аппроксимации, в то время как прямая — только первый; это означает, что ошибка центральной разности уменьшается намного быстрее при уменьшении шага, но за это приходится платить большими вычислительными затратами и необходимостью иметь доступ к значениям функции как до, так и после текущей точки, тогда как прямая разность проще в реализации и работает на границах, но даёт менее точный результат.
"""

def f2_18():
    """Билет 18: Метод Ньютона, метод степеней, Рунге-Кутта."""
    return """# Билет 18

## Задание 1: Метод Ньютона для уравнения Ван-дер-Ваальса

Решить уравнение Ван-дер-Ваальса (P + a/V²)(V − b) = RT при P = 1, T = 300, R = 0.08314, a = 0.034, b = 0.018 методом Ньютона с точностью 1e-6. Задать начальное приближение и ответить на вопросы: Как выбор начального приближения влияет на сходимость? Сравните метод Ньютона с методом бисекции по скорости сходимости и требованиям к функции. Приведите пример функции, где метод Ньютона может не сойтись.
### Решение

```python
import numpy as np

# === Задание 1: Метод Ньютона ===

# Параметры уравнения
R = 0.08314
a = 0.034
b = 0.018
P = 1.0
T = 300.0

# Функция f(V)
def f(V):
    return (P + a / V**2) * (V - b) - R * T

# Производная функции f'(V)
def df(V):
    term1 = (P + a / V**2)
    term2 = (2 * a / V**3) * (V - b)
    return term1 - term2

# Метод Ньютона
def newton_method(f, df, x0, tol=1e-6, max_iter=100):
    x = x0
    for i in range(max_iter):
        fx = f(x)
        dfx = df(x)
        if abs(dfx) < 1e-10:
            print("Производная близка к нулю. Остановка.")
            break
        dx = -fx / dfx
        x += dx
        if abs(fx) < tol:
            print(f"Сошлось за {i+1} итераций")
            break
    return x

# Начальное приближение
V_initial = 2.0
V_solution = newton_method(f, df, V_initial)

print("=== Задание 1: Уравнение Ван-дер-Ваальса ===")
print(f"Найденный удельный объем V = {V_solution:.6f}")
```


**Теоретические вопросы**:

*Как выбор начального приближения влияет на сходимость?*

Метод Ньютона сильно зависит от начального приближения. Если оно близко к истинному корню, метод сходится очень быстро. Однако если начальное значение слишком далеко или находится вблизи точки, где производная равна нулю, метод может расходиться или сойтись к другому корню. Поэтому важно правильно выбирать начальное приближение.

*Сравните метод Ньютона с методом бисекции по скорости сходимости и требованиям к функции.*

Метод Ньютона сходится быстрее — с квадратичной скоростью, в то время как бисекция имеет линейную скорость сходимости. Но метод Ньютона требует вычисления производной и хорошего начального приближения. Метод бисекции работает медленнее, но всегда сходится, если функция меняет знак на интервале.

*Приведите пример функции, где метод Ньютона может не сойтись.*

Например, для функции f(x) = x³ метод Ньютона может зациклиться или сходиться очень медленно, если начальное приближение выбрано близко к точке, где производная равна нулю. Также возможны проблемы с функциями, имеющими разрывы или изломы, так как это нарушает условия сходимости.

---

## Задание 2: Метод степеней для собственного значения

Найти наибольшее собственное значение матрицы A = [[2,-1,0],[-1,2,-1],[0,-1,2]] методом степеней с начальным вектором [1,1,1] и точностью 1e-5. . Ответить на вопросы: Что такое спектральный радиус матрицы и как он связан с собственными значениями? Как зазор между собственными значениями влияет на сходимость метода степеней? Как круги Гершгорина помогают оценить собственные значения?

### Решение

```python
import numpy as np

# === Задание 2: Метод степеней ===

# Матрица A
A = np.array([
    [2, -1, 0],
    [-1, 2, -1],
    [0, -1, 2]
], dtype=float)

# Начальный вектор
x = np.array([1.0, 1.0, 1.0])

# Точность и максимальное число итераций
tolerance = 1e-5
max_iter = 100

# Функция для вычисления евклидовой нормы
def vector_norm(v):
    sum_squares = 0.0
    for i in range(len(v)):
        sum_squares += v[i] * v[i]
    return np.sqrt(sum_squares)

# Функция для матричного умножения A * x
def matrix_vector_multiply(A, x):
    n = A.shape[0]
    result = np.zeros(n)
    for i in range(n):
        for j in range(n):
            result[i] += A[i, j] * x[j]
    return result

# Метод степеней
eigenvalue = 0.0
for i in range(max_iter):
    # Умножение A * x
    x_new = matrix_vector_multiply(A, x)
    # Вычисление приближения собственного значения (норма x_new)
    eigenvalue_new = vector_norm(x_new)
    # Нормализация вектора
    x_new = x_new / eigenvalue_new
    # Проверка сходимости
    diff = np.zeros(len(x))
    for j in range(len(x)):
        diff[j] = x_new[j] - x[j]
    if vector_norm(diff) < tolerance:
        eigenvalue = eigenvalue_new
        print(f"Сошлось за {i+1} итераций")
        break
    x = x_new.copy()

print("=== Метод степеней ===")
print(f"Наибольшее собственное значение: {eigenvalue:.6f}")
```

**Теоретические вопросы**:

*Что такое спектральный радиус матрицы и как он связан с собственными значениями?*

Спектральный радиус матрицы — это наибольшее по модулю собственное значение. Оно определяет поведение итерационных процессов и играет важную роль в устойчивости численных методов. Чем больше спектральный радиус, тем больше влияние соответствующего собственного направления.

*Как зазор между собственными значениями влияет на сходимость?*

Чем больше разница между первым и вторым собственными значениями (спектральный зазор), тем быстрее сходится метод степеней. При маленьком зазоре сходимость замедляется, потому что второй собственный вектор начинает влиять на результат.

*Как круги Гершгорина помогают оценить собственные значения?*

Круги Гершгорина дают грубую оценку диапазона, в котором могут находиться собственные значения матрицы. Они строятся на диагональных элементах матрицы и позволяют предсказать, например, является ли матрица вырожденной или положительно определённой.

---

## Задание 3: Метод Рунге-Кутты для ОДУ

Решить ОДУ dy/dt = -y + t, y(0)=0 на интервале [0,1] с шагом 0.1 методом Рунге-Кутты 4-го порядка. Сравнить численное решение с точным решением y(t) = t − 1 + e⁻ᵗ. Ответить на вопросы: В чём разница между локальной и глобальной ошибками численных методов для ОДУ? Как порядок точности метода влияет на величину ошибки? Что понимается под устойчивостью численного метода решения ОДУ?

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# === Задание 3: Метод Рунге-Кутты 4-го порядка ===

# Правая часть ОДУ
def dydt(t, y):
    return -y + t

# Точное решение
def exact(t):
    return t - 1 + np.exp(-t)

# Метод Рунге-Кутты 4-го порядка
def rk4(dydt, y0, t):
    y = np.zeros_like(t)
    y[0] = y0
    for i in range(1, len(t)):
        h = t[i] - t[i-1]
        k1 = dydt(t[i-1], y[i-1])
        k2 = dydt(t[i-1] + h/2, y[i-1] + h*k1/2)
        k3 = dydt(t[i-1] + h/2, y[i-1] + h*k2/2)
        k4 = dydt(t[i-1] + h, y[i-1] + h*k3)
        y[i] = y[i-1] + h*(k1 + 2*k2 + 2*k3 + k4)/6
    return y

# Параметры
t_start = 0.0
t_end = 1.0
h = 0.1
t = np.arange(t_start, t_end + h, h)
y0 = 0.0

# Решение
y_rk = rk4(dydt, y0, t)
y_exact = exact(t)

# Вывод
print("=== Задание 3: Решение ОДУ методом Рунге-Кутты ===")
print(f"Точное значение в конце: {y_exact[-1]:.6f}")
print(f"Численное значение:     {y_rk[-1]:.6f}")

# График
plt.plot(t, y_rk, 'b-o', label='Решение Рунге-Кутты')
plt.plot(t, y_exact, 'r--', label='Точное решение')
plt.legend()
plt.grid(True)
plt.xlabel('t')
plt.ylabel('y(t)')
plt.title('Метод Рунге-Кутты vs Точное решение')
plt.show()
```

**Теоретические вопросы**:

*Сравните численное решение с точным решением.*

Численное решение методом Рунге-Кутты 4-го порядка хорошо совпадает с точным решением, особенно при малом шаге интегрирования. Разница между решениями минимальна и обусловлена погрешностью метода и округлением чисел с плавающей точкой.

*В чём разница между локальной и глобальной ошибками численных методов для ОДУ?*

Локальная ошибка — это погрешность, возникающая на одном шаге интегрирования. Глобальная ошибка — это суммарная погрешность за весь интервал. Обычно глобальная ошибка растёт с длиной интервала, но остаётся под контролем при достаточно малом шаге.

*Как порядок точности метода влияет на величину ошибки?*

 Порядок точности показывает, как быстро уменьшается ошибка при уменьшении шага. Например, метод Рунге-Кутты 4-го порядка имеет локальную ошибку пятого порядка, а глобальную — четвёртого. Это означает, что при уменьшении шага в два раза ошибка уменьшается в 16 раз.

*Что понимается под устойчивостью численного метода решения ОДУ?*

Устойчивость означает, что малые изменения в начальных данных или во входных параметрах не приводят к большим изменениям решения. Неустойчивые методы могут давать некорректные результаты даже при правильной реализации, особенно при решении жёстких систем уравнений.
"""

def f2_29():
    """Билет 29: Гаусс-Зейдель, наивное умножение матриц, ДПФ."""
    return """# Билет 29

## Задание 1: Метод Гаусса-Зейделя

В популяционной динамике для моделирования взаимодействия видов решается система: x^2 - y = 1 и x - y^2 = 0. Используя метод Гаусса-Зейделя с начальным приближением (x0,y0)=(1.5,1.5), решите систему. Сравните метод Гаусса-Зейделя с методом Ньютона по вычислительной сложности и устойчивости к ошибкам округления. В чём разница между абсолютной и относительной погрешностями? Как эти понятия связаны с ошибками округления в арифметике с плавающей точкой?

### Решение

```python
import numpy as np

# Задание 1: Решение системы методом Гаусса-Зейделя

# Начальное приближение (выбрано ближе к корню)
x0, y0 = 1.5, 1.5
# Точность
epsilon = 1e-6
# Максимальное количество итераций
max_iter = 1000
# Ограничение на максимальное значение переменных
max_value = 1e10

# Функция для выполнения одной итерации метода Гаусса-Зейделя
def gauss_seidel(x, y):
    # Вычисляем новое x из второго уравнения: x = y^2
    x_new = y**2
    # Проверяем переполнение
    if x_new > max_value:
        raise OverflowError("x_new превысило максимально допустимое значение")
    # Вычисляем новое y из первого уравнения: y = x_new^2 - 1
    y_new = x_new**2 - 1
    if abs(y_new) > max_value:
        raise OverflowError("y_new превысило максимально допустимое значение")
    return x_new, y_new

# Итерационный процесс
x, y = x0, y0
converged = False
for i in range(max_iter):
    try:
        x_new, y_new = gauss_seidel(x, y)
        print(f"Итерация {i+1}: x = {x_new:.6f}, y = {y_new:.6f}")
        # Проверка сходимости по максимальной разнице
        if max(abs(x_new - x), abs(y_new - y)) < epsilon:
            print(f"Решение найдено на итерации {i+1}: x = {x_new:.6f}, y = {y_new:.6f}")
            converged = True
            break
        x, y = x_new, y_new
    except OverflowError as e:
        print(f"Ошибка: {e}")
        break
else:
    if not converged:
        print("Метод не сошёлся за максимальное число итераций")

# Ответ для задания 1
if converged:
    print("Ответ: x ≈ {:.6f}, y ≈ {:.6f}".format(x, y))
else:
    print("Ответ: Метод разошёлся")
```

**Теоретические вопросы**:

*Сравните метод Гаусса-Зейделя с методом Ньютона по вычислительной сложности и устойчивости к ошибкам округления.*

Метод Гаусса-Зейделя проще в реализации, так как не требует вычисления производных. Он работает итерационно, обновляя переменные поочерёдно. Однако он может сходиться медленно или вообще не сойтись, особенно если система плохо обусловлена. Метод Ньютона сходится быстрее (квадратично), но требует вычисления матрицы Якоби и обращения к ней, что повышает вычислительную сложность. Также он более чувствителен к ошибкам округления, особенно при плохой начальной точке или вырожденной матрице Якоби.

*В чём разница между абсолютной и относительной погрешностями?*

Абсолютная погрешность — это просто разница между истинным и приближённым значением. Она показывает, насколько число отличается от правильного. Относительная погрешность — это отношение абсолютной погрешности к истинному значению. Она показывает, на сколько процентов ошибка влияет на результат, и полезна, когда значения сильно различаются по масштабу.

*Как они связаны с ошибками округления в арифметике с плавающей точкой?*

Числа в компьютере хранятся с ограниченной точностью (например, float64). При множестве операций ошибки округления накапливаются. Абсолютная погрешность помогает оценить точность в конкретном диапазоне, а относительная — позволяет сравнивать точность для чисел разного порядка. Эти метрики важны для выбора критерия остановки и оценки надёжности результата.

---

## Задание 2: Наивное умножение матриц

Реализуйте наивный алгоритм умножения двух матриц размером 4×4 с элементами, сгенерированными случайно (например, с помощью np.random.rand). Подсчитайте количество операций умножения. Объясните, как это количество изменилось бы при использовании алгоритма Штрассена. Объясните, как архитектура памяти влияет на производительность алгоритмов умножения матриц. Как суммирование по Кахану может улучшить точность?

### Решение

```python
import numpy as np

# === Задание: Наивное умножение матриц 4x4 ===

# Генерация случайных матриц 4x4
A = np.random.rand(4, 4)
B = np.random.rand(4, 4)

# Результирующая матрица
result = np.zeros((4, 4))

# Счётчик операций умножения
mult_count = 0

# === Наивное умножение через тройной цикл ===
for i in range(4):          # строка A
    for j in range(4):      # столбец B
        temp = 0.0
        for k in range(4):  # сумма произведений
            temp += A[i, k] * B[k, j]
            mult_count += 1  # каждое умножение считаем
        result[i, j] = temp

# === Вывод результатов ===
print("=== Задание: Умножение матриц ===")
print("Матрица A:")
print(A)
print("\nМатрица B:")
print(B)
print("\nРезультат умножения:")
print(result)
print(f"\nЧисло операций умножения: {mult_count}")
```

**Теоретические вопросы**:

*Как количество операций умножения изменилось бы при использовании алгоритма Штрассена?*

Наивный алгоритм для 4х4 требует n^3 = 64 умножений. Штрассен использует 7 умножений для 2х2 подматриц вместо 8, что дает ~49 умножений для 4х4 снижая число операций.

*Как архитектура памяти влияет на производительность алгоритмов умножения матриц?*

Наивный алгоритм имеет хорошую локальность данных, так как обращается к строкам и столбцам последовательно, минимизируя промахи кэша. Штрассен из-за рекурсии и работы с несмежными подматрицами увеличивает промахи кэша, что снижает производительность, несмотря на меньшее число умножений.

*Как суммирование по Кахану улучшает точность?*

Суммирование по Кахану компенсирует ошибки округления, сохраняя потерянные младшие биты в переменной коррекции c. Это снижает накопление ошибок при суммировании большого числа слагаемых, особенно если они различаются по порядку.

---

## Задание 3: Дискретное преобразование Фурье

Сгенерируйте сигнал длины 32: ... Вычислите дискретное преобразование Фурье (ДПФ) и найдите амплитуды компонент с частотами 2/16 и 3/16 . Постройте амплитудный спектр. Как частота дискретизации влияет на разрешение частотного спектра в ДПФ? Как шум может повлиять на точность выделения частот? Объясните, как быстрое преобразование Фурье (БПФ) уменьшает вычислительную сложность по сравнению с ДПФ. Как это связано с разбиением сигнала?

### Решение

```python
# --- ЗАДАНИЕ 3: ДИСКРЕТНОЕ ПРЕОБРАЗОВАНИЕ ФУРЬЕ ВРУЧНУЮ ---

import numpy as np
import matplotlib.pyplot as plt

# === Шаг 1: Генерация сигнала ===
N = 32
sr = N        # условная частота дискретизации
T = 1 / sr    # период дискретизации
n = np.arange(N)

# x[n] = cos(2π * 2n / 16) + 0.4*sin(2π * 3n / 16)
x = np.cos(2 * np.pi * 2 * n / 16) + 0.4 * np.sin(2 * np.pi * 3 * n / 16)

# === Шаг 2: Вычисление ДПФ вручную ===
X = np.zeros(N, dtype=complex)

for k in range(N):
    for m in range(N):
        real = np.cos(2 * np.pi * k * m / N)
        imag = -np.sin(2 * np.pi * k * m / N)
        X[k] += x[m] * (real + 1j * imag)

# === Шаг 3: Амплитудный спектр ===
amplitude = np.sqrt(np.real(X)**2 + np.imag(X)**2) / N

# === Шаг 4: Частотная шкала ===
freq = np.arange(N) * sr / N

# === Шаг 5: Вывод амплитуд на нужных частотах ===
print("=== Задание 3: Дискретное преобразование Фурье ===")
print(f"Амплитуда на частоте 2/16 = {amplitude[2]:.4f}")
print(f"Амплитуда на частоте 3/16 = {amplitude[3]:.4f}")

# === Шаг 6: Построение графика амплитудного спектра ===
plt.figure(figsize=(10, 5))
plt.stem(freq[:N//2], amplitude[:N//2])  # убран параметр use_line_collection
plt.title('Амплитудный спектр')
plt.xlabel('Частота (Гц)')
plt.ylabel('Амплитуда |X(f)|')
plt.grid(True)
plt.tight_layout()
plt.show()
```

**Теоретические вопросы**:

*Как частота дискретизации влияет на разрешение частотного спектра в ДПФ?*

 Частота дискретизации определяет, какие частоты можно представить в сигнале. Чем выше частота дискретизации, тем больше максимальная частота в спектре. Однако разрешение по частоте зависит от длины сигнала, а не только от частоты дискретизации. Чтобы улучшить разрешение, нужно брать более длинные сигналы.

*Как шум может повлиять на точность выделения частот?*

Шум добавляет случайные колебания в сигнал и может затушевать реальные гармоники. В результате пики на спектре становятся менее выраженными, а их частоты труднее определить. Для борьбы с этим применяются оконные функции и усреднение спектров.

*Объясните, как БПФ уменьшает вычислительную сложность по сравнению с ДПФ.*

ДПФ имеет сложность O(n^2) , то есть время вычисления растёт квадратично с длиной сигнала. БПФ уменьшает сложность до O(nlogn) за счёт разбиения задачи на подзадачи меньшего размера и повторного использования промежуточных результатов.

*Как это связано с разбиением сигнала?*

БПФ делит сигнал на четные и нечетные отсчеты, вычисляя их ДПФ отдельно. Результаты комбинируются с использованием симметрии комплексных экспонент, что сокращает число операций.
"""

def f2_30():
    """Билет 30: Метод Ньютона, наивное умножение, Рунге-Кутта."""
    return """# Билет 30

## Задание 1: Метод Ньютона для системы уравнений

Для системы уравнений: x^2 + y^2 = 2 и x * y = 1 Используйте метод Ньютона с начальным приближением (x0, y0) = (1.0, 1.0). Решите систему с точностью 1e-6.Как ошибка округления в арифметике с плавающей точкой может повлиять на сходимость метода Ньютона? Приведите пример функции, где влияние ошибок особенно заметно. Почему важно правильно выбирать начальное приближение?

### Решение

```python
import numpy as np

# Евклидова норма
def norm(v):
    return np.sqrt(v[0]**2 + v[1]**2)

# Решение системы Ax = b для матрицы 2x2
def solve_2x2(A, b):
    a, b_, c, d = A[0,0], A[0,1], A[1,0], A[1,1]
    det = a*d - b_*c
    if abs(det) < 1e-14:
        raise ValueError("Матрица вырожденная")
    inv_A = np.array([
        [d, -b_],
        [-c, a]
    ]) / det
    x = np.array([
        inv_A[0,0]*b[0] + inv_A[0,1]*b[1],
        inv_A[1,0]*b[0] + inv_A[1,1]*b[1]
    ])
    return x

# Система уравнений F(x, y)
def F(vars):
    x, y = vars
    return np.array([x**2 + y**2 - 2, x*y - 1])

# Якобиан
def Jacobian(vars):
    x, y = vars
    return np.array([[2*x, 2*y],
                     [y,   x]])

# Метод Ньютона
def newton_system(F, J, x0, tol=1e-6, max_iter=1000):
    x = np.array(x0, dtype=float)
    for i in range(max_iter):
        fx = F(x)
        if norm(fx) < tol:
            print(f"Сошлось за {i} итераций.")
            return x
        Jx = J(x)
        delta = solve_2x2(Jx, -fx)
        x += delta

# Запуск
solution = newton_system(F, Jacobian, (1.000001, 1.0))
print("Решение:")
print(f"x = {solution[0]:.15f}, y = {solution[1]:.15f}")
```

**Теоретические вопросы**:

*Как ошибка округления в арифметике с плавающей точкой может повлиять на сходимость метода Ньютона?*

Числа с плавающей точкой (float) имеют ограниченную точность, что приводит к ошибкам округления при вычислениях. В методе Ньютона эти ошибки могут накапливаться при вычислении матрицы Якобиана и её обратной. Если матрица плохо обусловлена или близка к вырожденной, даже маленькие ошибки могут сильно исказить результат решения линейной подсистемы, что замедляет или полностью останавливает сходимость метода.

*Приведите пример функции, где влияние ошибок особенно заметно.*

Например, система с экспоненциальной нелинейностью или близкими к нулю производными. Ошибки округления становятся критичными, когда элементы матрицы Якобиана очень маленькие или очень большие.

*Почему важно правильно выбирать начальное приближение?*

Метод Ньютона чувствителен к начальному приближению. Если оно слишком далеко от истинного корня, метод может расходиться или сойтись к другому корню. Особенно это критично, если система имеет несколько решений. Хорошее начальное приближение обеспечивает быструю и устойчивую сходимость.

---

## Задание 2: Наивное умножение матриц

Умножьте две матрицы размером 4×4: A = [[3,5,2,1],[6,4,1,5],[1,7,3,2],[3,2,5,4]] B = [[2,5,3,0],[3,7,4,1],[4,3,5,3],[4,2,3,3]] Используйте тройной цикл. Подсчитайте число операций умножения. Объясните, как бы изменилось число умножений при использовании алгоритма Штрассена. Как архитектура памяти влияет на производительность умножения матриц? Как суммирование по Кахану может улучшить точность? Чем отличаются QR-алгоритмы Хаусхолдера и Гивенса?

### Решение

```python
import numpy as np

# === Задание 2: Наивное умножение матриц ===

# Матрицы A и B
A = np.array([
    [3, 5, 2, 1],
    [6, 4, 1, 5],
    [1, 7, 3, 2],
    [3, 2, 5, 4]
])

B = np.array([
    [2, 5, 3, 0],
    [3, 7, 4, 1],
    [4, 3, 5, 3],
    [4, 2, 3, 3]
])

# Наивный алгоритм умножения
n = len(A)
C = np.zeros((n, n))
mult_count = 0

for i in range(n):
    for k in range(n):
        aik = A[i, k]
        for j in range(n):
            C[i, j] += aik * B[k, j]
            mult_count += 1

print("=== Задание 2: Результат умножения матриц ===")
print("Число операций умножения:", mult_count)
print("Результат:")
print(C)
```

**Теоретические вопросы**:

*Как изменилось бы число умножений при использовании алгоритма Штрассена?*

Наивный алгоритм для 4х4 требует n^3 = 64 умножений. Штрассен использует 7 умножений для 2х2 подматриц вместо 8, что дает ~49 умножений для 4х4 снижая число операций.

*Как архитектура памяти влияет на производительность умножения матриц?*

Наивный алгоритм эффективно использует кэш благодаря последовательному доступу к данным. Штрассен из-за рекурсии и работы с несмежными блоками вызывает больше промахов кэша, что снижает производительность.

*Как суммирование по Кахану может улучшить точность?*

Кахан компенсирует потерю точности при суммировании, сохраняя коррекцию c, что уменьшает накопление ошибок округления, особенно для больших или разнородных слагаемых.

*Чем отличаются QR-алгоритмы Хаусхолдера и Гивенса?*

- **Хаусхолдер**: Использует отражения для обнуления столбцов под диагональю. Эффективен для плотных матриц, требует O(n^3) операций.
- **Гивенс**: Применяет повороты для обнуления отдельных элементов. Лучше для разреженных матриц и параллельных вычислений, но медленнее из-за большего числа операций.

---

## Задание 3: Метод Рунге-Кутты для системы ОДУ
Моделируется система дифференциальных уравнений: dx/dt = y и dy/dt = -x с начальными условиями x(0)=1, y(0)=0 на интервале [0,10], шаг h=0.1. Используйте метод Рунге-Кутты 4-го порядка. Постройте фазовый портрет. Как фазовый портрет помогает анализировать динамические системы? Как численные методы могут искажать фазовый портрет из-за ошибок округления? Сравните явные и неявные методы по устойчивости.

### Решение

```python
import numpy as np
import matplotlib.pyplot as plt

# === Задание 3: Решение ОДУ методом Рунге-Кутты 4-го порядка ===

# Система ОДУ
def system(t, state):
    x, y = state
    return np.array([y, -x])

# Метод Рунге-Кутты 4-го порядка
def rk4(f, t_span, y0, h):
    t_values = [t_span[0]]
    y_values = [y0.copy()]

    t = t_span[0]
    y = y0.copy()

    while t < t_span[1]:
        k1 = h * f(t, y)
        k2 = h * f(t + h/2, y + k1/2)
        k3 = h * f(t + h/2, y + k2/2)
        k4 = h * f(t + h, y + k3)

        y += (k1 + 2*k2 + 2*k3 + k4) / 6
        t += h

        t_values.append(t)
        y_values.append(y.copy())

    return np.array(t_values), np.array(y_values)

# Параметры
t_start = 0
t_end = 10
h = 0.1
y0 = np.array([1.0, 0.0])

# Решение
t, states = rk4(system, [t_start, t_end], y0, h)
x_values = states[:, 0]
y_values = states[:, 1]

# Построение фазового портрета
plt.figure(figsize=(8, 6))
plt.plot(x_values, y_values)
plt.title('Фазовый портрет гармонического осциллятора')
plt.xlabel('x')
plt.ylabel('y')
plt.grid(True)
plt.show()

print("=== Задание 3: Результаты ===")
print("Последнее значение x:", x_values[-1])
print("Последнее значение y:", y_values[-1])
```

**Теоретические вопросы**:

*Как фазовый портрет помогает анализировать динамические системы?*

Фазовый портрет показывает, как одна переменная зависит от другой без привязки ко времени. Он позволяет наглядно увидеть тип движения системы — устойчивое, колебательное, периодическое или хаотичное. Например, в случае гармонического осциллятора он должен быть окружностью, что говорит о сохранении энергии и периодичности.

*Как численные методы могут искажать фазовый портрет из-за ошибок округления?*

Ошибки округления накапливаются, вызывая дрейф траектории. Для осциллятора явные методы (например, Эйлер) могут увеличивать или уменьшать радиус окружности, нарушая сохранение энергии. Рунге-Кутта 4 минимизирует это благодаря высокой точности.

*Сравните явные и неявные методы по устойчивости.*
Явные методы, такие как Эйлер или Рунге-Кутта, проще в реализации, но менее устойчивы — они могут расходиться при больших шагах или вблизи особенностей. Неявные методы, такие как неявный метод Эйлера, более устойчивы и подходят для жёстких систем, но требуют решения уравнений на каждом шаге, что усложняет вычисления.
"""
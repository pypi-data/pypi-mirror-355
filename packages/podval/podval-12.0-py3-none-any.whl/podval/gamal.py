import pyperclip as pc

def all_topics():
    """
01_nx_graph - Работа с графами в NetworkX
02_drawing - Визуализация графов в NetworkX
03_degree - Распределение степеней узлов
04_models - Модель предпочтительного присоединения. Модель тесного мира Уотса-Строгатса. Случайные сети.
05_community_detection - Задача выделения сообществ и метрики оценки качества. Алгоритм Гирвана-Ньюмена. Лувенский алгоритм.
06_lpa - Метод распространения меток (LPA)
07_random_walk_pagerank - Случайное блуждание. PageRank
08_rwlpa - Метод распространения меток на основе случайных блужданий (RW-LPA)
09_cuts - Разрез графа. Матрица Лапласа.
10_clustering - Спектральная кластеризация
11_gcnn_mpf - Введение в GCNN. Message passing framework
12_gnn_nodes - Предсказание атрибутов узлов с использованием графовых нейронных сетей
13_gnn_graphs - Предсказание атрибутов узлов с использованием графовых нейронных сетей
14_kg_translation_models - Графы знаний. Трансляционные модели
    """
    pass

def all_texts():
    """
---1_nx_graph---
    1. Напишите функцию, генерирующую кольцевой неориентированный граф из n узлов (n - параметр функции). Идентификатором узла выступают целые числа от 0 до n-1. Каждому узлу добавьте атрибут age, заполненный случайными целыми числами от 1 до 100. Каждому ребру добавьте атрибут mean_incident_age, состоящий из среднего значения атрибута age у узлов, инцидентных данному ребру.

Создайте граф с n=5 узлами. Выведите на экран словари, содержащие атрибут age для узлов (ключ - идентификатор узла) и mean_incident_age для ребер (ключ - пара (u, v) идентификаторов начала и конца ребра). Визуализируйте граф при помощи функции nx.draw (или nx.draw_networkx) с аргументами по умолчанию.

    2. Напишите функцию, генерирующую ориентированный граф из 3n + 1 узлов (n - параметр функции), где центральный узел соединен с тремя путями длины n. Ребра всегда направлены в сторону удаления от центрального узла.

Сгенерируйте граф для n=3. Выведите на экран количество узлов и ребер в графе. Для каждого узла выведите на экран список идентификаторов его соседей (отдельно по входящим и исходящим связям). Визуализируйте граф при помощи функции nx.draw (или nx.draw_networkx) с аргументами по умолчанию.

3. Напишите собственную функцию создания двумерной решетки n на m. В качестве идентификаторов узлов используйте пары (x, y), где x - номер строки решетки, y - номер столбца решетки. Сохраните ширину и высоту решетки в виде атрибутов n и m  самого графа.

Сгенерируйте граф с n=5, m=4. Выведите на экран количество узлов и ребер в графе. Выведите созданные атрибуты графа на экран. Визуализируйте граф при помощи функции nx.draw с аргументами по умолчанию. Создайте и визуализируйте аналогичный по структуре граф при помощи готовой функции из networkx.

4. Напишите функцию, которая генерирует мультиграф, состоящий из m узлов, где количество связей между узлами u и v сэмплится из биномиального распределения с параметрами n и p (m, n, p - параметры функции). Петли в графе допускаются. На каждом ребре создайте атрибут weight, равный обратному значению количества ребер между индицентными ему узлами. Создайте и визуализируйте граф при помощи функции nx.draw с аргументами по умолчанию. Создайте версию графа с удаленными петлями и визуализуйте ее.

5. Граф «(u,v) цветок»: фрактальный граф, в котором на каждой итерации роста каждая связь заменяется на 2 пути, длиной u и v соответственно (см.рис). Реализуйте функцию построения (1, 2) цветка. Функция принимает один целочисленный параметр n - количество шагов построения. Создайте и визуализируйте граф при помощи функции nx.draw с аргументами по умолчанию. Выведите на экран количество узлов и ребер графа на каждой из итераций.

6. Загрузите матрицу смежности графа из файла weighted_wheel.npy и создайте на ее основе граф. Выведите на экран количество узлов и связей графа. Выведите на экран тип графа (nx.Graph, nx.DiGraph, ...). Выведите на экран словарь со значениями атрибута weight на ребрах.Сохраните граф на диск в формате edgelist.

7. Загрузите информацию о ребрах графа из файла data/karate.edgelist и создайте на ее основе граф при помощи функции read_edgelist. При создании графа приведите идентификаторы узлов к числовому типу. Выведите на экран количество узлов и ребер загруженного графа, выведите на экран список узлов графа. Загрузите граф карате-клуба при помощи функции nx.karate_club_graph. Сравните две матрицы смежности (загруженного из файла графа и графа, созданного функцией) и покажите, что они одинаковые. При создании матриц смежности учтите, что порядок следования узлов в графах может различаться.

---2_drawing---
1. Создайте двумерную решетку размера 5х3, используя готовую функцию из networkx. Создайте словарь pos, в котором ключами являются идентификаторы узлов, а значениями - координаты этих узлов на плоскости. Для получения координат узлов считайте, что идентификатор состоит из пар (x, y), где x - позиция по горизонтали, а y - позиция по вертикали. Визуализируйте граф, используя полученный словарь. Добавьте на рисунок подписи узлов.

2. Создайте граф карате-клуба, используя готовую функцию из networkx. Используя несколько функций вида x_layout, рассчитайте координаты узлов и визуализируйте все доступные в networkx различные укладки в виде сетки из изображений. Каждому изображению в сетке добавьте заголовок в виде названия используемой функции. Если какая-то функция укладки не применима к данному графу, игнорируйте ее.

3. Создайте граф карате-клуба, используя готовую функцию из networkx. Добавьте узлам атрибут age, заполненный целыми числами от 20 до 50. Визуализируйте граф, раскрасив узлы в цвет клуба (красный для "Mr. Hi" и зеленый для "Officer") и сделав размер узла пропорциональным величине атрибута age.

4. Создайте ориентированный граф "путь" из 3 узлов. Визуализируйте граф со стрелками увеличенного размера. Визуализируйте граф, развернув стрелки на визуализации в обратном направлении (сам граф оставьте без изменений). На всех визуализациях добавляйте подписи узлов.

5. Создайте граф Les Miserables. Для каждого узла зафиксируйте координату на плоскости при помощи функции random_layout. Получите список edge_colors значений атрибута weight на ребрах и ограничьте (clip) его сверху значением 10, снизу значением 1.
Отрисуйте узлы графа при помощи функции draw_networkx_nodes с указанием размера узлов 10. На тот рисунок добавьте ребра графа при помощи функции draw_networkx_edges, раскрасив ребра в цвет, соответствующий значениям edge_colors с применением цветовой карты viridis. Добавьте на визуализацию colorbar, показывающий градиент цветов ребер.

---3_degree---
1. Создайте неориентированный граф на основе набора данных fb-pages-food с сайта https://networkrepository.com/. Добавьте узлам атрибут name, который содержит имя человека. Удалите из графа все петли. Получите матрицу смежности графа и посчитайте степень каждого узла на основе этой матрицы. Выведите имя человека, чей узел имеет максимальную степень, и само значение степени.

2. Используя граф из предыдущего задания, посчитайте степени каждого узла, используя возможности графа nx.Graph. Посчитайте и выведите на экран среднюю степень узлов графа, округленную до ближайшего целого.

3. Создайте ориентированный граф на основе набора данных fb-pages-food с сайта https://networkrepository.com/. Удалите из графа все петли. Получите матрицу смежности графа и посчитайте входящую и исходящую степень каждого узла на основе этой матрицы. Найдите и выведите на экран максимальное значение исходящих степеней и среднее значение входящих степеней.

4. Постройте графики выборочной функции распределения входящих и исходящих степеней узлов для загруженного ориентированного графа. Для нахождения степеней узлов воспользуйтесь возможностями графа nx.DiGraph. Добавьте графикам названия, а осям - подписи. Расположите графики рядом по горизонтали.
    
5. Визуализируйте закон распределения степеней узлов для неориентированного варианта графа в обычной и логарифимических (по 1й и по 2м осям) системах координат для неориентированного графа. Расположите графики в виде сетки 2х2. Добавьте графикам названия, а осям - подписи. Использование готовых функций для создания и визуализации гистограмм не допускается.

6. Сгенерируйте выборку из Дзета-распределения (распределение Ципфа) с числом наблюдений 10 тыс. и параметром распределения  𝛼=2
 . Визуализируйте эмпирический закон распределения с использованием линейного и логарифмического биннинга. Визуализируйте оба варианта в линейных и логарифмических (log-log) координатах (должно получиться 4 изображения).
 
---4_models---
1. Используя реализацию модели предпочтительного присоединения Барабаши-Альберта из networkx, постройте сеть с кол-вом узлов n > 50. Визуализируйте полученный граф, сделайте размер и цвет узлов пропорциональными их степени. Постройте график выборочной функции распределения степеней узлов для сети.

2. Используя реализацию модели предпочтительного присоединения Барабаши-Альберта из networkx, постройте сеть с кол-вом узлов n=10000. Визуализируйте закон распределения степеней узлов сети в обычной и логарифимических (по 1й и по 2м осям) системах координат.

3. Визуализируйте закон распределения степеней узлов с использованием log-binning для сети из предыдущего задания. Для сравнения на этом же графике изобразите график закона распределения без использования log-binning в log-log координатах.

4. C помощью реализации модели Уотса-Строгатса из networkx сгенерируйте сети с кол-вом узлов порядка 1000 и с разным p. В качестве p рассмотрите несколько значений, равномерно распределенных по логарифмической шкале на отрезке  [10−4,1].
Визуализируйте результаты пересвязывания, построив на одном графике относительные изменения среднего коэффициента кластеризации и средней длины пути относительно варианта сети без пересвязывания. Обозначьте точки разных графиков различными маркерами и цветами. Добавьте легенду.

5. Создайте неориентированный граф на основе набора данных fb-pages-food с сайта https://networkrepository.com/ . C помощью nx.configuration_model постройте рандомизированный аналог данной сети. Визуализируйте исходный и рандомизированный граф рядом. Проверьте, совпадают ли распределения степеней узлов исходного и рандомизированного графа.

6. C помощью реализации алгоритма постоения случайного геометрического графа из networkx сгенерируйте сеть с кол-вом узлов порядка 500 при радиусе r=0.2 и  𝐿2  метрикой для вычисления расстояния. Визуализируйте полученный граф, уменьшив размер узлов и сделав цвет ребер черным прозрачным (используйте RGBA). Визуализируйте распределение степеней узлов полученного графа.
Повторите решение задачи, использовав  𝐿1

---5_community_detection---
1. Загрузите сеть Southern women с сайта http://konect.cc/, удалите из него петли и создайте не менее 5 разбиений этой сети на 2 сообщества (часть разбиений должны частично или полностью соответствовать интуитивным представлениям о разбиении на сообщества, часть - нет). Для создания разбиений не используйте специализированные алгоритмы. Визуализируйте графы, раскрасив узлы в цвет соответствующего им сообщества.

2. Для разбиений из предыдущей задачи посчитайте следующие величины:

плотность первого сообщества
плотность второго сообщества
среднюю плотность сообществ.
модулярность
покрытие (coverage)
эффективность (performance)
Все значения округлите до сотых и сведите в таблицу следующего вида:

Разбиение	Плотность первого сообщества	Плотность второго сообщества	...	...
1				
...				
Для представления данных в табличном виде используйте pandas.

3. Считайте сеть из файла communities.edgelist. Выполните разбиение сети на 2 сообщества с помощью алгоритма Гирвана-Ньюмена. Визуализируйте результат, раскрасив узлы в цвет, соответствующий их сообществу.

4. Постройте график динамики модулярности для шагов алгоритма Гирвана-Ньюмена. Визуализируйте разбиение сети из предыдущей задачи, при котором достигается наилучшее значение модулярности (выведите это значение на экран). Для визуализации выберите несколько цветов (в соответствии с количеством выделенных сообществ) и раскрасьте узлы и связи в рамках одного сообщества этими цветами. Связи между узлами, состоящими в разных сообществах, отрисуйте черным цветом.

5. Выполните разбиение сети из первого задания на сообщества с помощью Лувенского алгоритма. Визуализируйте результат аналогично предыдущему заданию. 

6. Постройте матрицу смежности для сети. Постройте матрицу смежности для сети, в которой узлы перенумерованы в порядке возрастания номера сообщества, которому они принадлежат (т.е. номера  0...𝐶1−1
  даются  𝐶1
  узлам, принадлежащим первому сообществу; номера  𝐶1...𝐶2−1
  даются  𝐶2
  узлам, принадлежащим второму сообществу и т.д.). Для разбиения графа на сообщества воспользуйтесь Лувенским алгоритмом.

Визуализируйте две матрицы смежности при помощи seaborn.heatmap. Расположите рисунки рядом по горизонтали, добавьте названия рисунков. Сравните полученные изображения и сделайте выводы.

---6_lpa---
1. Загрузите граф карате-клуба и выделите в нем сообщества с помощью реализации алгоритма распространения меток из networkx. Визуализируйте полученный результат. Для визуализации выберите несколько цветов (в соответствии с количеством выделенных сообществ) и раскрасьте узлы и связи в рамках одного сообщества этими цветами. Связи между узлами, состоящими в разных сообществах, отрисуйте черным цветом.

2. Реализуйте синхронный алгоритм распространения меток. В синхронном варианте алгоритма для выбора новой метки узла используются метки соседей с предыдущей итерации. Если среди меток соседей есть несколько вариантов с одинаковой максимальной частотой, то метка выбирается случайным образом. Алгоритм прекращает работу, когда на очередной итерации не была изменена метка ни одного узла. Визуализируйте пошаговую динамику распространения меток и итоговое разбиение сети аналогично предыдущей задаче.

3. Реализуйте асинхронный алгоритм распространения меток. В асинхронном варианте алгоритма для выбора новой метки узла используются в том числе метки соседей с текущей итерации алгоритма. Визуализируйте пошаговую динамику распространения меток и итоговое разбиение сети аналогично предыдущей задаче.

4. Используя собственную реализацию LPA, разбейте граф карате-клуба на сообщества N=10 раз. Создайте двумерный массив freqs размера MxM (M - количество узлов в графе), где freqs[i, j] показывает, как часто узлы i и j оказывались в одном сообществе.
Создайте граф, в котором между узлами i и j существует связь только тогда, когда freqs[i, j] >= 8. Получите компоненты связности данного графа. Интерпретируя данные компоненты связности как сообщества в исходном графе, визуализируйте полученное разбиение карате-клуба (оригинального графа) аналогично предыдущим заданиям.

5. Оформите результаты работы алгоритмов в виде таблицы
Алгоритм	Средняя плотность сообществ	Модулярность	Покрытие	Эффективность
Синхронный LPA				
Асинхронный LPA				
Множественный LPA				
Для представления данных в табличном виде используйте pandas. Все расчеты метрик при решении этой задачи выполните повторно.

---7-rand_walk---
1\. Загрузите граф карате-клуба. Получите матрицу смежности `A` этого графа. Получите на ее основе матрицу переходов `P` по следующему правилу:
$$\mathbf{P}=\mathbf{D}^{-1}\mathbf{A}$$
Продемонстрируйте, что выполняются условия (1) и (2).
$0 \le p_{ij} \le 1$ (1)
$\sum_j p_{ij}=1$    (2)
Все действия проводите с невзвешенной матрицей смежности.

2\. Создайте вектор начального состояния $\mathbf{p}^0 = [0, ..., 1]^T$. Получите стационарное состояние $\mathbf{p}^\infty$, используя итеративную процедуру
$\mathbf{p}^{t+1}=(\mathbf{P}^{\top})\mathbf{p}^t$ 
Процесс заканчивается, когда $||\mathbf{p}^{t+1} - \mathbf{p}^{t}|| < \epsilon $
Выведите полученный вектор стационарного состояния на экран.
3\. Найдите матрицу перехода к стационарному состоянию $(\mathbf{P}^{\top})^\infty$ при помощи процедуры возведения матрицы в степень.
Докажите, что полученная матрица является матрицей стационарного состояния, т.е. $||(\mathbf{P}^{\top})^{\infty}  -(\mathbf{P}^{\top})(\mathbf{P}^{\top})^{\infty}|| <= \epsilon$
Cоздайте вектор начального состояния $\mathbf{p}^0 = [0, ..., 1]^T $. Получите стационарное состояние $\mathbf{p}^\infty$, воспользовавшись полученной матрицей $(\mathbf{P}^{\top})^\infty$. Решите задачу двумя способами: при помощи матричного умножения и при помощи оператора индексации.
Используя функцию `np.allclose`, покажите, что векторы стационарных состояний, полученные двумя разными методами, совпадают (с точностью до тысячных).

4. Загрузите граф карате-клуба. Вычислите центральность каждого узла в сети. Визуализируйте граф, отмасшабировав размер каждого узла пропорционально полученным значениям. Постройте несколько визуализаций графа в виде сетки, используя следующие меры центральности:
центральность по степени;
центральность по посредничеству;
центральность по близости;
центральность по собственному вектору;
центральность по PageRank.
На каждом рисунке сделайте размеры узлов пропорционально соответствующей мере центральности. Сравните результаты и сделайте выводы.

5\. Реализуйте алгоритм PageRank с параметром затухания $\alpha=0.9$.
Загрузите граф карате-клуба и сделайте его ориентированным. Найдите стохастическую матрицу переходов $\mathbf{P}$.
$$\mathbf{P}=\mathbf{D}^{-1}\mathbf{A}$$
Модифицируйте матрицу $\mathbf{P}$:
$$\mathbf{P}' = \alpha \mathbf{P} + (1-\alpha)\frac{1}{n}\mathbf{E}$$
$$\mathbf{M}=(\mathbf{P}')^{\top}$$
где $\mathbf{E}$ - матрица размера `NxN`, состоящая из единиц.
Получите вектор $\mathbf{p}^\infty = \mathbf{pr}^{iter}$, используя итеративную процедуру
$$\mathbf{p}^{t+1}=\mathbf{M}\mathbf{p}^t$$
Процесс заканчивается, когда $||\mathbf{p}^{t+1} - \mathbf{p}^{t}|| < \epsilon $
Выведите полученный вектор стационарного состояния на экран.
Вызовите `nx.pagerank` с соответствующими параметрами и докажите, что полученные векторы совпадают.

6\. Вычислите коэффициенты PageRank при помощи задачи нахождения собственного вектора матрицы $\mathbf{M}$ $\mathbf{pr}^{eig}$, соответствующего собственному числу 1.
Покажите, что $\mathbf{pr}^{eig}$ и $\mathbf{pr}^{iter}$ оба являются с.в. матрицы $\mathbf{M}$.

---8_rwlpa---
1. Загрузите граф Karate Club. Получите значения атрибута club, хранящегося на узлах. Выберите случайным образом 4 узла, относящихся к клубу Mr. Hi и 4 узла, относящихся к клубу Officer. Получите матрицу смежности, в которой узлы перенумерованы таким образом, чтобы строки и столбцы с номерами 0, 1, ... 7 принадлежали узлам, которые были выбраны на предыдущем шаге.

2. Получите блочную матрицу  𝐏
  и матрицу начального состояния меток  𝐘0∈{0,1}𝑁×2
𝐏=(𝐏𝑙𝑙𝐏𝑢𝑙𝐏𝑙𝑢𝐏𝑢𝑢)=(𝐈𝐏𝑢𝑙0𝐏𝑢𝑢)
𝐘0=(𝐘𝐥𝐘𝐮)=(𝐘𝐥0)
Выведите на экран след матрицы  𝐏
 . Выведите на экран количество ненулевых элементов матрицы  𝐘0
# Процесс случайного блуждания с ловушками, попав в них при блуждании мы остаемся в них во время всех последующих итераций
# состояние узлов-ловушек всегда остается неизменным
# траектории случайных блажданий из узлов, относящихся к u, будут завершаться на
# изначально помеченных узлах

3. Известно, что для блочной матрицы справедливо:
𝐏∞=(𝐈(𝐈−𝐏𝑢𝑢)−1𝐏𝑢𝑙00)
Получите оценку  𝐏∞
  путем возведения матрицы в достаточно большую степень. Свяжите блоки матрицы с отдельными переменными P_ll, P_lu, P_ul, P_uu и продемонстрируйте, что каждый блок полученной матрицы удовлетворяет формуле выше при помощи функции np.allclose.
  
4. Используя Базовый вариант RW-LPA , расставьте метки для всех узлов. Визуализируйте сеть, показав цветом контура вокруг узла принадлежность узла к одному из сообществ, а цветом цветом заливки узла - сообщество, к которому узел был отнесен алгоритмом.

---9_cuts---
1. Задан граф G. Определите два разреза:

𝐶(1)=(𝑁(1)1,𝑁(1)2)
 ,  𝑁(1)1={0,1,2,3,4}
 ,  𝑁(1)2={5,6,7,8,9}
 
𝐶(2)=(𝑁(2)1,𝑁(2)2)
 ,  𝑁(2)1={0,1,2,3,4,5,6,7,8}
 ,  𝑁(2)2={9}
 
Для каждого из разрезов вычислите величину  𝑄(𝑘)
  двумя способами: явным образом просмотрев все ребра и при помощи функции из пакета networkx. Выведите величины разрезов на экран.

𝑄(𝑘)=𝑐𝑢𝑡(𝑁(𝑘)1,𝑁(𝑘)2)=∑𝑖∈𝑁(𝑘)1,𝑗∈𝑁(𝑘)2𝑙𝑖𝑗

2. Для разрезов из предыдущего задания вычислите
𝑉𝑜𝑙(𝑁(𝑘)𝑡)=∑𝑖∈𝑁(𝑘)𝑡,𝑗∈𝑁𝑙𝑖𝑗=∑𝑖∈𝑁(𝑘)𝑡𝑘𝑖,𝑡=1,2
𝑄(𝑘)𝑛𝑜𝑟𝑚𝑎𝑙𝑖𝑧𝑒𝑑=𝑐𝑢𝑡(𝑁(𝑘)1,𝑁(𝑘)2)𝑉𝑜𝑙(𝑁(𝑘)1)+𝑐𝑢𝑡(𝑁(𝑘)1,𝑁(𝑘)2)𝑉𝑜𝑙(𝑁(𝑘)2)
и
𝑄(𝑘)𝑞𝑢𝑜𝑡𝑖𝑒𝑛𝑡=𝑐𝑢𝑡(𝑁(𝑘)1,𝑁(𝑘)2)min(𝑉𝑜𝑙(𝑁(𝑘)1),𝑉𝑜𝑙(𝑁(𝑘)2))
Для каждого разреза выведите четыре величины  𝑉𝑜𝑙(𝑁(𝑘)1)
 ,  𝑉𝑜𝑙(𝑁(𝑘)2)
 ,  𝑄(𝑘)𝑛𝑜𝑟𝑚𝑎𝑙𝑖𝑧𝑒𝑑
  и  𝑄(𝑘)𝑞𝑢𝑜𝑡𝑖𝑒𝑛𝑡
  на экран.
  
3. Для графа G из задания 1 вычислите матрицу Лапласа двумя способами:
по формуле  𝐋=𝐃−𝐀;
используя готовую функцию из networkx.
Выведите полученные матрицы на экран и покажите, что они равны.

4. Для каждого из разрезов первого задания вычислите величины  𝑄(𝑘)
 , воспользовавшись матрицей Лапласа.
 
5. Найдите собственные значения и собственные векторы матрицы Лапласа. Выведите на экран кратность нулевого собственного значения матрицы Лапласа. Найдите и выведите на экран количество компонент связности в графе G.
Создайте копию графа G с удаленными ребрами в соответствии с разрезом 𝐶(1)
. Повторите решение задачи для получившегося несвязного графа.
 Проверено на семинаре
Лемма: для неориентированной сети с неотрицательными весами кратность нулевого собственного значения матрицы лапласа равна количеству связных компонент сети , … , . Собственные пространство с.з. 0, над собственными векторами, отвечающимим с.з. 0 является оболочкой над индикаторными векторами этих компонент.

---10_cluster---
1. Загрузите граф карате клуба. Вычислите матрицу Лапласа  𝐋=𝐃−𝐀
  при помощи готовой функции. Найдите собственные значения и собственные векторы матрицы Лапласа. При использовании функции явно укажите, что используется невзвешенная матрица смежности.
Выделите собственный вектор  𝐮
 , отвечающий минимальному ненулевому с.з.  𝜆𝑖≠0
  и выведите его на экран. Преобразуйте  𝐮
  в индикаторный вектор  𝐟
  по правилу  𝐟𝑖
  = sign( 𝐮𝑖)
 .
Визуализируйте  𝐮
 : по горизонтали откладывается номер узла, по вертикали значение соответствующей координаты вектора  𝐮
Визуализируйте граф, обозначив цветом узла компоненту, в которую попадает узел в соответствии с найденным разрезом.

2. Повторите предыдущую задачу, используя нормализованную матрицу Лапласа  𝐋𝑠𝑦𝑚=𝐃−12𝐋𝐃−12
 . При расчете матрицы Лапласа явно укажите, что используется взвешенная матрица смежности  𝐖
 
3. Дан набор данных (X, y). Визуализируйте набор данных, отрисовав точки на плоскости и раскрасив в цвета, соответствующие меткам объектов y. Решите задачу кластеризации данных на 3 кластера при помощи алгоритма KMeans из sklearn. Визуализируйте полученный результат.

4. На основе датасета из предыдущего задания создайте граф при помощи функции radius_neighbors_graph из sklearn (укажите аргумент radius=1). Получите матрицу  𝑈∈ℝ300×3
 , состояющую из собственных векторов матрицы Лапласа, соответствующих трем наименьшим ненулевым собственным значениям. Решите задачу кластеризации при помощи алгоритма KMeans из sklearn на основе матрицы  𝑈
 . Визуализируйте полученный результат.
 
5. Решите задачу кластеризации данных на 3 кластера при помощи алгоритма SpectralClustering из sklearn. Визуализируйте полученный результат. При создании модели кластеризации укажите значение аргумента affinity='nearest_neighbors'.

---11_gcnn_mpf---
1. Загрузите граф из файла karate.graphml. Визуализируйте граф, раскрасив в цвета классов (атрибут label у узлов).
Получите невзвешенную матрицу смежности  𝐀
  в виде тензора torch и тензор меток для каждого узла. Выведите полученные тензоры на экран.
  
2. Найдите стохастическую матрицу  𝐀̂.
𝐀̂ =𝐃−1𝐀
Сгенерируйте тензор  𝐗
  размера Nx5 с признаками узлов, используя torch.rand. Выполните один шаг рассылки сообщений при помощи матричного умножения.
𝐗̂ =𝐀̂ 𝐗
Выполните этот же шаг рассылки сообщений, явно проитерировавшись по узлам графа и усреднив атрибуты соседей. Сравните два полученных тензора и покажите, что они равны.

3. Опишите слой GCNLayer графовой сверточной нейронной сети. Создайте слой с n_outputs=3 и пропустите через этот слой матрицу смежности графа и тензор признаков  𝐗=𝐄
 . Выведите форму полученного тензора на экран.
 
4. Используя графовую сверточную нейронную сеть, решите задачу классификации узлов. Создайте модель, состоящую из двух последовательно идущих слоев GCNLayer. Первый слой имеет 2 нейрона и гиперболический тангенс в качестве функции активации. Количество нейронов во втором слое определяется количеством классов в задаче. Настройте модель (в качестве признаков узлов используйте единичную матрицу  𝐗=𝐄).
После завершения процесса обучения пропустите данные через первый слой и визуализируйте граф, используя полученные векторные представления в качестве координат. Раскрасьте узлы в цвет соответствующих классов.

5. Повторите решение предыдущей задачи, используя в качестве признаков  𝐗
  следующие характеристики узлов (каждый узел описывается четырьмя характеристиками):
количество треугольников, в которые входит узел;
степень узла;
эксцентриситет узла;
центральность по собственному вектору для узла.
Для ускорения процесса обучения стандартизуйте тензор признаков.

---12_gnn_nodes---
1. Загрузите граф Planetoid/Cora из torch_geometric.datasets.
Выведите на экран:
количество узлов графа;
количество ребер графа;
размерность признаков узлов;
количество узлов для обучения, валидации и тестирования.
Решите задачу классификации узлов графа, используя только полносвязные слои torch.nn.Linear (создайте модель из двух слоев). Для обучения используйте пакетный градиентный спуск (не разбивайте на батчи). Обратите внимание, что настройка весов модели должна проводиться только на основе примеров из обучающей выборки. Посчитайте и выведите на экран значение accuracy на тестовой выборке.

2. Решите задачу 1, используя два слоя torch_geometric.nn.GCNConv вместо полносвязных слоев torch.nn.Linear.

3. Воспользовавшись необученной моделью из предыдущего задания, получите прогнозы для всех узлов графа. Уменьшите размерность полученных прогнозов до 2 при помощи алгоритма t-SNE (sklearn.manifold.TSNE). Визуализируйте точки на плоскости, используя полученные значения в качестве координат. Раскрасьте точки в цвета, соответствующим меткам узлов.
Повторите данную процедуру, используя обученную модель. Сравните результаты и сделайте выводы.

4. Предыдущие решения не используют узлы, находящиеся в валидационном множестве. Решите задачу 2, используя валидационное множество для выполнения ранней остановки.

5. Повторите решение задачи 4, сравнив несколько различных слоев:
GCNConv
SAGEConv (укажите aggr="mean")
GATConv (выберите для первого слоя heads > 1)
Выведите результат в виде таблицы:
Модель	Loss на обучении	Acc на обучении	Acc на тесте	Кол-во эпох до ранней остановк

---13_gnn_graphs---
1. Реализуйте все описанные методы класса GraphsDataset. Используя данный класс, создайте датасет на основе файлов архива graphs.zip (можно разархивировать вручную или программно). Выведите на экран количество объектов в датасете. Выведите на экран значения признаков узлов для графа с индексом 0.

2. Используя датасет из предыдущего задания, создайте объект torch_geometric.loader.DataLodaer. Получите один батч размера 128 при помощи этого объекта и выведите на экран (используйте соответствующие атрибуты и методы):
количество узлов в графе-батче;
количество связей в графе-батче;
количество графов в батче;
количество узлов в каждом графе батча;
Выполните readout для графа на основе атрибута x. Выведите размерность полученного тензора на экран.

3. Решите задачу классификации графа, используя слои SAGEConv и операцию усреднения для процедуры readout. Для обучения используйте стохастический градиентный спуск с размером батча 128. Во время обучения выводите значение функции потерь по эпохам (используйте torchmetrics). Вычислите матрицу несоответствий прогнозов и точность обученной модели (используйте torchmetrics).

4. Повторите решение задачи 3, сравнив разные функции агрегации для проведения операции readout. Выведите результаты в виде таблицы:
Выведите результат в виде таблицы:
Readout op	Loss	Acc
sum		
mean		
max		
min		

---14_kg_trans---
1. Загрузите датасет CoDExSmall. Выведите на экран (обратитесь к соответстующим полям) количество сущностей в обучающем (<dataset>.training), валидационном (<dataset>.validation) и тестовом (<dataset>.testing) множестве.
Выведите на экран одну произвольную тройку  (ℎ,𝑟,𝑡)
  в оригинальном виде (<dataset>.triples) и эту же тройку, но с использованием числового представления сущностей/отношений (<dataset>.mapped_triples).
  
2. Обучите модель TransE на датасете CoDExSmall при помощи pipeline. Задайте количество эпох обучения (num_epochs в training_kwargs), равное 100, и размерность эмбеддингов (embedding_dim в  model_kwargs), равную 64.
Изобразите график значений функции потерь в зависимости от номера эпохи (<pipeline_result>.plot_losses).
Выведите на экран значение метрик в виде pd.DataFrame (<pipeline_result>.metric_results.to_df())

3. Выберите случайным образом одну тройку  (ℎ,𝑟,𝑡∗)
  из тестового множества. Используя обученную модель, рассчитайте оценки для всех троек  (ℎ,𝑟,𝑡),𝑡∈𝐾.
Найдите позицию (ранг) истинного значения объекта тройки  𝑡∗
  в списке прогнозов, отсортированном в порядке убывания величины score.
  
4. Обучите модели TransE, TransH и TransD на датасете CoDExSmall.
Сравните качество полученных моделей по следующим метрикам (side=both и type=realistic):
Adjusted Arithmetic Mean Rank (AAMR): adjusted_arithmetic_mean_rank
Mean Reciprocal Rank (MRR): inverse_harmonic_mean_rank
Hits @ 10: hits_at_10
Визуализируйте результаты в виде столбчатой диаграммы
    """
    text = """
---1_nx_graph---
    1. Напишите функцию, генерирующую кольцевой неориентированный граф из n узлов (n - параметр функции). Идентификатором узла выступают целые числа от 0 до n-1. Каждому узлу добавьте атрибут age, заполненный случайными целыми числами от 1 до 100. Каждому ребру добавьте атрибут mean_incident_age, состоящий из среднего значения атрибута age у узлов, инцидентных данному ребру.

Создайте граф с n=5 узлами. Выведите на экран словари, содержащие атрибут age для узлов (ключ - идентификатор узла) и mean_incident_age для ребер (ключ - пара (u, v) идентификаторов начала и конца ребра). Визуализируйте граф при помощи функции nx.draw (или nx.draw_networkx) с аргументами по умолчанию.

    2. Напишите функцию, генерирующую ориентированный граф из 3n + 1 узлов (n - параметр функции), где центральный узел соединен с тремя путями длины n. Ребра всегда направлены в сторону удаления от центрального узла.

Сгенерируйте граф для n=3. Выведите на экран количество узлов и ребер в графе. Для каждого узла выведите на экран список идентификаторов его соседей (отдельно по входящим и исходящим связям). Визуализируйте граф при помощи функции nx.draw (или nx.draw_networkx) с аргументами по умолчанию.

3. Напишите собственную функцию создания двумерной решетки n на m. В качестве идентификаторов узлов используйте пары (x, y), где x - номер строки решетки, y - номер столбца решетки. Сохраните ширину и высоту решетки в виде атрибутов n и m  самого графа.

Сгенерируйте граф с n=5, m=4. Выведите на экран количество узлов и ребер в графе. Выведите созданные атрибуты графа на экран. Визуализируйте граф при помощи функции nx.draw с аргументами по умолчанию. Создайте и визуализируйте аналогичный по структуре граф при помощи готовой функции из networkx.

4. Напишите функцию, которая генерирует мультиграф, состоящий из m узлов, где количество связей между узлами u и v сэмплится из биномиального распределения с параметрами n и p (m, n, p - параметры функции). Петли в графе допускаются. На каждом ребре создайте атрибут weight, равный обратному значению количества ребер между индицентными ему узлами. Создайте и визуализируйте граф при помощи функции nx.draw с аргументами по умолчанию. Создайте версию графа с удаленными петлями и визуализуйте ее.

5. Граф «(u,v) цветок»: фрактальный граф, в котором на каждой итерации роста каждая связь заменяется на 2 пути, длиной u и v соответственно (см.рис). Реализуйте функцию построения (1, 2) цветка. Функция принимает один целочисленный параметр n - количество шагов построения. Создайте и визуализируйте граф при помощи функции nx.draw с аргументами по умолчанию. Выведите на экран количество узлов и ребер графа на каждой из итераций.

6. Загрузите матрицу смежности графа из файла weighted_wheel.npy и создайте на ее основе граф. Выведите на экран количество узлов и связей графа. Выведите на экран тип графа (nx.Graph, nx.DiGraph, ...). Выведите на экран словарь со значениями атрибута weight на ребрах.Сохраните граф на диск в формате edgelist.

7. Загрузите информацию о ребрах графа из файла data/karate.edgelist и создайте на ее основе граф при помощи функции read_edgelist. При создании графа приведите идентификаторы узлов к числовому типу. Выведите на экран количество узлов и ребер загруженного графа, выведите на экран список узлов графа. Загрузите граф карате-клуба при помощи функции nx.karate_club_graph. Сравните две матрицы смежности (загруженного из файла графа и графа, созданного функцией) и покажите, что они одинаковые. При создании матриц смежности учтите, что порядок следования узлов в графах может различаться.

---2_drawing---
1. Создайте двумерную решетку размера 5х3, используя готовую функцию из networkx. Создайте словарь pos, в котором ключами являются идентификаторы узлов, а значениями - координаты этих узлов на плоскости. Для получения координат узлов считайте, что идентификатор состоит из пар (x, y), где x - позиция по горизонтали, а y - позиция по вертикали. Визуализируйте граф, используя полученный словарь. Добавьте на рисунок подписи узлов.

2. Создайте граф карате-клуба, используя готовую функцию из networkx. Используя несколько функций вида x_layout, рассчитайте координаты узлов и визуализируйте все доступные в networkx различные укладки в виде сетки из изображений. Каждому изображению в сетке добавьте заголовок в виде названия используемой функции. Если какая-то функция укладки не применима к данному графу, игнорируйте ее.

3. Создайте граф карате-клуба, используя готовую функцию из networkx. Добавьте узлам атрибут age, заполненный целыми числами от 20 до 50. Визуализируйте граф, раскрасив узлы в цвет клуба (красный для "Mr. Hi" и зеленый для "Officer") и сделав размер узла пропорциональным величине атрибута age.

4. Создайте ориентированный граф "путь" из 3 узлов. Визуализируйте граф со стрелками увеличенного размера. Визуализируйте граф, развернув стрелки на визуализации в обратном направлении (сам граф оставьте без изменений). На всех визуализациях добавляйте подписи узлов.

5. Создайте граф Les Miserables. Для каждого узла зафиксируйте координату на плоскости при помощи функции random_layout. Получите список edge_colors значений атрибута weight на ребрах и ограничьте (clip) его сверху значением 10, снизу значением 1.
Отрисуйте узлы графа при помощи функции draw_networkx_nodes с указанием размера узлов 10. На тот рисунок добавьте ребра графа при помощи функции draw_networkx_edges, раскрасив ребра в цвет, соответствующий значениям edge_colors с применением цветовой карты viridis. Добавьте на визуализацию colorbar, показывающий градиент цветов ребер.

---3_degree---
1. Создайте неориентированный граф на основе набора данных fb-pages-food с сайта https://networkrepository.com/. Добавьте узлам атрибут name, который содержит имя человека. Удалите из графа все петли. Получите матрицу смежности графа и посчитайте степень каждого узла на основе этой матрицы. Выведите имя человека, чей узел имеет максимальную степень, и само значение степени.

2. Используя граф из предыдущего задания, посчитайте степени каждого узла, используя возможности графа nx.Graph. Посчитайте и выведите на экран среднюю степень узлов графа, округленную до ближайшего целого.

3. Создайте ориентированный граф на основе набора данных fb-pages-food с сайта https://networkrepository.com/. Удалите из графа все петли. Получите матрицу смежности графа и посчитайте входящую и исходящую степень каждого узла на основе этой матрицы. Найдите и выведите на экран максимальное значение исходящих степеней и среднее значение входящих степеней.

4. Постройте графики выборочной функции распределения входящих и исходящих степеней узлов для загруженного ориентированного графа. Для нахождения степеней узлов воспользуйтесь возможностями графа nx.DiGraph. Добавьте графикам названия, а осям - подписи. Расположите графики рядом по горизонтали.
    
5. Визуализируйте закон распределения степеней узлов для неориентированного варианта графа в обычной и логарифимических (по 1й и по 2м осям) системах координат для неориентированного графа. Расположите графики в виде сетки 2х2. Добавьте графикам названия, а осям - подписи. Использование готовых функций для создания и визуализации гистограмм не допускается.

6. Сгенерируйте выборку из Дзета-распределения (распределение Ципфа) с числом наблюдений 10 тыс. и параметром распределения  𝛼=2
 . Визуализируйте эмпирический закон распределения с использованием линейного и логарифмического биннинга. Визуализируйте оба варианта в линейных и логарифмических (log-log) координатах (должно получиться 4 изображения).
 
---4_models---
1. Используя реализацию модели предпочтительного присоединения Барабаши-Альберта из networkx, постройте сеть с кол-вом узлов n > 50. Визуализируйте полученный граф, сделайте размер и цвет узлов пропорциональными их степени. Постройте график выборочной функции распределения степеней узлов для сети.

2. Используя реализацию модели предпочтительного присоединения Барабаши-Альберта из networkx, постройте сеть с кол-вом узлов n=10000. Визуализируйте закон распределения степеней узлов сети в обычной и логарифимических (по 1й и по 2м осям) системах координат.

3. Визуализируйте закон распределения степеней узлов с использованием log-binning для сети из предыдущего задания. Для сравнения на этом же графике изобразите график закона распределения без использования log-binning в log-log координатах.

4. C помощью реализации модели Уотса-Строгатса из networkx сгенерируйте сети с кол-вом узлов порядка 1000 и с разным p. В качестве p рассмотрите несколько значений, равномерно распределенных по логарифмической шкале на отрезке  [10−4,1].
Визуализируйте результаты пересвязывания, построив на одном графике относительные изменения среднего коэффициента кластеризации и средней длины пути относительно варианта сети без пересвязывания. Обозначьте точки разных графиков различными маркерами и цветами. Добавьте легенду.

5. Создайте неориентированный граф на основе набора данных fb-pages-food с сайта https://networkrepository.com/ . C помощью nx.configuration_model постройте рандомизированный аналог данной сети. Визуализируйте исходный и рандомизированный граф рядом. Проверьте, совпадают ли распределения степеней узлов исходного и рандомизированного графа.

6. C помощью реализации алгоритма постоения случайного геометрического графа из networkx сгенерируйте сеть с кол-вом узлов порядка 500 при радиусе r=0.2 и  𝐿2  метрикой для вычисления расстояния. Визуализируйте полученный граф, уменьшив размер узлов и сделав цвет ребер черным прозрачным (используйте RGBA). Визуализируйте распределение степеней узлов полученного графа.
Повторите решение задачи, использовав  𝐿1

---5_community_detection---
1. Загрузите сеть Southern women с сайта http://konect.cc/, удалите из него петли и создайте не менее 5 разбиений этой сети на 2 сообщества (часть разбиений должны частично или полностью соответствовать интуитивным представлениям о разбиении на сообщества, часть - нет). Для создания разбиений не используйте специализированные алгоритмы. Визуализируйте графы, раскрасив узлы в цвет соответствующего им сообщества.

2. Для разбиений из предыдущей задачи посчитайте следующие величины:

плотность первого сообщества
плотность второго сообщества
среднюю плотность сообществ.
модулярность
покрытие (coverage)
эффективность (performance)
Все значения округлите до сотых и сведите в таблицу следующего вида:

Разбиение	Плотность первого сообщества	Плотность второго сообщества	...	...
1				
...				
Для представления данных в табличном виде используйте pandas.

3. Считайте сеть из файла communities.edgelist. Выполните разбиение сети на 2 сообщества с помощью алгоритма Гирвана-Ньюмена. Визуализируйте результат, раскрасив узлы в цвет, соответствующий их сообществу.

4. Постройте график динамики модулярности для шагов алгоритма Гирвана-Ньюмена. Визуализируйте разбиение сети из предыдущей задачи, при котором достигается наилучшее значение модулярности (выведите это значение на экран). Для визуализации выберите несколько цветов (в соответствии с количеством выделенных сообществ) и раскрасьте узлы и связи в рамках одного сообщества этими цветами. Связи между узлами, состоящими в разных сообществах, отрисуйте черным цветом.

5. Выполните разбиение сети из первого задания на сообщества с помощью Лувенского алгоритма. Визуализируйте результат аналогично предыдущему заданию. 

6. Постройте матрицу смежности для сети. Постройте матрицу смежности для сети, в которой узлы перенумерованы в порядке возрастания номера сообщества, которому они принадлежат (т.е. номера  0...𝐶1−1
  даются  𝐶1
  узлам, принадлежащим первому сообществу; номера  𝐶1...𝐶2−1
  даются  𝐶2
  узлам, принадлежащим второму сообществу и т.д.). Для разбиения графа на сообщества воспользуйтесь Лувенским алгоритмом.

Визуализируйте две матрицы смежности при помощи seaborn.heatmap. Расположите рисунки рядом по горизонтали, добавьте названия рисунков. Сравните полученные изображения и сделайте выводы.

---6_lpa---
1. Загрузите граф карате-клуба и выделите в нем сообщества с помощью реализации алгоритма распространения меток из networkx. Визуализируйте полученный результат. Для визуализации выберите несколько цветов (в соответствии с количеством выделенных сообществ) и раскрасьте узлы и связи в рамках одного сообщества этими цветами. Связи между узлами, состоящими в разных сообществах, отрисуйте черным цветом.

2. Реализуйте синхронный алгоритм распространения меток. В синхронном варианте алгоритма для выбора новой метки узла используются метки соседей с предыдущей итерации. Если среди меток соседей есть несколько вариантов с одинаковой максимальной частотой, то метка выбирается случайным образом. Алгоритм прекращает работу, когда на очередной итерации не была изменена метка ни одного узла. Визуализируйте пошаговую динамику распространения меток и итоговое разбиение сети аналогично предыдущей задаче.

3. Реализуйте асинхронный алгоритм распространения меток. В асинхронном варианте алгоритма для выбора новой метки узла используются в том числе метки соседей с текущей итерации алгоритма. Визуализируйте пошаговую динамику распространения меток и итоговое разбиение сети аналогично предыдущей задаче.

4. Используя собственную реализацию LPA, разбейте граф карате-клуба на сообщества N=10 раз. Создайте двумерный массив freqs размера MxM (M - количество узлов в графе), где freqs[i, j] показывает, как часто узлы i и j оказывались в одном сообществе.
Создайте граф, в котором между узлами i и j существует связь только тогда, когда freqs[i, j] >= 8. Получите компоненты связности данного графа. Интерпретируя данные компоненты связности как сообщества в исходном графе, визуализируйте полученное разбиение карате-клуба (оригинального графа) аналогично предыдущим заданиям.

5. Оформите результаты работы алгоритмов в виде таблицы
Алгоритм	Средняя плотность сообществ	Модулярность	Покрытие	Эффективность
Синхронный LPA				
Асинхронный LPA				
Множественный LPA				
Для представления данных в табличном виде используйте pandas. Все расчеты метрик при решении этой задачи выполните повторно.

---7-rand_walk---
1\. Загрузите граф карате-клуба. Получите матрицу смежности `A` этого графа. Получите на ее основе матрицу переходов `P` по следующему правилу:
$$\mathbf{P}=\mathbf{D}^{-1}\mathbf{A}$$
Продемонстрируйте, что выполняются условия (1) и (2).
$0 \le p_{ij} \le 1$ (1)
$\sum_j p_{ij}=1$    (2)
Все действия проводите с невзвешенной матрицей смежности.

2\. Создайте вектор начального состояния $\mathbf{p}^0 = [0, ..., 1]^T$. Получите стационарное состояние $\mathbf{p}^\infty$, используя итеративную процедуру
$\mathbf{p}^{t+1}=(\mathbf{P}^{\top})\mathbf{p}^t$ 
Процесс заканчивается, когда $||\mathbf{p}^{t+1} - \mathbf{p}^{t}|| < \epsilon $
Выведите полученный вектор стационарного состояния на экран.
3\. Найдите матрицу перехода к стационарному состоянию $(\mathbf{P}^{\top})^\infty$ при помощи процедуры возведения матрицы в степень.
Докажите, что полученная матрица является матрицей стационарного состояния, т.е. $||(\mathbf{P}^{\top})^{\infty}  -(\mathbf{P}^{\top})(\mathbf{P}^{\top})^{\infty}|| <= \epsilon$
Cоздайте вектор начального состояния $\mathbf{p}^0 = [0, ..., 1]^T $. Получите стационарное состояние $\mathbf{p}^\infty$, воспользовавшись полученной матрицей $(\mathbf{P}^{\top})^\infty$. Решите задачу двумя способами: при помощи матричного умножения и при помощи оператора индексации.
Используя функцию `np.allclose`, покажите, что векторы стационарных состояний, полученные двумя разными методами, совпадают (с точностью до тысячных).

4. Загрузите граф карате-клуба. Вычислите центральность каждого узла в сети. Визуализируйте граф, отмасшабировав размер каждого узла пропорционально полученным значениям. Постройте несколько визуализаций графа в виде сетки, используя следующие меры центральности:
центральность по степени;
центральность по посредничеству;
центральность по близости;
центральность по собственному вектору;
центральность по PageRank.
На каждом рисунке сделайте размеры узлов пропорционально соответствующей мере центральности. Сравните результаты и сделайте выводы.

5\. Реализуйте алгоритм PageRank с параметром затухания $\alpha=0.9$.
Загрузите граф карате-клуба и сделайте его ориентированным. Найдите стохастическую матрицу переходов $\mathbf{P}$.
$$\mathbf{P}=\mathbf{D}^{-1}\mathbf{A}$$
Модифицируйте матрицу $\mathbf{P}$:
$$\mathbf{P}' = \alpha \mathbf{P} + (1-\alpha)\frac{1}{n}\mathbf{E}$$
$$\mathbf{M}=(\mathbf{P}')^{\top}$$
где $\mathbf{E}$ - матрица размера `NxN`, состоящая из единиц.
Получите вектор $\mathbf{p}^\infty = \mathbf{pr}^{iter}$, используя итеративную процедуру
$$\mathbf{p}^{t+1}=\mathbf{M}\mathbf{p}^t$$
Процесс заканчивается, когда $||\mathbf{p}^{t+1} - \mathbf{p}^{t}|| < \epsilon $
Выведите полученный вектор стационарного состояния на экран.
Вызовите `nx.pagerank` с соответствующими параметрами и докажите, что полученные векторы совпадают.

6\. Вычислите коэффициенты PageRank при помощи задачи нахождения собственного вектора матрицы $\mathbf{M}$ $\mathbf{pr}^{eig}$, соответствующего собственному числу 1.
Покажите, что $\mathbf{pr}^{eig}$ и $\mathbf{pr}^{iter}$ оба являются с.в. матрицы $\mathbf{M}$.

---8_rwlpa---
1. Загрузите граф Karate Club. Получите значения атрибута club, хранящегося на узлах. Выберите случайным образом 4 узла, относящихся к клубу Mr. Hi и 4 узла, относящихся к клубу Officer. Получите матрицу смежности, в которой узлы перенумерованы таким образом, чтобы строки и столбцы с номерами 0, 1, ... 7 принадлежали узлам, которые были выбраны на предыдущем шаге.

2. Получите блочную матрицу  𝐏
  и матрицу начального состояния меток  𝐘0∈{0,1}𝑁×2
𝐏=(𝐏𝑙𝑙𝐏𝑢𝑙𝐏𝑙𝑢𝐏𝑢𝑢)=(𝐈𝐏𝑢𝑙0𝐏𝑢𝑢)
𝐘0=(𝐘𝐥𝐘𝐮)=(𝐘𝐥0)
Выведите на экран след матрицы  𝐏
 . Выведите на экран количество ненулевых элементов матрицы  𝐘0
# Процесс случайного блуждания с ловушками, попав в них при блуждании мы остаемся в них во время всех последующих итераций
# состояние узлов-ловушек всегда остается неизменным
# траектории случайных блажданий из узлов, относящихся к u, будут завершаться на
# изначально помеченных узлах

3. Известно, что для блочной матрицы справедливо:
𝐏∞=(𝐈(𝐈−𝐏𝑢𝑢)−1𝐏𝑢𝑙00)
Получите оценку  𝐏∞
  путем возведения матрицы в достаточно большую степень. Свяжите блоки матрицы с отдельными переменными P_ll, P_lu, P_ul, P_uu и продемонстрируйте, что каждый блок полученной матрицы удовлетворяет формуле выше при помощи функции np.allclose.
  
4. Используя Базовый вариант RW-LPA , расставьте метки для всех узлов. Визуализируйте сеть, показав цветом контура вокруг узла принадлежность узла к одному из сообществ, а цветом цветом заливки узла - сообщество, к которому узел был отнесен алгоритмом.

---9_cuts---
1. Задан граф G. Определите два разреза:

𝐶(1)=(𝑁(1)1,𝑁(1)2)
 ,  𝑁(1)1={0,1,2,3,4}
 ,  𝑁(1)2={5,6,7,8,9}
 
𝐶(2)=(𝑁(2)1,𝑁(2)2)
 ,  𝑁(2)1={0,1,2,3,4,5,6,7,8}
 ,  𝑁(2)2={9}
 
Для каждого из разрезов вычислите величину  𝑄(𝑘)
  двумя способами: явным образом просмотрев все ребра и при помощи функции из пакета networkx. Выведите величины разрезов на экран.

𝑄(𝑘)=𝑐𝑢𝑡(𝑁(𝑘)1,𝑁(𝑘)2)=∑𝑖∈𝑁(𝑘)1,𝑗∈𝑁(𝑘)2𝑙𝑖𝑗

2. Для разрезов из предыдущего задания вычислите
𝑉𝑜𝑙(𝑁(𝑘)𝑡)=∑𝑖∈𝑁(𝑘)𝑡,𝑗∈𝑁𝑙𝑖𝑗=∑𝑖∈𝑁(𝑘)𝑡𝑘𝑖,𝑡=1,2
𝑄(𝑘)𝑛𝑜𝑟𝑚𝑎𝑙𝑖𝑧𝑒𝑑=𝑐𝑢𝑡(𝑁(𝑘)1,𝑁(𝑘)2)𝑉𝑜𝑙(𝑁(𝑘)1)+𝑐𝑢𝑡(𝑁(𝑘)1,𝑁(𝑘)2)𝑉𝑜𝑙(𝑁(𝑘)2)
и
𝑄(𝑘)𝑞𝑢𝑜𝑡𝑖𝑒𝑛𝑡=𝑐𝑢𝑡(𝑁(𝑘)1,𝑁(𝑘)2)min(𝑉𝑜𝑙(𝑁(𝑘)1),𝑉𝑜𝑙(𝑁(𝑘)2))
Для каждого разреза выведите четыре величины  𝑉𝑜𝑙(𝑁(𝑘)1)
 ,  𝑉𝑜𝑙(𝑁(𝑘)2)
 ,  𝑄(𝑘)𝑛𝑜𝑟𝑚𝑎𝑙𝑖𝑧𝑒𝑑
  и  𝑄(𝑘)𝑞𝑢𝑜𝑡𝑖𝑒𝑛𝑡
  на экран.
  
3. Для графа G из задания 1 вычислите матрицу Лапласа двумя способами:
по формуле  𝐋=𝐃−𝐀;
используя готовую функцию из networkx.
Выведите полученные матрицы на экран и покажите, что они равны.

4. Для каждого из разрезов первого задания вычислите величины  𝑄(𝑘)
 , воспользовавшись матрицей Лапласа.
 
5. Найдите собственные значения и собственные векторы матрицы Лапласа. Выведите на экран кратность нулевого собственного значения матрицы Лапласа. Найдите и выведите на экран количество компонент связности в графе G.
Создайте копию графа G с удаленными ребрами в соответствии с разрезом 𝐶(1)
. Повторите решение задачи для получившегося несвязного графа.
 Проверено на семинаре
Лемма: для неориентированной сети с неотрицательными весами кратность нулевого собственного значения матрицы лапласа равна количеству связных компонент сети , … , . Собственные пространство с.з. 0, над собственными векторами, отвечающимим с.з. 0 является оболочкой над индикаторными векторами этих компонент.

---10_cluster---
1. Загрузите граф карате клуба. Вычислите матрицу Лапласа  𝐋=𝐃−𝐀
  при помощи готовой функции. Найдите собственные значения и собственные векторы матрицы Лапласа. При использовании функции явно укажите, что используется невзвешенная матрица смежности.
Выделите собственный вектор  𝐮
 , отвечающий минимальному ненулевому с.з.  𝜆𝑖≠0
  и выведите его на экран. Преобразуйте  𝐮
  в индикаторный вектор  𝐟
  по правилу  𝐟𝑖
  = sign( 𝐮𝑖)
 .
Визуализируйте  𝐮
 : по горизонтали откладывается номер узла, по вертикали значение соответствующей координаты вектора  𝐮
Визуализируйте граф, обозначив цветом узла компоненту, в которую попадает узел в соответствии с найденным разрезом.

2. Повторите предыдущую задачу, используя нормализованную матрицу Лапласа  𝐋𝑠𝑦𝑚=𝐃−12𝐋𝐃−12
 . При расчете матрицы Лапласа явно укажите, что используется взвешенная матрица смежности  𝐖
 
3. Дан набор данных (X, y). Визуализируйте набор данных, отрисовав точки на плоскости и раскрасив в цвета, соответствующие меткам объектов y. Решите задачу кластеризации данных на 3 кластера при помощи алгоритма KMeans из sklearn. Визуализируйте полученный результат.

4. На основе датасета из предыдущего задания создайте граф при помощи функции radius_neighbors_graph из sklearn (укажите аргумент radius=1). Получите матрицу  𝑈∈ℝ300×3
 , состояющую из собственных векторов матрицы Лапласа, соответствующих трем наименьшим ненулевым собственным значениям. Решите задачу кластеризации при помощи алгоритма KMeans из sklearn на основе матрицы  𝑈
 . Визуализируйте полученный результат.
 
5. Решите задачу кластеризации данных на 3 кластера при помощи алгоритма SpectralClustering из sklearn. Визуализируйте полученный результат. При создании модели кластеризации укажите значение аргумента affinity='nearest_neighbors'.

---11_gcnn_mpf---
1. Загрузите граф из файла karate.graphml. Визуализируйте граф, раскрасив в цвета классов (атрибут label у узлов).
Получите невзвешенную матрицу смежности  𝐀
  в виде тензора torch и тензор меток для каждого узла. Выведите полученные тензоры на экран.
  
2. Найдите стохастическую матрицу  𝐀̂.
𝐀̂ =𝐃−1𝐀
Сгенерируйте тензор  𝐗
  размера Nx5 с признаками узлов, используя torch.rand. Выполните один шаг рассылки сообщений при помощи матричного умножения.
𝐗̂ =𝐀̂ 𝐗
Выполните этот же шаг рассылки сообщений, явно проитерировавшись по узлам графа и усреднив атрибуты соседей. Сравните два полученных тензора и покажите, что они равны.

3. Опишите слой GCNLayer графовой сверточной нейронной сети. Создайте слой с n_outputs=3 и пропустите через этот слой матрицу смежности графа и тензор признаков  𝐗=𝐄
 . Выведите форму полученного тензора на экран.
 
4. Используя графовую сверточную нейронную сеть, решите задачу классификации узлов. Создайте модель, состоящую из двух последовательно идущих слоев GCNLayer. Первый слой имеет 2 нейрона и гиперболический тангенс в качестве функции активации. Количество нейронов во втором слое определяется количеством классов в задаче. Настройте модель (в качестве признаков узлов используйте единичную матрицу  𝐗=𝐄).
После завершения процесса обучения пропустите данные через первый слой и визуализируйте граф, используя полученные векторные представления в качестве координат. Раскрасьте узлы в цвет соответствующих классов.

5. Повторите решение предыдущей задачи, используя в качестве признаков  𝐗
  следующие характеристики узлов (каждый узел описывается четырьмя характеристиками):
количество треугольников, в которые входит узел;
степень узла;
эксцентриситет узла;
центральность по собственному вектору для узла.
Для ускорения процесса обучения стандартизуйте тензор признаков.

---12_gnn_nodes---
1. Загрузите граф Planetoid/Cora из torch_geometric.datasets.
Выведите на экран:
количество узлов графа;
количество ребер графа;
размерность признаков узлов;
количество узлов для обучения, валидации и тестирования.
Решите задачу классификации узлов графа, используя только полносвязные слои torch.nn.Linear (создайте модель из двух слоев). Для обучения используйте пакетный градиентный спуск (не разбивайте на батчи). Обратите внимание, что настройка весов модели должна проводиться только на основе примеров из обучающей выборки. Посчитайте и выведите на экран значение accuracy на тестовой выборке.

2. Решите задачу 1, используя два слоя torch_geometric.nn.GCNConv вместо полносвязных слоев torch.nn.Linear.

3. Воспользовавшись необученной моделью из предыдущего задания, получите прогнозы для всех узлов графа. Уменьшите размерность полученных прогнозов до 2 при помощи алгоритма t-SNE (sklearn.manifold.TSNE). Визуализируйте точки на плоскости, используя полученные значения в качестве координат. Раскрасьте точки в цвета, соответствующим меткам узлов.
Повторите данную процедуру, используя обученную модель. Сравните результаты и сделайте выводы.

4. Предыдущие решения не используют узлы, находящиеся в валидационном множестве. Решите задачу 2, используя валидационное множество для выполнения ранней остановки.

5. Повторите решение задачи 4, сравнив несколько различных слоев:
GCNConv
SAGEConv (укажите aggr="mean")
GATConv (выберите для первого слоя heads > 1)
Выведите результат в виде таблицы:
Модель	Loss на обучении	Acc на обучении	Acc на тесте	Кол-во эпох до ранней остановк

---13_gnn_graphs---
1. Реализуйте все описанные методы класса GraphsDataset. Используя данный класс, создайте датасет на основе файлов архива graphs.zip (можно разархивировать вручную или программно). Выведите на экран количество объектов в датасете. Выведите на экран значения признаков узлов для графа с индексом 0.

2. Используя датасет из предыдущего задания, создайте объект torch_geometric.loader.DataLodaer. Получите один батч размера 128 при помощи этого объекта и выведите на экран (используйте соответствующие атрибуты и методы):
количество узлов в графе-батче;
количество связей в графе-батче;
количество графов в батче;
количество узлов в каждом графе батча;
Выполните readout для графа на основе атрибута x. Выведите размерность полученного тензора на экран.

3. Решите задачу классификации графа, используя слои SAGEConv и операцию усреднения для процедуры readout. Для обучения используйте стохастический градиентный спуск с размером батча 128. Во время обучения выводите значение функции потерь по эпохам (используйте torchmetrics). Вычислите матрицу несоответствий прогнозов и точность обученной модели (используйте torchmetrics).

4. Повторите решение задачи 3, сравнив разные функции агрегации для проведения операции readout. Выведите результаты в виде таблицы:
Выведите результат в виде таблицы:
Readout op	Loss	Acc
sum		
mean		
max		
min		

---14_kg_trans---
1. Загрузите датасет CoDExSmall. Выведите на экран (обратитесь к соответстующим полям) количество сущностей в обучающем (<dataset>.training), валидационном (<dataset>.validation) и тестовом (<dataset>.testing) множестве.
Выведите на экран одну произвольную тройку  (ℎ,𝑟,𝑡)
  в оригинальном виде (<dataset>.triples) и эту же тройку, но с использованием числового представления сущностей/отношений (<dataset>.mapped_triples).
  
2. Обучите модель TransE на датасете CoDExSmall при помощи pipeline. Задайте количество эпох обучения (num_epochs в training_kwargs), равное 100, и размерность эмбеддингов (embedding_dim в  model_kwargs), равную 64.
Изобразите график значений функции потерь в зависимости от номера эпохи (<pipeline_result>.plot_losses).
Выведите на экран значение метрик в виде pd.DataFrame (<pipeline_result>.metric_results.to_df())

3. Выберите случайным образом одну тройку  (ℎ,𝑟,𝑡∗)
  из тестового множества. Используя обученную модель, рассчитайте оценки для всех троек  (ℎ,𝑟,𝑡),𝑡∈𝐾.
Найдите позицию (ранг) истинного значения объекта тройки  𝑡∗
  в списке прогнозов, отсортированном в порядке убывания величины score.
  
4. Обучите модели TransE, TransH и TransD на датасете CoDExSmall.
Сравните качество полученных моделей по следующим метрикам (side=both и type=realistic):
Adjusted Arithmetic Mean Rank (AAMR): adjusted_arithmetic_mean_rank
Mean Reciprocal Rank (MRR): inverse_harmonic_mean_rank
Hits @ 10: hits_at_10
Визуализируйте результаты в виде столбчатой диаграммы
    """
    pc.copy(text)
    
    
def get_link():
    """
    https://drive.google.com/drive/folders/1Py0NikzzIp3o-gqqLzT-bTsfv-arh_wO?usp=sharing
    """
    text = 'https://drive.google.com/drive/folders/1Py0NikzzIp3o-gqqLzT-bTsfv-arh_wO?usp=sharing'
    pc.copy(text)


def one_nx_graph_1():
    """
    1. Напишите функцию, генерирующую кольцевой неориентированный граф из n узлов (n - параметр функции). Идентификатором узла выступают целые числа от 0 до n-1. Каждому узлу добавьте атрибут age, заполненный случайными целыми числами от 1 до 100. Каждому ребру добавьте атрибут mean_incident_age, состоящий из среднего значения атрибута age у узлов, инцидентных данному ребру.

Создайте граф с n=5 узлами. Выведите на экран словари, содержащие атрибут age для узлов (ключ - идентификатор узла) и mean_incident_age для ребер (ключ - пара (u, v) идентификаторов начала и конца ребра). Визуализируйте граф при помощи функции nx.draw (или nx.draw_networkx) с аргументами по умолчанию.

def generate_ring_graph(n: int = 5) -> (nx.classes.graph.Graph, dict, dict):
    G = nx.Graph()
    G.add_node(0, age=random.randint(1, 100))
    
    for i in range(1, n):
        G.add_node(i, age=random.randint(1, 100))
        data_nodes = G.nodes(data=True)
        G.add_edge(i, i-1, mean_incident_age=np.mean([data_nodes[i]["age"], data_nodes[i-1]["age"]]))
        
    G.add_edge(0, i, mean_incident_age=np.mean([data_nodes[0]["age"], data_nodes[i]["age"]]))
    dict_nodes = {i[0]: i[1]["age"] for i in G.nodes(data=True)}
    dict_edges = {(i[0], i[1]): i[2]["mean_incident_age"] for i in G.edges(data=True)}
    return G, dict_nodes, dict_edges
    
    graph, dict_nodes, dict_edges = generate_ring_graph(n=5)
    print(dict_nodes)
    print(dict_edges)
    nx.draw(graph, with_labels=True, )
    """
    text = """
def generate_ring_graph(n: int = 5) -> (nx.classes.graph.Graph, dict, dict):
    G = nx.Graph()
    G.add_node(0, age=random.randint(1, 100))
    
    for i in range(1, n):
        G.add_node(i, age=random.randint(1, 100))
        data_nodes = G.nodes(data=True)
        G.add_edge(i, i-1, mean_incident_age=np.mean([data_nodes[i]["age"], data_nodes[i-1]["age"]]))
        
    G.add_edge(0, i, mean_incident_age=np.mean([data_nodes[0]["age"], data_nodes[i]["age"]]))
    dict_nodes = {i[0]: i[1]["age"] for i in G.nodes(data=True)}
    dict_edges = {(i[0], i[1]): i[2]["mean_incident_age"] for i in G.edges(data=True)}
    return G, dict_nodes, dict_edges
    
    graph, dict_nodes, dict_edges = generate_ring_graph(n=5)
    print(dict_nodes)
    print(dict_edges)
    nx.draw(graph, with_labels=True, )
    
    """
    pc.copy(text)
    
    
def one_nx_graph_2():
    """
    2. Напишите функцию, генерирующую ориентированный граф из 3n + 1 узлов (n - параметр функции), где центральный узел соединен с тремя путями длины n. Ребра всегда направлены в сторону удаления от центрального узла.

Сгенерируйте граф для n=3. Выведите на экран количество узлов и ребер в графе. Для каждого узла выведите на экран список идентификаторов его соседей (отдельно по входящим и исходящим связям). Визуализируйте граф при помощи функции nx.draw (или nx.draw_networkx) с аргументами по умолчанию.

def generate_di_graph(n: int = 3) -> nx.classes.graph.Graph:
    G1 = nx.DiGraph()
    G1.add_node(0)
    for i in range(1, n*3, 3):
        G1.add_node(i)
        G1.add_edge(0, i)
        ar = np.arange(i+1, i+2)
        G1.add_nodes_from(ar)
        G1.add_edges_from([(i, j) for j in ar])
    return G1
    
graph_2 = generate_di_graph(3)
print("Количество узлов в графе:", graph_2.number_of_nodes())
print("Количество ребер в графе:", graph_2.size())

for node in graph_2.nodes():
    print("Узел:", node)
    print("Список id соседей по входящим связям:", list(graph_2.predecessors(node)))
    print("Список id соседей по исходящим связям:", list(graph_2.successors(node)))
    
nx.draw(graph_2, with_labels=True)
    """
    text = """
def generate_di_graph(n: int = 3) -> nx.classes.graph.Graph:
    G1 = nx.DiGraph()
    G1.add_node(0)
    for i in range(1, n*3, 3):
        G1.add_node(i)
        G1.add_edge(0, i)
        ar = np.arange(i+1, i+2)
        G1.add_nodes_from(ar)
        G1.add_edges_from([(i, j) for j in ar])
    return G1
    
graph_2 = generate_di_graph(3)
print("Количество узлов в графе:", graph_2.number_of_nodes())
print("Количество ребер в графе:", graph_2.size())

for node in graph_2.nodes():
    print("Узел:", node)
    print("Список id соседей по входящим связям:", list(graph_2.predecessors(node)))
    print("Список id соседей по исходящим связям:", list(graph_2.successors(node)))
    
nx.draw(graph_2, with_labels=True)
    """
    pc.copy(text)
    

def one_nx_graph_3():
    """
3. Напишите собственную функцию создания двумерной решетки n на m. В качестве идентификаторов узлов используйте пары (x, y), где x - номер строки решетки, y - номер столбца решетки. Сохраните ширину и высоту решетки в виде атрибутов n и m  самого графа.

Сгенерируйте граф с n=5, m=4. Выведите на экран количество узлов и ребер в графе. Выведите созданные атрибуты графа на экран. Визуализируйте граф при помощи функции nx.draw с аргументами по умолчанию. Создайте и визуализируйте аналогичный по структуре граф при помощи готовой функции из networkx.

def generate_grid_graph(n: int, m: int) -> nx.classes.graph.Graph:
    G = nx.Graph(n=n, m=m)
    for x in range(n):
        for y in range(m):
            G.add_node((x, y))
            if x < n - 1:
                G.add_edge((x, y), (x+1, y))
            if y < m - 1:
                G.add_edge((x, y), (x, y+1)) 
    return G
graph_3 = generate_grid_graph(n=5, m=4)
print("Количество узлов в графе:", graph_3.number_of_nodes())
print("Количество ребер в графе:", graph_3.size())
print("Атрибуты графа:", graph_3.graph)

nx.draw(graph_3)

G3 = nx.grid_2d_graph(n=5, m=4)
nx.draw(G3)
    """
    text = """
def generate_grid_graph(n: int, m: int) -> nx.classes.graph.Graph:
    G = nx.Graph(n=n, m=m)
    for x in range(n):
        for y in range(m):
            G.add_node((x, y))
            if x < n - 1:
                G.add_edge((x, y), (x+1, y))
            if y < m - 1:
                G.add_edge((x, y), (x, y+1)) 
    return G
graph_3 = generate_grid_graph(n=5, m=4)
print("Количество узлов в графе:", graph_3.number_of_nodes())
print("Количество ребер в графе:", graph_3.size())
print("Атрибуты графа:", graph_3.graph)

nx.draw(graph_3)

G3 = nx.grid_2d_graph(n=5, m=4)
nx.draw(G3)
    """
    pc.copy(text)
    
    
def one_nx_graph_4():
    """
4. Напишите функцию, которая генерирует мультиграф, состоящий из m узлов, где количество связей между узлами u и v сэмплится из биномиального распределения с параметрами n и p (m, n, p - параметры функции). Петли в графе допускаются. На каждом ребре создайте атрибут weight, равный обратному значению количества ребер между индицентными ему узлами. Создайте и визуализируйте граф при помощи функции nx.draw с аргументами по умолчанию. Создайте версию графа с удаленными петлями и визуализуйте ее.

def generate_multigraph(m: int, n: int, p: int) -> nx.classes.graph.Graph:
    G = nx.MultiGraph()
    G.add_nodes_from(range(m))
    for i in range(m):
        for j in range(i, m):
            count_edges = np.random.binomial(n, p)
            for _ in range(count_edges):
                G.add_edge(i, j, weight=round(1/count_edges, 3))
    return G
    
graph_4 = generate_multigraph(m=5, n=4, p=0.25)
nx.draw(graph_4, with_labels=True)

# без петлей
loops = list(nx.selfloop_edges(graph_4))
graph_4.remove_edges_from(loops)
nx.draw(graph_4, with_labels=True)
    """
    text = """
def generate_multigraph(m: int, n: int, p: int) -> nx.classes.graph.Graph:
    G = nx.MultiGraph()
    G.add_nodes_from(range(m))
    for i in range(m):
        for j in range(i, m):
            count_edges = np.random.binomial(n, p)
            for _ in range(count_edges):
                G.add_edge(i, j, weight=round(1/count_edges, 3))
    return G
    
graph_4 = generate_multigraph(m=5, n=4, p=0.25)
nx.draw(graph_4, with_labels=True)

# без петлей
loops = list(nx.selfloop_edges(graph_4))
graph_4.remove_edges_from(loops)
nx.draw(graph_4, with_labels=True)
    """
    pc.copy(text)
    

def one_nx_graph_5():
    """
5. Граф «(u,v) цветок»: фрактальный граф, в котором на каждой итерации роста каждая связь заменяется на 2 пути, длиной u и v соответственно (см.рис). Реализуйте функцию построения (1, 2) цветка. Функция принимает один целочисленный параметр n - количество шагов построения. Создайте и визуализируйте граф при помощи функции nx.draw с аргументами по умолчанию. Выведите на экран количество узлов и ребер графа на каждой из итераций.

def generate_flower_graph(n: int) -> nx.classes.graph.Graph:
    G = nx.Graph()
    G.add_nodes_from(range(2))
    G.add_edge(0, 1)
    for iteration in range(n):
        print("Итерация номер:", iteration+1)
        n_o_n = G.number_of_nodes()
        graph_edges = list(G.edges)
        
        for edge in graph_edges:
            G.add_node(n_o_n)
            G.add_edge(edge[0], n_o_n)
            G.add_edge(edge[1], n_o_n)
            n_o_n = G.number_of_nodes()
            
        print("Количество узлов:", n_o_n)
        print("Количество ребер:", G.size(), end='\n\n')
    return G
    
graph_5 = generate_flower_graph(n=4)
nx.draw(graph_5, with_labels=True)
    """
    text = """
def generate_flower_graph(n: int) -> nx.classes.graph.Graph:
    G = nx.Graph()
    G.add_nodes_from(range(2))
    G.add_edge(0, 1)
    for iteration in range(n):
        print("Итерация номер:", iteration+1)
        n_o_n = G.number_of_nodes()
        graph_edges = list(G.edges)
        
        for edge in graph_edges:
            G.add_node(n_o_n)
            G.add_edge(edge[0], n_o_n)
            G.add_edge(edge[1], n_o_n)
            n_o_n = G.number_of_nodes()
            
        print("Количество узлов:", n_o_n)
        print("Количество ребер:", G.size(), end='\n\n')
    return G
    
graph_5 = generate_flower_graph(n=4)
nx.draw(graph_5, with_labels=True)
    """
    pc.copy(text)
    
    
def one_nx_graph_6():
    """
6. Загрузите матрицу смежности графа из файла weighted_wheel.npy и создайте на ее основе граф. Выведите на экран количество узлов и связей графа. Выведите на экран тип графа (nx.Graph, nx.DiGraph, ...). Выведите на экран словарь со значениями атрибута weight на ребрах.Сохраните граф на диск в формате edgelist.

numpy_adj_matr = np.load("data/weighted_wheel.npy")
adj_matrix = nx.from_numpy_array(numpy_adj_matr, edge_attr='weight')

print("Количество узлов:", adj_matrix.number_of_nodes())
print("Количество связей:", adj_matrix.size())
print("Тип графа:", 'nx.' + str(type(adj_matrix)).split('.')[-1][:-2])

weight_dict = {(u, v): d['weight'] for u, v, d in adj_matrix.edges(data=True)}
print("Значение 'weight' на ребрах", weight_dict)

nx.write_weighted_edgelist(adj_matrix, 'data/weighted_wheel.edgelist')

pos = nx.spring_layout(adj_matrix)

nx.draw(adj_matrix, pos, with_labels=True)
nx.draw_networkx_edge_labels(adj_matrix, pos, edge_labels=weight_dict, font_color='red');
    """
    
    text = """
numpy_adj_matr = np.load("data/weighted_wheel.npy")
adj_matrix = nx.from_numpy_array(numpy_adj_matr, edge_attr='weight')

print("Количество узлов:", adj_matrix.number_of_nodes())
print("Количество связей:", adj_matrix.size())
print("Тип графа:", 'nx.' + str(type(adj_matrix)).split('.')[-1][:-2])

weight_dict = {(u, v): d['weight'] for u, v, d in adj_matrix.edges(data=True)}
print("Значение 'weight' на ребрах", weight_dict)

nx.write_weighted_edgelist(adj_matrix, 'data/weighted_wheel.edgelist')

pos = nx.spring_layout(adj_matrix)

nx.draw(adj_matrix, pos, with_labels=True)
nx.draw_networkx_edge_labels(adj_matrix, pos, edge_labels=weight_dict, font_color='red');
    """
    pc.copy(text)
    
    
def one_nx_graph_7():
    """
7. Загрузите информацию о ребрах графа из файла data/karate.edgelist и создайте на ее основе граф при помощи функции read_edgelist. При создании графа приведите идентификаторы узлов к числовому типу. Выведите на экран количество узлов и ребер загруженного графа, выведите на экран список узлов графа. Загрузите граф карате-клуба при помощи функции nx.karate_club_graph. Сравните две матрицы смежности (загруженного из файла графа и графа, созданного функцией) и покажите, что они одинаковые. При создании матриц смежности учтите, что порядок следования узлов в графах может различаться.

karate_graph = nx.read_edgelist("data/karate.edgelist", delimiter='/', nodetype=int)

print("Количество узлов:", karate_graph.number_of_nodes())
print("Количество ребер:", karate_graph.size())
print("Список узлов графа:", list(karate_graph.nodes()))

pos = nx.spring_layout(karate_graph)
nx.draw(karate_graph, pos, with_labels=True)

karate_club_graph = nx.karate_club_graph()
nx.draw(karate_club_graph, pos, node_color='green', with_labels=True)

nodelist = karate_graph.nodes()
adj_mat_karate_graph = nx.adjacency_matrix(G=karate_graph, nodelist=nodelist).toarray()
adj_mat_karate_club_graph = nx.adjacency_matrix(G=karate_club_graph, nodelist=nodelist).toarray()

if np.array_equal(adj_mat_karate_graph, adj_mat_karate_club_graph):
    print("Матрицы смежности одинаковые")
else:
    print("Матрицы смежности разные")
    """
    text = """
karate_graph = nx.read_edgelist("data/karate.edgelist", delimiter='/', nodetype=int)

print("Количество узлов:", karate_graph.number_of_nodes())
print("Количество ребер:", karate_graph.size())
print("Список узлов графа:", list(karate_graph.nodes()))

pos = nx.spring_layout(karate_graph)
nx.draw(karate_graph, pos, with_labels=True)

karate_club_graph = nx.karate_club_graph()
nx.draw(karate_club_graph, pos, node_color='green', with_labels=True)

nodelist = karate_graph.nodes()
adj_mat_karate_graph = nx.adjacency_matrix(G=karate_graph, nodelist=nodelist).toarray()
adj_mat_karate_club_graph = nx.adjacency_matrix(G=karate_club_graph, nodelist=nodelist).toarray()

if np.array_equal(adj_mat_karate_graph, adj_mat_karate_club_graph):
    print("Матрицы смежности одинаковые")
else:
    print("Матрицы смежности разные")
    """
    pc.copy(text)
    
    
def two_drawing_1():
    """
1. Создайте двумерную решетку размера 5х3, используя готовую функцию из networkx. Создайте словарь pos, в котором ключами являются идентификаторы узлов, а значениями - координаты этих узлов на плоскости. Для получения координат узлов считайте, что идентификатор состоит из пар (x, y), где x - позиция по горизонтали, а y - позиция по вертикали. Визуализируйте граф, используя полученный словарь. Добавьте на рисунок подписи узлов.

G1 = nx.grid_2d_graph(5,3)
pos = {node:[node[0], node[1]] for node in G1.nodes}
pos

nx.draw(G1, pos=pos, with_labels=True)

    """
    text = """
G1 = nx.grid_2d_graph(5,3)
pos = {node:[node[0], node[1]] for node in G1.nodes}
pos

nx.draw(G1, pos=pos, with_labels=True)
    """
    pc.copy(text)
    

def two_drawing_2():
    """
2. Создайте граф карате-клуба, используя готовую функцию из networkx. Используя несколько функций вида x_layout, рассчитайте координаты узлов и визуализируйте все доступные в networkx различные укладки в виде сетки из изображений. Каждому изображению в сетке добавьте заголовок в виде названия используемой функции. Если какая-то функция укладки не применима к данному графу, игнорируйте ее.

G2 = nx.karate_club_graph()

layout_funcs = [
    nx.arf_layout, # больще симметрии, чем в spring_layout
    nx.bipartite_layout, # двудольный
    nx.spring_layout,
    nx.circular_layout,
    nx.kamada_kawai_layout, # длина пути kamada-kawai
    nx.random_layout,
    nx.shell_layout,
    nx.spectral_layout, # спектры матрицы Лапласса
    nx.spiral_layout,
    nx.planar_layout,
]

fig = plt.figure(figsize=(12, 8))
gs = gridspec.GridSpec(3, 3, figure=fig)

for i, layout_func in enumerate(layout_funcs):
    try:
        pos = layout_func(G2)
        ax = fig.add_subplot(gs[i])        
        nx.draw(G2, pos, with_labels=True, ax=ax)
        ax.set_title(layout_func.__name__)
    except:
        print("Ошибка в функции:", layout_func.__name__)

plt.tight_layout()
plt.show()
    """
    text = """
G2 = nx.karate_club_graph()

layout_funcs = [
    nx.arf_layout, # больще симметрии, чем в spring_layout
    nx.bipartite_layout, # двудольный
    nx.spring_layout,
    nx.circular_layout,
    nx.kamada_kawai_layout, # длина пути kamada-kawai
    nx.random_layout,
    nx.shell_layout,
    nx.spectral_layout, # спектры матрицы Лапласса
    nx.spiral_layout,
    nx.planar_layout,
]

fig = plt.figure(figsize=(12, 8))
gs = gridspec.GridSpec(3, 3, figure=fig)

for i, layout_func in enumerate(layout_funcs):
    try:
        pos = layout_func(G2)
        ax = fig.add_subplot(gs[i])        
        nx.draw(G2, pos, with_labels=True, ax=ax)
        ax.set_title(layout_func.__name__)
    except:
        print("Ошибка в функции:", layout_func.__name__)

plt.tight_layout()
plt.show()
    """
    pc.copy(text)
    
    
def two_drawing_3():
    """
3. Создайте граф карате-клуба, используя готовую функцию из networkx. Добавьте узлам атрибут age, заполненный целыми числами от 20 до 50. Визуализируйте граф, раскрасив узлы в цвет клуба (красный для "Mr. Hi" и зеленый для "Officer") и сделав размер узла пропорциональным величине атрибута age.

G3 = nx.karate_club_graph()
age_attr = {node:np.random.randint(20, 51) for node in G3.nodes}
nx.set_node_attributes(G3, age_attr, name="age")
G3.nodes(data=True)

node_size = [attr["age"]*10 for node, attr in G3.nodes(data=True)]
colors = ["red" if attr["club"] == "Mr. Hi" else "green" for node, attr in G3.nodes(data=True)]
nx.draw(G3, with_labels=True, node_size=node_size, node_color=colors)
    """
    text = """
G3 = nx.karate_club_graph()
age_attr = {node:np.random.randint(20, 51) for node in G3.nodes}
nx.set_node_attributes(G3, age_attr, name="age")
G3.nodes(data=True)

node_size = [attr["age"]*10 for node, attr in G3.nodes(data=True)]
colors = ["red" if attr["club"] == "Mr. Hi" else "green" for node, attr in G3.nodes(data=True)]
nx.draw(G3, with_labels=True, node_size=node_size, node_color=colors)
    """
    pc.copy(text)
    
    
def two_drawing_4():
    """
4. Создайте ориентированный граф "путь" из 3 узлов. Визуализируйте граф со стрелками увеличенного размера. Визуализируйте граф, развернув стрелки на визуализации в обратном направлении (сам граф оставьте без изменений). На всех визуализациях добавляйте подписи узлов.

G4 = nx.DiGraph()
G4.add_edges_from([(1, 2), (2, 3)])
pos = nx.spring_layout(G3)
nx.draw(G4, pos=pos, arrowsize=50, with_labels=True)

nx.draw(G4, pos=pos, arrowsize=50, arrowstyle='<|-', with_labels=True)
    """
    text = """
G4 = nx.DiGraph()
G4.add_edges_from([(1, 2), (2, 3)])
pos = nx.spring_layout(G3)
nx.draw(G4, pos=pos, arrowsize=50, with_labels=True)

nx.draw(G4, pos=pos, arrowsize=50, arrowstyle='<|-', with_labels=True)
    """
    pc.copy(text)
    
    
def two_drawing_5():
    """
5. Создайте граф Les Miserables. Для каждого узла зафиксируйте координату на плоскости при помощи функции random_layout. Получите список edge_colors значений атрибута weight на ребрах и ограничьте (clip) его сверху значением 10, снизу значением 1.

Отрисуйте узлы графа при помощи функции draw_networkx_nodes с указанием размера узлов 10. На тот рисунок добавьте ребра графа при помощи функции draw_networkx_edges, раскрасив ребра в цвет, соответствующий значениям edge_colors с применением цветовой карты viridis. Добавьте на визуализацию colorbar, показывающий градиент цветов ребер.

G5 = nx.les_miserables_graph()
pos = nx.random_layout(G5)
edge_colors = [attr["weight"] for _, _, attr in G5.edges(data=True)]
edge_colors = np.clip(edge_colors, a_min=1, a_max=10)
edge_colors

plt.figure(figsize=(10, 8))
nx.draw_networkx_nodes(G5, pos=pos, node_size=10);

fig, ax = plt.subplots(figsize=(10, 8))

nx.draw_networkx_nodes(G5, pos=pos, node_size=10)
nx.draw_networkx_edges(G5, pos, edge_color=edge_colors, edge_cmap=plt.cm.viridis)

plt.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=plt.Normalize(vmin=1, vmax=10)), ax=ax)
plt.show()
    """
    text = """
G5 = nx.les_miserables_graph()
pos = nx.random_layout(G5)
edge_colors = [attr["weight"] for _, _, attr in G5.edges(data=True)]
edge_colors = np.clip(edge_colors, a_min=1, a_max=10)
edge_colors

plt.figure(figsize=(10, 8))
nx.draw_networkx_nodes(G5, pos=pos, node_size=10);

fig, ax = plt.subplots(figsize=(10, 8))

nx.draw_networkx_nodes(G5, pos=pos, node_size=10)
nx.draw_networkx_edges(G5, pos, edge_color=edge_colors, edge_cmap=plt.cm.viridis)

plt.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=plt.Normalize(vmin=1, vmax=10)), ax=ax)
plt.show()
    """
    pc.copy(text)
    
    
def three_degree_1():
    """
1. Создайте неориентированный граф на основе набора данных fb-pages-food с сайта https://networkrepository.com/. Добавьте узлам атрибут name, который содержит имя человека. Удалите из графа все петли. Получите матрицу смежности графа и посчитайте степень каждого узла на основе этой матрицы. Выведите имя человека, чей узел имеет максимальную степень, и само значение степени.

G1 = nx.Graph()

nodes_path = 'data/fb-pages-food.nodes'
with open(nodes_path, 'r') as f:
    next(f)
    for line in f:
        splited = line.strip().split(',')
        new_id_node = int(splited[-1])
        G1.add_node(new_id_node)
        G1.nodes[new_id_node]['name'] = splited[1]

edges_path = 'data/fb-pages-food.edges'
with open(edges_path, 'r') as f:
    next(f)
    for line in f:
        node1, node2 = map(int, line.strip().split(','))
        G1.add_edge(node1, node2)
        
G1.remove_edges_from(nx.selfloop_edges(G1)) # Удаление петлей

pos = nx.spring_layout(G1)

G1.nodes(data=True)

nx.draw_networkx(G1, pos=pos, node_size=50, with_labels=False)

A = nx.adjacency_matrix(G1).toarray()

degrees_G1 = A.sum(axis=0)
display(degrees_G1)
degrees_G1.shape

max_degree = degrees_G1.max()
print("Максимальная степень:", max_degree)

adj_max_degree_ind = list(degrees_G1).index(max_degree)
node_id_max_degree = list(G1.nodes)[adj_max_degree_ind]
print("Имя человека с макс. степенью:", G1.nodes(data=True)[node_id_max_degree]["name"])
    """
    text = """
G1 = nx.Graph()

nodes_path = 'data/fb-pages-food.nodes'
with open(nodes_path, 'r') as f:
    next(f)
    for line in f:
        splited = line.strip().split(',')
        new_id_node = int(splited[-1])
        G1.add_node(new_id_node)
        G1.nodes[new_id_node]['name'] = splited[1]

edges_path = 'data/fb-pages-food.edges'
with open(edges_path, 'r') as f:
    next(f)
    for line in f:
        node1, node2 = map(int, line.strip().split(','))
        G1.add_edge(node1, node2)
        
G1.remove_edges_from(nx.selfloop_edges(G1)) # Удаление петлей

pos = nx.spring_layout(G1)

G1.nodes(data=True)

nx.draw_networkx(G1, pos=pos, node_size=50, with_labels=False)

A = nx.adjacency_matrix(G1).toarray()

degrees_G1 = A.sum(axis=0)
display(degrees_G1)
degrees_G1.shape

max_degree = degrees_G1.max()
print("Максимальная степень:", max_degree)

adj_max_degree_ind = list(degrees_G1).index(max_degree)
node_id_max_degree = list(G1.nodes)[adj_max_degree_ind]
print("Имя человека с макс. степенью:", G1.nodes(data=True)[node_id_max_degree]["name"])
    """
    pc.copy(text)
    
    
def three_degree_2():
    """
2. Используя граф из предыдущего задания, посчитайте степени каждого узла, используя возможности графа nx.Graph. Посчитайте и выведите на экран среднюю степень узлов графа, округленную до ближайшего целого.

G1_degrees_2 = G1.degree
G1_degrees_2

G1_degrees_2 = dict(G1_degrees_2)

print("Средняя степень узлов графа:", np.round(np.mean(list(G1_degrees_2.values()))))
    """
    text = """
    G1_degrees_2 = G1.degree
G1_degrees_2

G1_degrees_2 = dict(G1_degrees_2)

print("Средняя степень узлов графа:", np.round(np.mean(list(G1_degrees_2.values()))))
    """
    pc.copy(text)
    
    
def three_degree_3():
    """
3. Создайте ориентированный граф на основе набора данных fb-pages-food с сайта https://networkrepository.com/. Удалите из графа все петли. Получите матрицу смежности графа и посчитайте входящую и исходящую степень каждого узла на основе этой матрицы. Найдите и выведите на экран максимальное значение исходящих степеней и среднее значение входящих степеней.

G3 = nx.DiGraph()

nodes_path = 'data/fb-pages-food.nodes'
with open(nodes_path, 'r') as f:
    next(f)
    for line in f:
        splited = line.strip().split(',')
        new_id_node = int(splited[-1])
        G3.add_node(new_id_node)
        G3.nodes[new_id_node]['name'] = splited[1]

edges_path = 'data/fb-pages-food.edges'
with open(edges_path, 'r') as f:
    next(f)
    for line in f:
        node1, node2 = map(int, line.strip().split(','))
        G3.add_edge(node1, node2)
        
G3.remove_edges_from(nx.selfloop_edges(G3)) # Удаление петлей

pos = nx.spring_layout(G3)

nx.draw_networkx(G3, pos=pos, node_size=50, with_labels=False, node_color="lightblue", arrowsize=5)
A = nx.adjacency_matrix(G3).toarray()

in_degrees = A.sum(axis=0)
out_degrees = A.sum(axis=1)

print("Максимальное значение исходящих степеней:", out_degrees.max())
print("Среднее значение входящих степеней:", in_degrees.mean())
    """
    pc.copy(text)


def three_degree_4():
    """
    4. Постройте графики выборочной функции распределения входящих и исходящих степеней узлов для загруженного ориентированного графа. Для нахождения степеней узлов воспользуйтесь возможностями графа nx.DiGraph. Добавьте графикам названия, а осям - подписи. Расположите графики рядом по горизонтали.
    
in_degrees = dict(G3.in_degree())
out_degrees = dict(G3.out_degree())

in_degree_values = sorted(in_degrees.values())
out_degree_values = sorted(out_degrees.values())
print(in_degree_values)

in_edf = np.arange(1, len(in_degree_values) + 1) / len(in_degree_values)
out_edf = np.arange(1, len(out_degree_values) + 1) / len(out_degree_values)

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

axes[0].step(in_degree_values, in_edf, where="post")
axes[0].set_title("EDF входящих степеней")
axes[0].set_xlabel("Входящая степень узлов")
axes[0].set_ylabel("Кумулятивная вероятность")
axes[0].grid(True)


axes[1].step(out_degree_values, out_edf, where="post", color='green')
axes[1].set_title("EDF исходящих степеней")
axes[1].set_xlabel("Исходящая степень узлов")
axes[1].set_ylabel("Кумулятивная вероятность")
axes[1].grid(True)

plt.tight_layout()
plt.show()
    """
    text = """
in_degrees = dict(G3.in_degree())
out_degrees = dict(G3.out_degree())

in_degree_values = sorted(in_degrees.values())
out_degree_values = sorted(out_degrees.values())
print(in_degree_values)

in_edf = np.arange(1, len(in_degree_values) + 1) / len(in_degree_values)
out_edf = np.arange(1, len(out_degree_values) + 1) / len(out_degree_values)

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

axes[0].step(in_degree_values, in_edf, where="post")
axes[0].set_title("EDF входящих степеней")
axes[0].set_xlabel("Входящая степень узлов")
axes[0].set_ylabel("Кумулятивная вероятность")
axes[0].grid(True)


axes[1].step(out_degree_values, out_edf, where="post", color='green')
axes[1].set_title("EDF исходящих степеней")
axes[1].set_xlabel("Исходящая степень узлов")
axes[1].set_ylabel("Кумулятивная вероятность")
axes[1].grid(True)

plt.tight_layout()
plt.show()
    """
    pc.copy(text)
    

def three_degree_5():
    """
5. Визуализируйте закон распределения степеней узлов для неориентированного варианта графа в обычной и логарифимических (по 1й и по 2м осям) системах координат для неориентированного графа. Расположите графики в виде сетки 2х2. Добавьте графикам названия, а осям - подписи. Использование готовых функций для создания и визуализации гистограмм не допускается.

degree = dict(G1.degree())
degree_values = sorted(set(degree.values()))
print(degree_values)

degree_counts = [list(degree.values()).count(x) for x in degree_values]
print(degree_counts)

degree_counts_freq = [d / sum(degree_counts) for d in degree_counts]

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

axes[0, 0].bar(degree_values, degree_counts_freq, edgecolor='black')
axes[0, 0].set_title('Закон распределения степеней узлов')
axes[0, 0].set_xlabel('Степень узла')
axes[0, 0].set_ylabel('Вероятность')
axes[0, 0].grid(True)

axes[0, 1].bar(degree_values, degree_counts_freq, color='lightblue', edgecolor='black')
axes[0, 1].set_title('Закон распределения степеней узлов (log(X))')
axes[0, 1].set_xlabel('log(Степень узла)')
axes[0, 1].set_ylabel('Вероятность')
axes[0, 1].set_xscale("log")
axes[0, 1].grid(True)

axes[1, 0].bar(degree_values, degree_counts_freq, color='red', alpha=0.6, edgecolor='black')
axes[1, 0].set_title('Закон распределения степеней узлов (log(Y))')
axes[1, 0].set_xlabel('Степень узла')
axes[1, 0].set_ylabel('log(Вероятность)')
axes[1, 0].set_yscale("log")
axes[1, 0].grid(True)

axes[1, 1].bar(degree_values, degree_counts_freq, color='green', alpha=0.6, edgecolor='black')
axes[1, 1].set_title('Закон распределения степеней узлов (log(X) и log(Y))')
axes[1, 1].set_xlabel('log(Степень узла)')
axes[1, 1].set_ylabel('log(Вероятность)')
axes[1, 1].set_xscale("log")
axes[1, 1].set_yscale("log")
axes[1, 1].grid(True)

plt.tight_layout()
plt.show()
    """
    text = """
degree = dict(G1.degree())
degree_values = sorted(set(degree.values()))
print(degree_values)

degree_counts = [list(degree.values()).count(x) for x in degree_values]
print(degree_counts)

degree_counts_freq = [d / sum(degree_counts) for d in degree_counts]

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

axes[0, 0].bar(degree_values, degree_counts_freq, edgecolor='black')
axes[0, 0].set_title('Закон распределения степеней узлов')
axes[0, 0].set_xlabel('Степень узла')
axes[0, 0].set_ylabel('Вероятность')
axes[0, 0].grid(True)

axes[0, 1].bar(degree_values, degree_counts_freq, color='lightblue', edgecolor='black')
axes[0, 1].set_title('Закон распределения степеней узлов (log(X))')
axes[0, 1].set_xlabel('log(Степень узла)')
axes[0, 1].set_ylabel('Вероятность')
axes[0, 1].set_xscale("log")
axes[0, 1].grid(True)

axes[1, 0].bar(degree_values, degree_counts_freq, color='red', alpha=0.6, edgecolor='black')
axes[1, 0].set_title('Закон распределения степеней узлов (log(Y))')
axes[1, 0].set_xlabel('Степень узла')
axes[1, 0].set_ylabel('log(Вероятность)')
axes[1, 0].set_yscale("log")
axes[1, 0].grid(True)

axes[1, 1].bar(degree_values, degree_counts_freq, color='green', alpha=0.6, edgecolor='black')
axes[1, 1].set_title('Закон распределения степеней узлов (log(X) и log(Y))')
axes[1, 1].set_xlabel('log(Степень узла)')
axes[1, 1].set_ylabel('log(Вероятность)')
axes[1, 1].set_xscale("log")
axes[1, 1].set_yscale("log")
axes[1, 1].grid(True)

plt.tight_layout()
plt.show()
    """
    pc.copy(text)
    
    
def three_degree_6():
    """
6. Сгенерируйте выборку из Дзета-распределения (распределение Ципфа) с числом наблюдений 10 тыс. и параметром распределения  𝛼=2
 . Визуализируйте эмпирический закон распределения с использованием линейного и логарифмического биннинга. Визуализируйте оба варианта в линейных и логарифмических (log-log) координатах (должно получиться 4 изображения).

Zipf = np.random.zipf(a=2, size=10_000)
Zipf = Zipf[(Zipf < 50)]
Zipf.shape

fig, axes = plt.subplots(2, 2, figsize=(12, 10))
n_bins = 50

axes[0, 0].hist(Zipf, bins=n_bins, color='blue', edgecolor='black', density=True)
axes[0, 0].set_title('Linear binning (linear coords)')
axes[0, 0].set_xlabel('Значение')
axes[0, 0].set_ylabel('Плотность вероятности')
axes[0, 0].set_ylim(0, 0.2)

axes[0, 1].hist(Zipf, bins=n_bins, color='lightblue', edgecolor='black', density=True)
axes[0, 1].set_title('Linear binning (log-log coords)')
axes[0, 1].set_xlabel('log(Значение)')
axes[0, 1].set_ylabel('log(Плотность вероятности)')
axes[0, 1].set_yscale("log")
axes[0, 1].set_xscale("log")

log_bins = np.logspace(np.log10(1), np.log10(max(Zipf)), n_bins)
axes[1, 0].hist(Zipf, bins=log_bins, color='red', edgecolor='black', density=True)
axes[1, 0].set_title('Log binning (linear coords)')
axes[1, 0].set_xlabel('Значение')
axes[1, 0].set_ylabel('Плотность вероятности')
axes[1, 0].set_ylim(0, 0.1)

axes[1, 1].hist(Zipf, bins=log_bins, color='green', edgecolor='black', density=True)
axes[1, 1].set_title('Log binning (log-log coords)')
axes[1, 1].set_xlabel('log(Значение)')
axes[1, 1].set_ylabel('log(Плотность вероятности)')
axes[1, 1].set_yscale("log")
axes[1, 1].set_xscale("log")

plt.tight_layout()
plt.show()
    """
    text = """
Zipf = np.random.zipf(a=2, size=10_000)
Zipf = Zipf[(Zipf < 50)]
Zipf.shape

fig, axes = plt.subplots(2, 2, figsize=(12, 10))
n_bins = 50

axes[0, 0].hist(Zipf, bins=n_bins, color='blue', edgecolor='black', density=True)
axes[0, 0].set_title('Linear binning (linear coords)')
axes[0, 0].set_xlabel('Значение')
axes[0, 0].set_ylabel('Плотность вероятности')
axes[0, 0].set_ylim(0, 0.2)

axes[0, 1].hist(Zipf, bins=n_bins, color='lightblue', edgecolor='black', density=True)
axes[0, 1].set_title('Linear binning (log-log coords)')
axes[0, 1].set_xlabel('log(Значение)')
axes[0, 1].set_ylabel('log(Плотность вероятности)')
axes[0, 1].set_yscale("log")
axes[0, 1].set_xscale("log")

log_bins = np.logspace(np.log10(1), np.log10(max(Zipf)), n_bins)
axes[1, 0].hist(Zipf, bins=log_bins, color='red', edgecolor='black', density=True)
axes[1, 0].set_title('Log binning (linear coords)')
axes[1, 0].set_xlabel('Значение')
axes[1, 0].set_ylabel('Плотность вероятности')
axes[1, 0].set_ylim(0, 0.1)

axes[1, 1].hist(Zipf, bins=log_bins, color='green', edgecolor='black', density=True)
axes[1, 1].set_title('Log binning (log-log coords)')
axes[1, 1].set_xlabel('log(Значение)')
axes[1, 1].set_ylabel('log(Плотность вероятности)')
axes[1, 1].set_yscale("log")
axes[1, 1].set_xscale("log")

plt.tight_layout()
plt.show()
    """
    pc.copy(text)
    
    
def four_models_1():
    """
1. Используя реализацию модели предпочтительного присоединения Барабаши-Альберта из networkx, постройте сеть с кол-вом узлов n > 50. Визуализируйте полученный граф, сделайте размер и цвет узлов пропорциональными их степени. Постройте график выборочной функции распределения степеней узлов для сети.

n = np.random.randint(50, 100)
m = 2
G1 = nx.barabasi_albert_graph(n=n, m=m)

degrees_values = np.array(list(dict(G1.degree).values()))
nx.draw(G1, node_size=30 * degrees_values, node_color=degrees_values)

edf = np.arange(1, len(degrees_values) + 1) / len(degrees_values)

plt.figure(figsize=(12, 8))
plt.step(sorted(degrees_values), edf, where="post")
plt.title("EDF степеней")
plt.xlabel("Степень узлов")
plt.ylabel("Кумулятивная вероятность")
plt.grid(True)

plt.show()
    """
    text = """
n = np.random.randint(50, 100)
m = 2
G1 = nx.barabasi_albert_graph(n=n, m=m)

degrees_values = np.array(list(dict(G1.degree).values()))
nx.draw(G1, node_size=30 * degrees_values, node_color=degrees_values)

edf = np.arange(1, len(degrees_values) + 1) / len(degrees_values)

plt.figure(figsize=(12, 8))
plt.step(sorted(degrees_values), edf, where="post")
plt.title("EDF степеней")
plt.xlabel("Степень узлов")
plt.ylabel("Кумулятивная вероятность")
plt.grid(True)

plt.show()
    """
    pc.copy(text)
    
    
def four_models_2():
    """
2. Используя реализацию модели предпочтительного присоединения Барабаши-Альберта из networkx, постройте сеть с кол-вом узлов n=10000. Визуализируйте закон распределения степеней узлов сети в обычной и логарифимических (по 1й и по 2м осям) системах координат.

n = 10_000
m = 2
G2 = nx.barabasi_albert_graph(n=n, m=m)

degree = dict(G2.degree)
degree_values = sorted(set(list(degree.values())))
print(degrees_values)

degree_counts = [list(degree.values()).count(x) for x in degree_values]
print(degree_counts)

degree_counts_freq = [d / sum(degree_counts) for d in degree_counts]

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

axes[0, 0].scatter(degree_values, degree_counts_freq, edgecolor='black')
axes[0, 0].set_title('Закон распределения степеней узлов')
axes[0, 0].set_xlabel('Степень узла')
axes[0, 0].set_ylabel('Вероятность')
axes[0, 0].grid(True)

axes[0, 1].scatter(degree_values, degree_counts_freq, color='lightblue', edgecolor='black')
axes[0, 1].set_title('Закон распределения степеней узлов (log(X))')
axes[0, 1].set_xlabel('log(Степень узла)')
axes[0, 1].set_ylabel('Вероятность')
axes[0, 1].set_xscale("log")
axes[0, 1].grid(True)

axes[1, 0].scatter(degree_values, degree_counts_freq, color='red', alpha=0.8, edgecolor='black')
axes[1, 0].set_title('Закон распределения степеней узлов (log(Y))')
axes[1, 0].set_xlabel('Степень узла')
axes[1, 0].set_ylabel('log(Вероятность)')
axes[1, 0].set_yscale("log")
axes[1, 0].grid(True)

axes[1, 1].scatter(degree_values, degree_counts_freq, color='green', alpha=0.8, edgecolor='black')
axes[1, 1].set_title('Закон распределения степеней узлов (log(X) и log(Y))')
axes[1, 1].set_xlabel('log(Степень узла)')
axes[1, 1].set_ylabel('log(Вероятность)')
axes[1, 1].set_xscale("log")
axes[1, 1].set_yscale("log")
axes[1, 1].grid(True)

plt.tight_layout()
plt.show()
    """
    text = """
    n = 10_000
m = 2
G2 = nx.barabasi_albert_graph(n=n, m=m)

degree = dict(G2.degree)
degree_values = sorted(set(list(degree.values())))
print(degrees_values)

degree_counts = [list(degree.values()).count(x) for x in degree_values]
print(degree_counts)

degree_counts_freq = [d / sum(degree_counts) for d in degree_counts]

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

axes[0, 0].scatter(degree_values, degree_counts_freq, edgecolor='black')
axes[0, 0].set_title('Закон распределения степеней узлов')
axes[0, 0].set_xlabel('Степень узла')
axes[0, 0].set_ylabel('Вероятность')
axes[0, 0].grid(True)

axes[0, 1].scatter(degree_values, degree_counts_freq, color='lightblue', edgecolor='black')
axes[0, 1].set_title('Закон распределения степеней узлов (log(X))')
axes[0, 1].set_xlabel('log(Степень узла)')
axes[0, 1].set_ylabel('Вероятность')
axes[0, 1].set_xscale("log")
axes[0, 1].grid(True)

axes[1, 0].scatter(degree_values, degree_counts_freq, color='red', alpha=0.8, edgecolor='black')
axes[1, 0].set_title('Закон распределения степеней узлов (log(Y))')
axes[1, 0].set_xlabel('Степень узла')
axes[1, 0].set_ylabel('log(Вероятность)')
axes[1, 0].set_yscale("log")
axes[1, 0].grid(True)

axes[1, 1].scatter(degree_values, degree_counts_freq, color='green', alpha=0.8, edgecolor='black')
axes[1, 1].set_title('Закон распределения степеней узлов (log(X) и log(Y))')
axes[1, 1].set_xlabel('log(Степень узла)')
axes[1, 1].set_ylabel('log(Вероятность)')
axes[1, 1].set_xscale("log")
axes[1, 1].set_yscale("log")
axes[1, 1].grid(True)

plt.tight_layout()
plt.show()
    """
    pc.copy(text)
    

def four_models_3():
    """
3. Визуализируйте закон распределения степеней узлов с использованием log-binning для сети из предыдущего задания. Для сравнения на этом же графике изобразите график закона распределения без использования log-binning в log-log координатах.

lst_degrees = list(degree.values())
n_bins = 50
log_bins = np.logspace(np.log10(min(lst_degrees)), np.log10(max(lst_degrees)), n_bins)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))

axes[0].hist(lst_degrees, bins=n_bins, edgecolor='black', density=True)
axes[0].set_title('Linear binning (log-log coords)')
axes[0].set_xlabel('log(Степень узла)')
axes[0].set_ylabel('log(Плотность вероятности)')
axes[0].set_yscale("log")
axes[0].set_xscale("log")

axes[1].hist(lst_degrees, bins=log_bins, color='green', edgecolor='black', density=True)
axes[1].set_title('Log binning (log-log coords)')
axes[1].set_xlabel('log(Степень узла)')
axes[1].set_ylabel('log(Плотность вероятности)')
axes[1].set_yscale("log")
axes[1].set_xscale("log")

plt.tight_layout()
plt.show()

y_lienar, bin_linear = np.histogram(lst_degrees, bins=n_bins, density=True)
bin_center_linear = (bin_linear[:-1] + bin_linear[1:])/ 2

y_log, bin_log = np.histogram(lst_degrees, bins=log_bins, density=True)
bin_center_log = (bin_log[:-1] + bin_log[1:])/ 2

plt.figure(figsize=(8, 6))

plt.scatter(bin_center_linear, y_lienar, label='Linear-binning', alpha=1)
plt.scatter(bin_center_log, y_log, color='green', label='Log-binning', alpha=1)

plt.title('Linear и log-binning (log-log coords)')
plt.xlabel('log(Степень узла)')
plt.ylabel('log(Плотность вероятности)')
plt.xscale('log')
plt.yscale('log')
plt.legend()
plt.grid(True)

plt.show()
    """
    text = """
lst_degrees = list(degree.values())
n_bins = 50
log_bins = np.logspace(np.log10(min(lst_degrees)), np.log10(max(lst_degrees)), n_bins)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))

axes[0].hist(lst_degrees, bins=n_bins, edgecolor='black', density=True)
axes[0].set_title('Linear binning (log-log coords)')
axes[0].set_xlabel('log(Степень узла)')
axes[0].set_ylabel('log(Плотность вероятности)')
axes[0].set_yscale("log")
axes[0].set_xscale("log")

axes[1].hist(lst_degrees, bins=log_bins, color='green', edgecolor='black', density=True)
axes[1].set_title('Log binning (log-log coords)')
axes[1].set_xlabel('log(Степень узла)')
axes[1].set_ylabel('log(Плотность вероятности)')
axes[1].set_yscale("log")
axes[1].set_xscale("log")

plt.tight_layout()
plt.show()

y_lienar, bin_linear = np.histogram(lst_degrees, bins=n_bins, density=True)
bin_center_linear = (bin_linear[:-1] + bin_linear[1:])/ 2

y_log, bin_log = np.histogram(lst_degrees, bins=log_bins, density=True)
bin_center_log = (bin_log[:-1] + bin_log[1:])/ 2

plt.figure(figsize=(8, 6))

plt.scatter(bin_center_linear, y_lienar, label='Linear-binning', alpha=1)
plt.scatter(bin_center_log, y_log, color='green', label='Log-binning', alpha=1)

plt.title('Linear и log-binning (log-log coords)')
plt.xlabel('log(Степень узла)')
plt.ylabel('log(Плотность вероятности)')
plt.xscale('log')
plt.yscale('log')
plt.legend()
plt.grid(True)

plt.show()
    """
    pc.copy(text)
    
    
def four_models_4():
    """
4. C помощью реализации модели Уотса-Строгатса из networkx сгенерируйте сети с кол-вом узлов порядка 1000 и с разным p. В качестве p рассмотрите несколько значений, равномерно распределенных по логарифмической шкале на отрезке  [10−4,1].
Визуализируйте результаты пересвязывания, построив на одном графике относительные изменения среднего коэффициента кластеризации и средней длины пути относительно варианта сети без пересвязывания. Обозначьте точки разных графиков различными маркерами и цветами. Добавьте легенду.

n = 16
k = 4
G3 = nx.watts_strogatz_graph(n, k, p=0)
pos = nx.circular_layout(G3)
nx.draw(G3, pos=pos, with_labels=True)

G3 = nx.watts_strogatz_graph(n, k, p=0.25)
nx.draw(G3, pos=pos, with_labels=True)

n = 1000
k = 4
p_values = np.logspace(-4, 0, num=14)

G_0 = nx.watts_strogatz_graph(n=n, k=k, p=0)
clustering_0 = nx.average_clustering(G_0)
length_0 = nx.average_shortest_path_length(G_0)
clustering_0, length_0

clusters = []
lengths = []

for p in p_values:
    G = nx.watts_strogatz_graph(n, k, p)
    clustering = nx.average_clustering(G)
    length = nx.average_shortest_path_length(G)
    
    clusters.append(clustering / clustering_0)
    lengths.append(length / length_0)

plt.figure(figsize=(10, 6))
plt.plot(p_values, clusters, 'o-', label='C/C₀', linewidth=1.5)
plt.plot(p_values, lengths, 'o-', label='L/L₀', linewidth=1.5)

plt.xlabel('log(p)')
plt.ylabel('Отношения')
plt.title('Зависимость C/C0 и L/L0 от p в модели Уоттса-Строгатца')
plt.xscale('log')
plt.grid(True)
plt.legend()
plt.show()
    """
    text = """
n = 16
k = 4
G3 = nx.watts_strogatz_graph(n, k, p=0)
pos = nx.circular_layout(G3)
nx.draw(G3, pos=pos, with_labels=True)

G3 = nx.watts_strogatz_graph(n, k, p=0.25)
nx.draw(G3, pos=pos, with_labels=True)

n = 1000
k = 4
p_values = np.logspace(-4, 0, num=14)

G_0 = nx.watts_strogatz_graph(n=n, k=k, p=0)
clustering_0 = nx.average_clustering(G_0)
length_0 = nx.average_shortest_path_length(G_0)
clustering_0, length_0

clusters = []
lengths = []

for p in p_values:
    G = nx.watts_strogatz_graph(n, k, p)
    clustering = nx.average_clustering(G)
    length = nx.average_shortest_path_length(G)
    
    clusters.append(clustering / clustering_0)
    lengths.append(length / length_0)

plt.figure(figsize=(10, 6))
plt.plot(p_values, clusters, 'o-', label='C/C₀', linewidth=1.5)
plt.plot(p_values, lengths, 'o-', label='L/L₀', linewidth=1.5)

plt.xlabel('log(p)')
plt.ylabel('Отношения')
plt.title('Зависимость C/C0 и L/L0 от p в модели Уоттса-Строгатца')
plt.xscale('log')
plt.grid(True)
plt.legend()
plt.show()
    """
    pc.copy(text)
    
    
def four_models_5():
    """
5. Создайте неориентированный граф на основе набора данных fb-pages-food с сайта https://networkrepository.com/ . C помощью nx.configuration_model постройте рандомизированный аналог данной сети. Визуализируйте исходный и рандомизированный граф рядом. Проверьте, совпадают ли распределения степеней узлов исходного и рандомизированного графа.

G5 = nx.Graph()

nodes_path = 'data/fb-pages-food.nodes'
with open(nodes_path, 'r') as f:
    next(f)
    for line in f:
        splited = line.strip().split(',')
        new_id_node = int(splited[-1])
        G5.add_node(new_id_node)
        G5.nodes[new_id_node]['name'] = splited[1]

edges_path = 'data/fb-pages-food.edges'
with open(edges_path, 'r') as f:
    next(f)
    for line in f:
        node1, node2 = map(int, line.strip().split(','))
        G5.add_edge(node1, node2)
        
# G5.remove_edges_from(nx.selfloop_edges(G5)) # Удаление петлей

degrees_G5 = list(dict(G5.degree).values())
G5_conf = nx.configuration_model(deg_sequence=degrees_G5)
# G5_conf.remove_edges_from(nx.selfloop_edges(G5_conf))

plt.figure(figsize=(12, 5))

plt.subplot(121)
pos_G5 = nx.spring_layout(G5)
nx.draw(G5, pos_G5, node_size=20, edge_color='gray', with_labels=False)
plt.title('Исходный граф G5')

plt.subplot(122)
nx.draw(G5_conf, pos_G5, node_size=20, node_color='red', edge_color='gray', with_labels=False)
plt.title('Рандомизированный граф G5_conf')

plt.tight_layout()
plt.show()

degree = dict(G5.degree)
degree_values = sorted(set(list(degree.values())))
degree_counts = [list(degree.values()).count(x) for x in degree_values]
degree_counts_freq = [d / sum(degree_counts) for d in degree_counts]

degree_conf = dict(G5_conf.degree)
degree_values_conf = sorted(set(list(degree_conf.values())))
degree_counts_conf = [list(degree_conf.values()).count(x) for x in degree_values_conf]
degree_counts_freq_conf = [d / sum(degree_counts_conf) for d in degree_counts_conf]

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

axes[0, 0].scatter(degree_values, degree_counts_freq, edgecolor='black')
axes[0, 0].set_title('Распределение степеней узлов (исходный граф)')
axes[0, 0].set_xlabel('Степень узла')
axes[0, 0].set_ylabel('Вероятность')
axes[0, 0].grid(True)

axes[0, 1].scatter(degree_values_conf, degree_counts_freq_conf, color='green', edgecolor='black')
axes[0, 1].set_title('Распределение степеней узлов (рандомизированный граф)')
axes[0, 1].set_xlabel('Степень узла')
axes[0, 1].set_ylabel('Вероятность')
axes[0, 1].grid(True)

axes[1, 0].scatter(degree_values, degree_counts_freq, edgecolor='black')
axes[1, 0].set_title('Распределение степеней узлов (исходный граф, log-log)')
axes[1, 0].set_xlabel('log(Степень узла)')
axes[1, 0].set_ylabel('log(Вероятность)')
axes[1, 0].set_xscale("log")
axes[1, 0].set_yscale("log")
axes[1, 0].grid(True)

axes[1, 1].scatter(degree_values_conf, degree_counts_freq_conf, color='green', edgecolor='black')
axes[1, 1].set_title('Распределение степеней узлов (рандомизированный граф, log-log)')
axes[1, 1].set_xlabel('log(Степень узла)')
axes[1, 1].set_ylabel('log(Вероятность)')
axes[1, 1].set_xscale("log")
axes[1, 1].set_yscale("log")
axes[1, 1].grid(True)

plt.tight_layout()
plt.show()
    """
    text = """
    G5 = nx.Graph()

nodes_path = 'data/fb-pages-food.nodes'
with open(nodes_path, 'r') as f:
    next(f)
    for line in f:
        splited = line.strip().split(',')
        new_id_node = int(splited[-1])
        G5.add_node(new_id_node)
        G5.nodes[new_id_node]['name'] = splited[1]

edges_path = 'data/fb-pages-food.edges'
with open(edges_path, 'r') as f:
    next(f)
    for line in f:
        node1, node2 = map(int, line.strip().split(','))
        G5.add_edge(node1, node2)
        
# G5.remove_edges_from(nx.selfloop_edges(G5)) # Удаление петлей

degrees_G5 = list(dict(G5.degree).values())
G5_conf = nx.configuration_model(deg_sequence=degrees_G5)
# G5_conf.remove_edges_from(nx.selfloop_edges(G5_conf))

plt.figure(figsize=(12, 5))

plt.subplot(121)
pos_G5 = nx.spring_layout(G5)
nx.draw(G5, pos_G5, node_size=20, edge_color='gray', with_labels=False)
plt.title('Исходный граф G5')

plt.subplot(122)
nx.draw(G5_conf, pos_G5, node_size=20, node_color='red', edge_color='gray', with_labels=False)
plt.title('Рандомизированный граф G5_conf')

plt.tight_layout()
plt.show()

degree = dict(G5.degree)
degree_values = sorted(set(list(degree.values())))
degree_counts = [list(degree.values()).count(x) for x in degree_values]
degree_counts_freq = [d / sum(degree_counts) for d in degree_counts]

degree_conf = dict(G5_conf.degree)
degree_values_conf = sorted(set(list(degree_conf.values())))
degree_counts_conf = [list(degree_conf.values()).count(x) for x in degree_values_conf]
degree_counts_freq_conf = [d / sum(degree_counts_conf) for d in degree_counts_conf]

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

axes[0, 0].scatter(degree_values, degree_counts_freq, edgecolor='black')
axes[0, 0].set_title('Распределение степеней узлов (исходный граф)')
axes[0, 0].set_xlabel('Степень узла')
axes[0, 0].set_ylabel('Вероятность')
axes[0, 0].grid(True)

axes[0, 1].scatter(degree_values_conf, degree_counts_freq_conf, color='green', edgecolor='black')
axes[0, 1].set_title('Распределение степеней узлов (рандомизированный граф)')
axes[0, 1].set_xlabel('Степень узла')
axes[0, 1].set_ylabel('Вероятность')
axes[0, 1].grid(True)

axes[1, 0].scatter(degree_values, degree_counts_freq, edgecolor='black')
axes[1, 0].set_title('Распределение степеней узлов (исходный граф, log-log)')
axes[1, 0].set_xlabel('log(Степень узла)')
axes[1, 0].set_ylabel('log(Вероятность)')
axes[1, 0].set_xscale("log")
axes[1, 0].set_yscale("log")
axes[1, 0].grid(True)

axes[1, 1].scatter(degree_values_conf, degree_counts_freq_conf, color='green', edgecolor='black')
axes[1, 1].set_title('Распределение степеней узлов (рандомизированный граф, log-log)')
axes[1, 1].set_xlabel('log(Степень узла)')
axes[1, 1].set_ylabel('log(Вероятность)')
axes[1, 1].set_xscale("log")
axes[1, 1].set_yscale("log")
axes[1, 1].grid(True)

plt.tight_layout()
plt.show()
    """
    pc.copy(text)
    
    
def four_models_6():
    """
6. C помощью реализации алгоритма постоения случайного геометрического графа из networkx сгенерируйте сеть с кол-вом узлов порядка 500 при радиусе r=0.2 и  𝐿2
  метрикой для вычисления расстояния. Визуализируйте полученный граф, уменьшив размер узлов и сделав цвет ребер черным прозрачным (используйте RGBA). Визуализируйте распределение степеней узлов полученного графа.
Повторите решение задачи, использовав  𝐿1
  метрику для вычисления расстояния. Сравните результаты и сделайте выводы.
  
n = 500
r = 0.2
G6_L2 = nx.random_geometric_graph(n, r, p=2)
print(G6_L2.nodes(data=True)[0]) # pos есть

plt.figure(figsize=(8, 8))
pos = nx.get_node_attributes(G6_L2, 'pos')
nx.draw(G6_L2, pos=pos, node_size=10, node_color='blue', 
        edge_color=(0, 0, 0, 0.1),
        with_labels=False)
plt.title('L2 метрика')
plt.show()

degree = dict(G6_L2.degree)
degree_values = sorted(set(list(degree.values())))
degree_counts = [list(degree.values()).count(x) for x in degree_values]
degree_counts_freq = [d / sum(degree_counts) for d in degree_counts]

plt.figure(figsize=(6,4))

plt.scatter(degree_values, degree_counts_freq, edgecolor='black')
plt.title('Распределения степеней узлов, L2')
plt.xlabel('Степень узла')
plt.ylabel('Вероятность')
plt.grid(True)
plt.show()

G6_L1 = nx.random_geometric_graph(n, r, p=1)

plt.figure(figsize=(8, 8))
pos = nx.get_node_attributes(G6_L1, 'pos')
nx.draw(G6_L1, pos=pos, node_size=10, node_color='red', 
        edge_color=(0, 0, 0, 0.1),
        with_labels=False)
plt.title('L1 метрика')
plt.show()

degree = dict(G6_L1.degree)
degree_values = sorted(set(list(degree.values())))
degree_counts = [list(degree.values()).count(x) for x in degree_values]
degree_counts_freq = [d / sum(degree_counts) for d in degree_counts]

plt.figure(figsize=(6,4))

plt.scatter(degree_values, degree_counts_freq, edgecolor='black', color="red")
plt.title('Распределения степеней узлов, L1')
plt.xlabel('Степень узла')
plt.ylabel('Вероятность')
plt.grid(True)
plt.show()
    """
    text = """
n = 500
r = 0.2
G6_L2 = nx.random_geometric_graph(n, r, p=2)
print(G6_L2.nodes(data=True)[0]) # pos есть

plt.figure(figsize=(8, 8))
pos = nx.get_node_attributes(G6_L2, 'pos')
nx.draw(G6_L2, pos=pos, node_size=10, node_color='blue', 
        edge_color=(0, 0, 0, 0.1),
        with_labels=False)
plt.title('L2 метрика')
plt.show()

degree = dict(G6_L2.degree)
degree_values = sorted(set(list(degree.values())))
degree_counts = [list(degree.values()).count(x) for x in degree_values]
degree_counts_freq = [d / sum(degree_counts) for d in degree_counts]

plt.figure(figsize=(6,4))

plt.scatter(degree_values, degree_counts_freq, edgecolor='black')
plt.title('Распределения степеней узлов, L2')
plt.xlabel('Степень узла')
plt.ylabel('Вероятность')
plt.grid(True)
plt.show()

G6_L1 = nx.random_geometric_graph(n, r, p=1)

plt.figure(figsize=(8, 8))
pos = nx.get_node_attributes(G6_L1, 'pos')
nx.draw(G6_L1, pos=pos, node_size=10, node_color='red', 
        edge_color=(0, 0, 0, 0.1),
        with_labels=False)
plt.title('L1 метрика')
plt.show()

degree = dict(G6_L1.degree)
degree_values = sorted(set(list(degree.values())))
degree_counts = [list(degree.values()).count(x) for x in degree_values]
degree_counts_freq = [d / sum(degree_counts) for d in degree_counts]

plt.figure(figsize=(6,4))

plt.scatter(degree_values, degree_counts_freq, edgecolor='black', color="red")
plt.title('Распределения степеней узлов, L1')
plt.xlabel('Степень узла')
plt.ylabel('Вероятность')
plt.grid(True)
plt.show()
    """
    pc.copy(text)
    
    
def five_community_detection_1():
    """
1. Загрузите сеть Southern women с сайта http://konect.cc/, удалите из него петли и создайте не менее 5 разбиений этой сети на 2 сообщества (часть разбиений должны частично или полностью соответствовать интуитивным представлениям о разбиении на сообщества, часть - нет). Для создания разбиений не используйте специализированные алгоритмы. Визуализируйте графы, раскрасив узлы в цвет соответствующего им сообщества.

SW = nx.read_edgelist(path="data/out.opsahl-southernwomen", comments="%")
SW.remove_edges_from(nx.selfloop_edges(SW))
pos = nx.spring_layout(SW)
nx.draw(SW, pos=pos, with_labels=True)

# разбинение пополам по id узлов
SW_1_1 = range(1, len(SW) // 2 + 1)
SW_1_2 = range(len(SW) // 2 + 1, len(SW) + 1)
colors_1 = ["red" if int(node) in SW_1_1 else "green" for node in SW.nodes()]
nx.draw(SW, pos=pos, with_labels=True, node_color=colors_1)

# случайный выбор принадлежности узла к сообществу
SW_2_1 = np.random.randint(0, len(SW), size=len(SW)//2)
colors_2 = ["red" if int(node) in SW_2_1 else "green" for node in SW.nodes()]
nx.draw(SW, pos=pos, with_labels=True, node_color=colors_2)

# Одно сообщество - узлы со степенью более медианы, другое не более
degrees = dict(SW.degree())
degrees_median = np.median(list(degrees.values()))
print("Median", degrees_median)
colors_3 = ["red" if degrees[node] > degrees_median else "green" for node in SW.nodes()]
nx.draw(SW, pos=pos, with_labels=True, node_color=colors_3)

# в одном сообществе узлы, имеющие связь с узлом "7", во втором неимеющие
conn_nodes = set()
for edge in list(SW.edges()):
    if "7" in edge:
        conn_nodes.add(edge[0])
        conn_nodes.add(edge[1])
colors_4 = ["red" if node in conn_nodes else "green" for node in SW.nodes()]
nx.draw(SW, pos=pos, with_labels=True, node_color=colors_4)

# в одном сообществе узлы, имеющие связь с узлом "5", во втором неимеющие
conn_nodes = set()
for edge in list(SW.edges()):
    if "5" in edge:
        conn_nodes.add(edge[0])
        conn_nodes.add(edge[1])
colors_5 = ["red" if node in conn_nodes else "green" for node in SW.nodes()]
nx.draw(SW, pos=pos, with_labels=True, node_color=colors_5)
    """
    text = """
SW = nx.read_edgelist(path="data/out.opsahl-southernwomen", comments="%")
SW.remove_edges_from(nx.selfloop_edges(SW))
pos = nx.spring_layout(SW)
nx.draw(SW, pos=pos, with_labels=True)

# разбинение пополам по id узлов
SW_1_1 = range(1, len(SW) // 2 + 1)
SW_1_2 = range(len(SW) // 2 + 1, len(SW) + 1)
colors_1 = ["red" if int(node) in SW_1_1 else "green" for node in SW.nodes()]
nx.draw(SW, pos=pos, with_labels=True, node_color=colors_1)

# случайный выбор принадлежности узла к сообществу
SW_2_1 = np.random.randint(0, len(SW), size=len(SW)//2)
colors_2 = ["red" if int(node) in SW_2_1 else "green" for node in SW.nodes()]
nx.draw(SW, pos=pos, with_labels=True, node_color=colors_2)

# Одно сообщество - узлы со степенью более медианы, другое не более
degrees = dict(SW.degree())
degrees_median = np.median(list(degrees.values()))
print("Median", degrees_median)
colors_3 = ["red" if degrees[node] > degrees_median else "green" for node in SW.nodes()]
nx.draw(SW, pos=pos, with_labels=True, node_color=colors_3)

# в одном сообществе узлы, имеющие связь с узлом "7", во втором неимеющие
conn_nodes = set()
for edge in list(SW.edges()):
    if "7" in edge:
        conn_nodes.add(edge[0])
        conn_nodes.add(edge[1])
colors_4 = ["red" if node in conn_nodes else "green" for node in SW.nodes()]
nx.draw(SW, pos=pos, with_labels=True, node_color=colors_4)

# в одном сообществе узлы, имеющие связь с узлом "5", во втором неимеющие
conn_nodes = set()
for edge in list(SW.edges()):
    if "5" in edge:
        conn_nodes.add(edge[0])
        conn_nodes.add(edge[1])
colors_5 = ["red" if node in conn_nodes else "green" for node in SW.nodes()]
nx.draw(SW, pos=pos, with_labels=True, node_color=colors_5)
    """
    pc.copy(text)
    
    
def five_community_detection_2():
    """
2. Для разбиений из предыдущей задачи посчитайте следующие величины:

плотность первого сообщества
плотность второго сообщества
среднюю плотность сообществ.
модулярность
покрытие (coverage)
эффективность (performance)
Все значения округлите до сотых и сведите в таблицу следующего вида:

Разбиение	Плотность первого сообщества	Плотность второго сообщества	...	...
1				
...				
Для представления данных в табличном виде используйте pandas.

comms_list = []
colors_list = [colors_1, colors_2, colors_3, colors_4, colors_5]
for color in colors_list:
    com_1 = [str(i + 1) for i, val in enumerate(color) if val == "red"]
    com_2 = [str(i + 1) for i, val in enumerate(color) if val == "green"]
    sub_1 = nx.subgraph(SW, com_1)
    sub_2 = nx.subgraph(SW, com_2)
    comms_list.append([sub_1, sub_2])
    
data = []

for i, (comm1, comm2) in enumerate(comms_list, start=1):
    density1 = nx.density(comm1)
    density2 = nx.density(comm2)
    avg_density = (density1 + density2) / 2
    modularity = nx.community.modularity(SW, communities=[set(comm1.nodes()), set(comm2.nodes())])
    coverage, performance = nx.community.partition_quality(SW, partition=[set(comm1.nodes()), set(comm2.nodes())])
    
    data.append({
        "Разбиение": str(i),
        "Плотность первого сообщества": density1,
        "Плотность второго сообщества": density2,
        "Средняя плотность сообществ": avg_density,
        "Модулярность": modularity,
        "Покрытие (coverage)": coverage,
        "Эффективность (performance)": performance
    })
table = pd.DataFrame(data)
table["Разбиение"] = ["По id узлов", "Случайный", "По медиане", "Связь с '7'", "Связь с '5'"]
table
    """
    text = """
comms_list = []
colors_list = [colors_1, colors_2, colors_3, colors_4, colors_5]
for color in colors_list:
    com_1 = [str(i + 1) for i, val in enumerate(color) if val == "red"]
    com_2 = [str(i + 1) for i, val in enumerate(color) if val == "green"]
    sub_1 = nx.subgraph(SW, com_1)
    sub_2 = nx.subgraph(SW, com_2)
    comms_list.append([sub_1, sub_2])
    
data = []

for i, (comm1, comm2) in enumerate(comms_list, start=1):
    density1 = nx.density(comm1)
    density2 = nx.density(comm2)
    avg_density = (density1 + density2) / 2
    modularity = nx.community.modularity(SW, communities=[set(comm1.nodes()), set(comm2.nodes())])
    coverage, performance = nx.community.partition_quality(SW, partition=[set(comm1.nodes()), set(comm2.nodes())])
    
    data.append({
        "Разбиение": str(i),
        "Плотность первого сообщества": density1,
        "Плотность второго сообщества": density2,
        "Средняя плотность сообществ": avg_density,
        "Модулярность": modularity,
        "Покрытие (coverage)": coverage,
        "Эффективность (performance)": performance
    })
table = pd.DataFrame(data)
table["Разбиение"] = ["По id узлов", "Случайный", "По медиане", "Связь с '7'", "Связь с '5'"]
table
    """
    pc.copy(text)
    
    
def five_community_detection_3():
    """
3. Считайте сеть из файла communities.edgelist. Выполните разбиение сети на 2 сообщества с помощью алгоритма Гирвана-Ньюмена. Визуализируйте результат, раскрасив узлы в цвет, соответствующий их сообществу.

G_C = nx.read_edgelist(path="data/communities.edgelist")
pos = nx.spring_layout(G_C)
nx.draw(G_C, with_labels=True)

GN_com = next(nx.community.girvan_newman(G_C))
print(GN_com)

colors = ["red" if node in GN_com[0] else "green" for node in G_C.nodes()]
nx.draw(G_C, with_labels=True, pos=pos, node_color=colors)
    """
    text = """
G_C = nx.read_edgelist(path="data/communities.edgelist")
pos = nx.spring_layout(G_C)
nx.draw(G_C, with_labels=True)

GN_com = next(nx.community.girvan_newman(G_C))
print(GN_com)

colors = ["red" if node in GN_com[0] else "green" for node in G_C.nodes()]
nx.draw(G_C, with_labels=True, pos=pos, node_color=colors)
    """
    pc.copy(text)
    
    
def five_community_detection_4():
    """
4. Постройте график динамики модулярности для шагов алгоритма Гирвана-Ньюмена. Визуализируйте разбиение сети из предыдущей задачи, при котором достигается наилучшее значение модулярности (выведите это значение на экран). Для визуализации выберите несколько цветов (в соответствии с количеством выделенных сообществ) и раскрасьте узлы и связи в рамках одного сообщества этими цветами. Связи между узлами, состоящими в разных сообществах, отрисуйте черным цветом.

GN_com_gen = nx.community.girvan_newman(G_C)

modularity_values = []
partitions_list = []

for communities in GN_com_gen:
    partition = list(communities)
    partitions_list.append(partition)
    
    modularity = nx.community.modularity(G_C, partition)
    modularity_values.append(modularity)

plt.figure(figsize=(10, 6))
plt.plot(modularity_values, marker='o')
plt.title("Динамика модулярности для шагов алгоритма Гирвана-Ньюмена")
plt.xlabel("Шаг алгоритма")
plt.ylabel("Модулярность")
plt.grid(True)
plt.show()

best_index_comm = modularity_values.index(max(modularity_values))
best_partition = partitions_list[best_index_comm]

print(f"Лучшее разбиение - на {len(best_partition)} сообщества\n\
Максимальное значение модулярности: {modularity_values[best_index_comm]}")

colors_list = ["red", "green", "blue", "purple"]
colors_dict = {n:colors_list[i] for i, part in enumerate(best_partition) for n in part }
node_colors = [colors_dict[node] for node in G_C.nodes()]

edge_colors = []
for edge in G_C.edges():
    same_community = False
    for community in best_partition:
        if edge[0] in community and edge[1] in community:
            same_community = True
            break
    if same_community:
        edge_colors.append(colors_dict[edge[0]])
    else:
        edge_colors.append("black")
nx.draw(G_C, with_labels=True, pos=pos, node_color=node_colors, edge_color=edge_colors)
    """
    text = """
GN_com_gen = nx.community.girvan_newman(G_C)

modularity_values = []
partitions_list = []

for communities in GN_com_gen:
    partition = list(communities)
    partitions_list.append(partition)
    
    modularity = nx.community.modularity(G_C, partition)
    modularity_values.append(modularity)

plt.figure(figsize=(10, 6))
plt.plot(modularity_values, marker='o')
plt.title("Динамика модулярности для шагов алгоритма Гирвана-Ньюмена")
plt.xlabel("Шаг алгоритма")
plt.ylabel("Модулярность")
plt.grid(True)
plt.show()

best_index_comm = modularity_values.index(max(modularity_values))
best_partition = partitions_list[best_index_comm]

print(f"Лучшее разбиение - на {len(best_partition)} сообщества\n\
Максимальное значение модулярности: {modularity_values[best_index_comm]}")

colors_list = ["red", "green", "blue", "purple"]
colors_dict = {n:colors_list[i] for i, part in enumerate(best_partition) for n in part }
node_colors = [colors_dict[node] for node in G_C.nodes()]

edge_colors = []
for edge in G_C.edges():
    same_community = False
    for community in best_partition:
        if edge[0] in community and edge[1] in community:
            same_community = True
            break
    if same_community:
        edge_colors.append(colors_dict[edge[0]])
    else:
        edge_colors.append("black")
nx.draw(G_C, with_labels=True, pos=pos, node_color=node_colors, edge_color=edge_colors) 

    """
    pc.copy(text)
    
    
def five_community_detection_5():
    """
    5. Выполните разбиение сети из первого задания на сообщества с помощью Лувенского алгоритма. Визуализируйте результат аналогично предыдущему заданию.
    
G_5 = nx.read_edgelist(path="data/communities.edgelist")
community_louv = nx.community.louvain_communities(G_5, seed=0)
moodularity = nx.community.modularity(G_5, communities=community_louv)
print(f"Лучшее разбиение - на {len(community_louv)} сообщества\nЗначение модулярности: {moodularity}")

colors_list = ["red", "green", "blue", "purple"]
colors_dict = {n:colors_list[i] for i, part in enumerate(community_louv) for n in part}
node_colors = [colors_dict[node] for node in G_5.nodes()]

edge_colors = []
for edge in G_5.edges():
    same_community = False
    for community in community_louv:
        if edge[0] in community and edge[1] in community:
            same_community = True
            break
    if same_community:
        edge_colors.append(colors_dict[edge[0]])
    else:
        edge_colors.append("black")
        
nx.draw(G_5, with_labels=True, pos=pos, node_color=node_colors, edge_color=edge_colors)

    """
    text = """
G_5 = nx.read_edgelist(path="data/communities.edgelist")
community_louv = nx.community.louvain_communities(G_5, seed=0)
moodularity = nx.community.modularity(G_5, communities=community_louv)
print(f"Лучшее разбиение - на {len(community_louv)} сообщества\nЗначение модулярности: {moodularity}")

colors_list = ["red", "green", "blue", "purple"]
colors_dict = {n:colors_list[i] for i, part in enumerate(community_louv) for n in part}
node_colors = [colors_dict[node] for node in G_5.nodes()]

edge_colors = []
for edge in G_5.edges():
    same_community = False
    for community in community_louv:
        if edge[0] in community and edge[1] in community:
            same_community = True
            break
    if same_community:
        edge_colors.append(colors_dict[edge[0]])
    else:
        edge_colors.append("black")
        
nx.draw(G_5, with_labels=True, pos=pos, node_color=node_colors, edge_color=edge_colors)
    """
    pc.copy(text)
    

def five_community_detection_6():
    """
6. Постройте матрицу смежности для сети. Постройте матрицу смежности для сети, в которой узлы перенумерованы в порядке возрастания номера сообщества, которому они принадлежат (т.е. номера  0...𝐶1−1
  даются  𝐶1
  узлам, принадлежащим первому сообществу; номера  𝐶1...𝐶2−1
  даются  𝐶2
  узлам, принадлежащим второму сообществу и т.д.). Для разбиения графа на сообщества воспользуйтесь Лувенским алгоритмом.

Визуализируйте две матрицы смежности при помощи seaborn.heatmap. Расположите рисунки рядом по горизонтали, добавьте названия рисунков. Сравните полученные изображения и сделайте выводы.

G_6 = nx.read_edgelist(path="data/communities.edgelist")
community_louv = nx.community.louvain_communities(G_6, seed=0)

node_to_community = {}
for i, com in enumerate(community_louv):
    for node in com:
        node_to_community[node] = i
        
adj_matrix = nx.adjacency_matrix(G_6).toarray()
sorted_nodes = sorted(G_6.nodes(), key=lambda x: node_to_community[x])
reorder_adj_matrix = nx.adjacency_matrix(G_6, nodelist=sorted_nodes).toarray()

fig, axes = plt.subplots(1, 2, figsize=(16, 8))

sns.heatmap(adj_matrix, ax=axes[0], cbar=False)
axes[0].set_title("Исходная матрица смежности")
axes[0].set_xlabel("Узлы")
axes[0].set_ylabel("Узлы")

sns.heatmap(reorder_adj_matrix, ax=axes[1], cbar=False)
axes[1].set_title("Упорядоченная матрица смежности")
axes[1].set_xlabel("Узлы (сортированные по сообществам)")
axes[1].set_ylabel("Узлы (сортированные по сообществам)")

plt.tight_layout()
plt.show()
    """
    text = """
G_6 = nx.read_edgelist(path="data/communities.edgelist")
community_louv = nx.community.louvain_communities(G_6, seed=0)

node_to_community = {}
for i, com in enumerate(community_louv):
    for node in com:
        node_to_community[node] = i
        
adj_matrix = nx.adjacency_matrix(G_6).toarray()
sorted_nodes = sorted(G_6.nodes(), key=lambda x: node_to_community[x])
reorder_adj_matrix = nx.adjacency_matrix(G_6, nodelist=sorted_nodes).toarray()

fig, axes = plt.subplots(1, 2, figsize=(16, 8))

sns.heatmap(adj_matrix, ax=axes[0], cbar=False)
axes[0].set_title("Исходная матрица смежности")
axes[0].set_xlabel("Узлы")
axes[0].set_ylabel("Узлы")

sns.heatmap(reorder_adj_matrix, ax=axes[1], cbar=False)
axes[1].set_title("Упорядоченная матрица смежности")
axes[1].set_xlabel("Узлы (сортированные по сообществам)")
axes[1].set_ylabel("Узлы (сортированные по сообществам)")

plt.tight_layout()
plt.show()
    """
    pc.copy(text)
    
    
def six_lpa_1():
    """
1. Загрузите граф карате-клуба и выделите в нем сообщества с помощью реализации алгоритма распространения меток из networkx. Визуализируйте полученный результат. Для визуализации выберите несколько цветов (в соответствии с количеством выделенных сообществ) и раскрасьте узлы и связи в рамках одного сообщества этими цветами. Связи между узлами, состоящими в разных сообществах, отрисуйте черным цветом.

G1 = nx.karate_club_graph()
lpc_G1 = nx.community.label_propagation_communities(G1)
pos = nx.spring_layout(G1)

colors_list = ["red", "green", "blue", "purple"]
colors_dict = {n:colors_list[i] for i, part in enumerate(lpc_G1) for n in part}
node_colors = [colors_dict[node] for node in G1.nodes()]

edge_colors = []
for edge in G1.edges():
    same_community = False
    for community in lpc_G1:
        if edge[0] in community and edge[1] in community:
            same_community = True
            break
    if same_community:
        edge_colors.append(colors_dict[edge[0]])
    else:
        edge_colors.append("black")
        
nx.draw(G1, pos=pos, node_color=node_colors,edge_color=edge_colors, with_labels=True)
    """
    text = """
G1 = nx.karate_club_graph()
lpc_G1 = nx.community.label_propagation_communities(G1)
pos = nx.spring_layout(G1)

colors_list = ["red", "green", "blue", "purple"]
colors_dict = {n:colors_list[i] for i, part in enumerate(lpc_G1) for n in part}
node_colors = [colors_dict[node] for node in G1.nodes()]

edge_colors = []
for edge in G1.edges():
    same_community = False
    for community in lpc_G1:
        if edge[0] in community and edge[1] in community:
            same_community = True
            break
    if same_community:
        edge_colors.append(colors_dict[edge[0]])
    else:
        edge_colors.append("black")
        
nx.draw(G1, pos=pos, node_color=node_colors,edge_color=edge_colors, with_labels=True)
    """
    pc.copy(text)
    
    
def six_lpa_2():
    """
2. Реализуйте синхронный алгоритм распространения меток. В синхронном варианте алгоритма для выбора новой метки узла используются метки соседей с предыдущей итерации. Если среди меток соседей есть несколько вариантов с одинаковой максимальной частотой, то метка выбирается случайным образом. Алгоритм прекращает работу, когда на очередной итерации не была изменена метка ни одного узла. Визуализируйте пошаговую динамику распространения меток и итоговое разбиение сети аналогично предыдущей задаче.

G2 = nx.karate_club_graph()
pos = nx.spring_layout(G2)

colors_map = {i:"#"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])
         for i in range(len(G2.nodes()))}
         
def add_labels(G):
    for i, node in enumerate(G.nodes()):
        G.nodes[node]["label"] = i
    return G
    
def draw_graph(G, colors_dict):
    labels = [G.nodes[n]["label"] for n in G.nodes]
    unique_labels = list(set(labels))
    node_colors = [colors_dict[label] for label in labels]
    
    edge_colors = []
    for edge in G.edges():
        same_community = False
        for community in unique_labels:
            if G.nodes[edge[0]]["label"]==community and G.nodes[edge[1]]["label"]==community:
                same_community = True
                break
        if same_community:
            edge_colors.append(colors_dict[labels[edge[0]]])
        else:
            edge_colors.append("black")
            
    nx.draw(G, pos=pos, node_color=node_colors,edge_color=edge_colors, with_labels=True)
    plt.show()
    
def get_lpc_syn(G, colors_map, logging=True, printing=True):
    changes_flag = True
    count_iters = 0
    while changes_flag:
        changes_flag = False
        nodes_dict = copy.deepcopy(dict(G.nodes(data=True)))
        count_iters += 1
        for node, attrs in nodes_dict.items():
            freqs_neighbors = Counter(nodes_dict[n]["label"] for n in G.neighbors(node))
            max_freq_labels = [key for key, val in freqs_neighbors.items() if val==max(freqs_neighbors.values())]
            if attrs["label"] not in max_freq_labels:
                if len(max_freq_labels) > 1:
                    new_label = random.choice(max_freq_labels)
                else:
                    new_label = max_freq_labels[0]
                if logging:
                    print(f"Метка узла {node} меняется с {attrs['label']} на {new_label}")
                G.nodes[node]["label"] = new_label
                changes_flag = True
        if printing:
            draw_graph(G, colors_map)
    print("Количество итераций:",count_iters)
    return G
    
G2 = add_labels(G2)

# начальный
draw_graph(G2, colors_map)

G2_lpc = get_lpc_syn(G2, colors_map)

# финальный
draw_graph(G2_lpc, colors_map)
    """
    text = """
G2 = nx.karate_club_graph()
pos = nx.spring_layout(G2)

colors_map = {i:"#"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])
         for i in range(len(G2.nodes()))}
         
def add_labels(G):
    for i, node in enumerate(G.nodes()):
        G.nodes[node]["label"] = i
    return G
    
def draw_graph(G, colors_dict):
    labels = [G.nodes[n]["label"] for n in G.nodes]
    unique_labels = list(set(labels))
    node_colors = [colors_dict[label] for label in labels]
    
    edge_colors = []
    for edge in G.edges():
        same_community = False
        for community in unique_labels:
            if G.nodes[edge[0]]["label"]==community and G.nodes[edge[1]]["label"]==community:
                same_community = True
                break
        if same_community:
            edge_colors.append(colors_dict[labels[edge[0]]])
        else:
            edge_colors.append("black")
            
    nx.draw(G, pos=pos, node_color=node_colors,edge_color=edge_colors, with_labels=True)
    plt.show()
    
def get_lpc_syn(G, colors_map, logging=True, printing=True):
    changes_flag = True
    count_iters = 0
    while changes_flag:
        changes_flag = False
        nodes_dict = copy.deepcopy(dict(G.nodes(data=True)))
        count_iters += 1
        for node, attrs in nodes_dict.items():
            freqs_neighbors = Counter(nodes_dict[n]["label"] for n in G.neighbors(node))
            max_freq_labels = [key for key, val in freqs_neighbors.items() if val==max(freqs_neighbors.values())]
            if attrs["label"] not in max_freq_labels:
                if len(max_freq_labels) > 1:
                    new_label = random.choice(max_freq_labels)
                else:
                    new_label = max_freq_labels[0]
                if logging:
                    print(f"Метка узла {node} меняется с {attrs['label']} на {new_label}")
                G.nodes[node]["label"] = new_label
                changes_flag = True
        if printing:
            draw_graph(G, colors_map)
    print("Количество итераций:",count_iters)
    return G
    
G2 = add_labels(G2)

# начальный
draw_graph(G2, colors_map)

G2_lpc = get_lpc_syn(G2, colors_map)

# финальный
draw_graph(G2_lpc, colors_map)
    """
    pc.copy(text)
    
    
def six_lpa_3():
    """
3. Реализуйте асинхронный алгоритм распространения меток. В асинхронном варианте алгоритма для выбора новой метки узла используются в том числе метки соседей с текущей итерации алгоритма. Визуализируйте пошаговую динамику распространения меток и итоговое разбиение сети аналогично предыдущей задаче.

def get_lpc_async(G, colors_map, logging=True, printing=True):
    changes_flag = True
    count_iters = 0
    while changes_flag:
        changes_flag = False
        count_iters += 1
        nodes_dict = dict(G.nodes(data=True)) 
        for node, attrs in nodes_dict.items():
            freqs_neighbors = Counter(G.nodes[n]["label"] for n in G.neighbors(node))
            max_freq_labels = [key for key, val in freqs_neighbors.items() if val==max(freqs_neighbors.values())]
            if attrs["label"] not in max_freq_labels:
                if len(max_freq_labels) > 1:
                    new_label = random.choice(max_freq_labels)
                else:
                    new_label = max_freq_labels[0]
                if logging:
                    print("\n\n")
                    print(f"Метка узла {node} меняется с {attrs['label']} на {new_label}")
                    
                G.nodes[node]["label"] = new_label
                changes_flag = True
                
                if printing:
                    draw_graph(G, colors_map)
    print("Количество итераций:",count_iters)    
    return G
    
G3 = nx.karate_club_graph()
G3 = add_labels(G3)

# начальный
draw_graph(G3, colors_map)

G3_lpc = get_lpc_async(G3, colors_map)
    """
    text = """
def get_lpc_async(G, colors_map, logging=True, printing=True):
    changes_flag = True
    count_iters = 0
    while changes_flag:
        changes_flag = False
        count_iters += 1
        nodes_dict = dict(G.nodes(data=True)) 
        for node, attrs in nodes_dict.items():
            freqs_neighbors = Counter(G.nodes[n]["label"] for n in G.neighbors(node))
            max_freq_labels = [key for key, val in freqs_neighbors.items() if val==max(freqs_neighbors.values())]
            if attrs["label"] not in max_freq_labels:
                if len(max_freq_labels) > 1:
                    new_label = random.choice(max_freq_labels)
                else:
                    new_label = max_freq_labels[0]
                if logging:
                    print("\n\n")
                    print(f"Метка узла {node} меняется с {attrs['label']} на {new_label}")
                    
                G.nodes[node]["label"] = new_label
                changes_flag = True
                
                if printing:
                    draw_graph(G, colors_map)
    print("Количество итераций:",count_iters)    
    return G
    
G3 = nx.karate_club_graph()
G3 = add_labels(G3)

# начальный
draw_graph(G3, colors_map)

G3_lpc = get_lpc_async(G3, colors_map)
    """
    pc.copy(text)
    
    
def six_lpa_4():
    """
4. Используя собственную реализацию LPA, разбейте граф карате-клуба на сообщества N=10 раз. Создайте двумерный массив freqs размера MxM (M - количество узлов в графе), где freqs[i, j] показывает, как часто узлы i и j оказывались в одном сообществе.
Создайте граф, в котором между узлами i и j существует связь только тогда, когда freqs[i, j] >= 8. Получите компоненты связности данного графа. Интерпретируя данные компоненты связности как сообщества в исходном графе, визуализируйте полученное разбиение карате-клуба (оригинального графа) аналогично предыдущим заданиям.

G4 = nx.karate_club_graph()
G4 = add_labels(G4)
freqs = np.zeros((len(G4), len(G4)))
freqs.shape

for _ in range(10):
    GG = copy.deepcopy(G4)
    G4_lps = get_lpc_async(GG, colors_map, logging=False, printing=False)
    labels_dict = {n:G4_lps.nodes[n]["label"] for n in G4_lps.nodes}
    nodes_list = list(G4_lps.nodes())
    for i in nodes_list:
         for j in nodes_list:
            if labels_dict[i]==labels_dict[j]:
                freqs[i, j] += 1 
                
G4_freqs = nx.Graph()
G4_freqs.add_nodes_from(G4)
nodes_list = list(G4_freqs.nodes())

for i in nodes_list:
    for j in nodes_list:
        if freqs[i, j] >= 8:
            if i != j:
                G4_freqs.add_edge(i, j)
                
# компоненты связности
print("Количество компонент связности:", nx.number_connected_components(G4_freqs))
nx.draw(G4_freqs, pos, with_labels=True)

for i, group in enumerate(nx.connected_components(G4_freqs)):  
    for node in group:
        G4.nodes[node]["label"] = i
        
draw_graph(G4, colors_map)
    """
    text = """
G4 = nx.karate_club_graph()
G4 = add_labels(G4)
freqs = np.zeros((len(G4), len(G4)))
freqs.shape

for _ in range(10):
    GG = copy.deepcopy(G4)
    G4_lps = get_lpc_async(GG, colors_map, logging=False, printing=False)
    labels_dict = {n:G4_lps.nodes[n]["label"] for n in G4_lps.nodes}
    nodes_list = list(G4_lps.nodes())
    for i in nodes_list:
         for j in nodes_list:
            if labels_dict[i]==labels_dict[j]:
                freqs[i, j] += 1 
                
G4_freqs = nx.Graph()
G4_freqs.add_nodes_from(G4)
nodes_list = list(G4_freqs.nodes())

for i in nodes_list:
    for j in nodes_list:
        if freqs[i, j] >= 8:
            if i != j:
                G4_freqs.add_edge(i, j)
                
# компоненты связности
print("Количество компонент связности:", nx.number_connected_components(G4_freqs))
nx.draw(G4_freqs, pos, with_labels=True)

for i, group in enumerate(nx.connected_components(G4_freqs)):  
    for node in group:
        G4.nodes[node]["label"] = i
        
draw_graph(G4, colors_map)
    """
    pc.copy(text)
    
    
def six_lpa_5():
    """
5. Оформите результаты работы алгоритмов в виде таблицы
Алгоритм	Средняя плотность сообществ	Модулярность	Покрытие	Эффективность
Синхронный LPA				
Асинхронный LPA				
Множественный LPA				
Для представления данных в табличном виде используйте pandas. Все расчеты метрик при решении этой задачи выполните повторно.

G5 = nx.karate_club_graph()

data = []
graph_list = [G2_lpc, G3_lpc, G4]

for i, graph in enumerate(graph_list):
    labels_dict = {n:graph.nodes[n]["label"] for n in graph.nodes}
    
    communities = defaultdict(list)
    for key, value in labels_dict.items():
        communities[value].append(key)
    communities_list = list(communities.values())
                            
    densities = []
    for comm in communities_list:
        if len(comm) > 1:
            subg = nx.subgraph(graph, comm)
            dens = nx.density(subg)
            densities.append(dens)
    avg_density = np.array(densities).mean()
    
    modularity = nx.community.modularity(graph, communities=communities_list)
    coverage, performance = nx.community.partition_quality(graph, partition=communities_list)
    
    data.append({
        "Алгоритм": str(i),
        "Средняя плотность сообществ": avg_density,
        "Модулярность": modularity,
        "Покрытие": coverage,
        "Эффективность": performance
    })
    
table = pd.DataFrame(data)
table["Алгоритм"] = ["Синхронный LPA", "Асинхронный LPA", "Множественный LPA"]
table
    """
    text = """
G5 = nx.karate_club_graph()

data = []
graph_list = [G2_lpc, G3_lpc, G4]

for i, graph in enumerate(graph_list):
    labels_dict = {n:graph.nodes[n]["label"] for n in graph.nodes}
    
    communities = defaultdict(list)
    for key, value in labels_dict.items():
        communities[value].append(key)
    communities_list = list(communities.values())
                            
    densities = []
    for comm in communities_list:
        if len(comm) > 1:
            subg = nx.subgraph(graph, comm)
            dens = nx.density(subg)
            densities.append(dens)
    avg_density = np.array(densities).mean()
    
    modularity = nx.community.modularity(graph, communities=communities_list)
    coverage, performance = nx.community.partition_quality(graph, partition=communities_list)
    
    data.append({
        "Алгоритм": str(i),
        "Средняя плотность сообществ": avg_density,
        "Модулярность": modularity,
        "Покрытие": coverage,
        "Эффективность": performance
    })
    
table = pd.DataFrame(data)
table["Алгоритм"] = ["Синхронный LPA", "Асинхронный LPA", "Множественный LPA"]
table
    """
    pc.copy(text)
    
    
def seven_rand_walk_1():
    """
1\. Загрузите граф карате-клуба. Получите матрицу смежности `A` этого графа. Получите на ее основе матрицу переходов `P` по следующему правилу:

$$\mathbf{P}=\mathbf{D}^{-1}\mathbf{A}$$

Продемонстрируйте, что выполняются условия (1) и (2).

$0 \le p_{ij} \le 1$ (1)

$\sum_j p_{ij}=1$    (2)

Все действия проводите с невзвешенной матрицей смежности.

G = nx.karate_club_graph()
A = nx.adjacency_matrix(G).toarray()

degrees = np.sum(A, axis=1)
D = np.diag(degrees)

P = np.linalg.inv(D) @ A
P[(~((P >= 0) & (P <= 1)))]
P.sum(axis=1)
    """
    text = """
G = nx.karate_club_graph()
A = nx.adjacency_matrix(G).toarray()

degrees = np.sum(A, axis=1)
D = np.diag(degrees)

P = np.linalg.inv(D) @ A
P[(~((P >= 0) & (P <= 1)))]
P.sum(axis=1)
    """
    pc.copy(text)
    
    
def seven_rand_walk_2():
    """
2\. Создайте вектор начального состояния $\mathbf{p}^0 = [0, ..., 1]^T$. Получите стационарное состояние $\mathbf{p}^\infty$, используя итеративную процедуру

$\mathbf{p}^{t+1}=(\mathbf{P}^{\top})\mathbf{p}^t$ 

Процесс заканчивается, когда $||\mathbf{p}^{t+1} - \mathbf{p}^{t}|| < \epsilon $

Выведите полученный вектор стационарного состояния на экран.

p = np.zeros(P.shape[0])
p[-1] = 1
p = p.T

eps = 0.01

p_t = p
iters = 0
while True:
    iters += 1
    p_t_1 = (P.T) @ p_t
    norm = np.linalg.norm(p_t_1 - p_t)
    print(iters, norm)
    if norm < eps:
        break
    p_t = p_t_1
    """
    text = """
p = np.zeros(P.shape[0])
p[-1] = 1
p = p.T

eps = 0.01

p_t = p
iters = 0
while True:
    iters += 1
    p_t_1 = (P.T) @ p_t
    norm = np.linalg.norm(p_t_1 - p_t)
    print(iters, norm)
    if norm < eps:
        break
    p_t = p_t_1
    """
    pc.copy(text)
    
    
def seven_rand_walk_3():
    """
3\. Найдите матрицу перехода к стационарному состоянию $(\mathbf{P}^{\top})^\infty$ при помощи процедуры возведения матрицы в степень.

Докажите, что полученная матрица является матрицей стационарного состояния, т.е. $||(\mathbf{P}^{\top})^{\infty}  -(\mathbf{P}^{\top})(\mathbf{P}^{\top})^{\infty}|| <= \epsilon$

Cоздайте вектор начального состояния $\mathbf{p}^0 = [0, ..., 1]^T $. Получите стационарное состояние $\mathbf{p}^\infty$, воспользовавшись полученной матрицей $(\mathbf{P}^{\top})^\infty$. Решите задачу двумя способами: при помощи матричного умножения и при помощи оператора индексации.

Используя функцию `np.allclose`, покажите, что векторы стационарных состояний, полученные двумя разными методами, совпадают (с точностью до тысячных).

P_st = np.linalg.matrix_power(P.T, 10**10)
np.linalg.norm(P_st - P.T @ P_st) <= eps

p = np.zeros(P.shape[0])
p[-1] = 1
p = p.T
p

p_st_1 = P_st[:, 0]
p_st_1

p_st_2 = P_st @ p
p_st_2

np.allclose(p_st_1, p_st_2, 0.001)
    """
    text = """
P_st = np.linalg.matrix_power(P.T, 10**10)
np.linalg.norm(P_st - P.T @ P_st) <= eps

p = np.zeros(P.shape[0])
p[-1] = 1
p = p.T
p

p_st_1 = P_st[:, 0]
p_st_1

p_st_2 = P_st @ p
p_st_2

np.allclose(p_st_1, p_st_2, 0.001)
    """
    pc.copy(text)
    

def seven_rand_walk_4():
    """
4. Загрузите граф карате-клуба. Вычислите центральность каждого узла в сети. Визуализируйте граф, отмасшабировав размер каждого узла пропорционально полученным значениям. Постройте несколько визуализаций графа в виде сетки, используя следующие меры центральности:

центральность по степени;
центральность по посредничеству;
центральность по близости;
центральность по собственному вектору;
центральность по PageRank.
На каждом рисунке сделайте размеры узлов пропорционально соответствующей мере центральности. Сравните результаты и сделайте выводы.

G = nx.karate_club_graph()
pos = nx.spring_layout(G)

# центральность по степени;
centr_deg = nx.degree_centrality(G)
node_size = [2000 * v for k,v in centr_deg.items()]
nx.draw(G, pos=pos, with_labels=True, node_size=node_size)

# центральность по посредничеству;
centr_betw = nx.betweenness_centrality(G)
node_size = [2000 * v for k,v in centr_betw.items()]
nx.draw(G, pos=pos, with_labels=True, node_size=node_size)

# центральность по близости;
centr_cl = nx.closeness_centrality(G)
node_size = [2000 * v for k,v in centr_cl.items()]
nx.draw(G, pos=pos, with_labels=True, node_size=node_size)

# центральность по собственному вектору;
centr_ei = nx.eigenvector_centrality(G)
node_size = [2000 * v for k,v in centr_ei.items()]
nx.draw(G, pos=pos, with_labels=True, node_size=node_size)

# центральность по PageRank
centr_page = nx.pagerank(G)
node_size = [3000 * v for k,v in centr_page.items()]
nx.draw(G, pos=pos, with_labels=True, node_size=node_size)
    """
    text = """
G = nx.karate_club_graph()
pos = nx.spring_layout(G)

# центральность по степени;
centr_deg = nx.degree_centrality(G)
node_size = [2000 * v for k,v in centr_deg.items()]
nx.draw(G, pos=pos, with_labels=True, node_size=node_size)

# центральность по посредничеству;
centr_betw = nx.betweenness_centrality(G)
node_size = [2000 * v for k,v in centr_betw.items()]
nx.draw(G, pos=pos, with_labels=True, node_size=node_size)

# центральность по близости;
centr_cl = nx.closeness_centrality(G)
node_size = [2000 * v for k,v in centr_cl.items()]
nx.draw(G, pos=pos, with_labels=True, node_size=node_size)

# центральность по собственному вектору;
centr_ei = nx.eigenvector_centrality(G)
node_size = [2000 * v for k,v in centr_ei.items()]
nx.draw(G, pos=pos, with_labels=True, node_size=node_size)

# центральность по PageRank
centr_page = nx.pagerank(G)
node_size = [3000 * v for k,v in centr_page.items()]
nx.draw(G, pos=pos, with_labels=True, node_size=node_size)
    """
    pc.copy(text)
    
    
def seven_rand_walk_5():
    """
5\. Реализуйте алгоритм PageRank с параметром затухания $\alpha=0.9$.

Загрузите граф карате-клуба и сделайте его ориентированным. Найдите стохастическую матрицу переходов $\mathbf{P}$.

$$\mathbf{P}=\mathbf{D}^{-1}\mathbf{A}$$

Модифицируйте матрицу $\mathbf{P}$:

$$\mathbf{P}' = \alpha \mathbf{P} + (1-\alpha)\frac{1}{n}\mathbf{E}$$
$$\mathbf{M}=(\mathbf{P}')^{\top}$$

где $\mathbf{E}$ - матрица размера `NxN`, состоящая из единиц.

Получите вектор $\mathbf{p}^\infty = \mathbf{pr}^{iter}$, используя итеративную процедуру

$$\mathbf{p}^{t+1}=\mathbf{M}\mathbf{p}^t$$

Процесс заканчивается, когда $||\mathbf{p}^{t+1} - \mathbf{p}^{t}|| < \epsilon $

Выведите полученный вектор стационарного состояния на экран.
Вызовите `nx.pagerank` с соответствующими параметрами и докажите, что полученные векторы совпадают.

G = nx.karate_club_graph()

D = nx.DiGraph()
pos = nx.spring_layout(D)

for u, v in G.edges():
    D.add_edge(u, v)
    D.add_edge(v, u)
    
nx.draw(D, with_labels=True)

A = nx.adjacency_matrix(D).toarray()
degrees = list(A.sum(axis=1))
Diag = np.diag(degrees)

P = np.linalg.inv(Diag) @ A

alpha = 0.9
n = len(degrees)
E = np.ones((n,n))
P_ = P * alpha + (1 - alpha) * (1 / n) * E
M = P_.T

eps = 1e-10

p = np.zeros(M.shape[0])
p[-1] = 1
p = p.T

p_old = M @ p
iters = 0
while True:
    iters+=1
    p_new = M @ p_old
    norm = np.linalg.norm(p_new - p_old, 1)
    print(iters, norm)
    if norm <= eps:
        break
    p_old = p_new
    
p_new

p_page = list(nx.pagerank(D, alpha=0.9, tol=eps).values())
p_page

np.allclose(p_new, p_page, eps)
    """
    text = """
G = nx.karate_club_graph()

D = nx.DiGraph()
pos = nx.spring_layout(D)

for u, v in G.edges():
    D.add_edge(u, v)
    D.add_edge(v, u)
    
nx.draw(D, with_labels=True)

A = nx.adjacency_matrix(D).toarray()
degrees = list(A.sum(axis=1))
Diag = np.diag(degrees)

P = np.linalg.inv(Diag) @ A

alpha = 0.9
n = len(degrees)
E = np.ones((n,n))
P_ = P * alpha + (1 - alpha) * (1 / n) * E
M = P_.T

eps = 1e-10

p = np.zeros(M.shape[0])
p[-1] = 1
p = p.T

p_old = M @ p
iters = 0
while True:
    iters+=1
    p_new = M @ p_old
    norm = np.linalg.norm(p_new - p_old, 1)
    print(iters, norm)
    if norm <= eps:
        break
    p_old = p_new
    
p_new

p_page = list(nx.pagerank(D, alpha=0.9, tol=eps).values())
p_page

np.allclose(p_new, p_page, eps)
    """
    pc.copy(text)

    
def seven_rand_walk_6():
    """
6\. Вычислите коэффициенты PageRank при помощи задачи нахождения собственного вектора матрицы $\mathbf{M}$ $\mathbf{pr}^{eig}$, соответствующего собственному числу 1.

Покажите, что $\mathbf{pr}^{eig}$ и $\mathbf{pr}^{iter}$ оба являются с.в. матрицы $\mathbf{M}$.

eigenvalues, eigenvectors = np.linalg.eig(M)
idx = np.argmax(np.real(eigenvalues))
pr_eig = np.real(eigenvectors[:, idx])
# pr_eig = pr_eig / pr_eig.sum()

np.real(eigenvalues[idx])

diff_p_eig = np.linalg.norm(M @ pr_eig - pr_eig)
diff_p_eig

np.allclose(M.dot(pr_eig), pr_eig, atol=1e-6)

diff_p_iter = np.linalg.norm(M @ p_new - p_new)
diff_p_iter

np.allclose(M.dot(p_new), p_new, atol=1e-6)
    """
    text = """
eigenvalues, eigenvectors = np.linalg.eig(M)
idx = np.argmax(np.real(eigenvalues))
pr_eig = np.real(eigenvectors[:, idx])
# pr_eig = pr_eig / pr_eig.sum()

np.real(eigenvalues[idx])

diff_p_eig = np.linalg.norm(M @ pr_eig - pr_eig)
diff_p_eig

np.allclose(M.dot(pr_eig), pr_eig, atol=1e-6)

diff_p_iter = np.linalg.norm(M @ p_new - p_new)
diff_p_iter

np.allclose(M.dot(p_new), p_new, atol=1e-6)
    """
    pc.copy(text)
    

def eight_rwlpa_1():
    """
1. Загрузите граф Karate Club. Получите значения атрибута club, хранящегося на узлах. Выберите случайным образом 4 узла, относящихся к клубу Mr. Hi и 4 узла, относящихся к клубу Officer. Получите матрицу смежности, в которой узлы перенумерованы таким образом, чтобы строки и столбцы с номерами 0, 1, ... 7 принадлежали узлам, которые были выбраны на предыдущем шаге.

G = nx.karate_club_graph()
pos = nx.spring_layout(G)
mrhi = [node for node, attr in G.nodes(data=True) if attr["club"]=="Mr. Hi"]
officer = [node for node, attr in G.nodes(data=True) if attr["club"]=="Officer"]

random.seed(2797)
mrhi_4 = random.sample(mrhi, 4)
officer_4 = random.sample(officer, 4)

choosed_nodes = mrhi_4 + officer_4
choosed_nodes

nodes = list(G.nodes())

for node in choosed_nodes:
    if node in nodes:
        nodes.remove(node)

nodes_list = choosed_nodes + nodes
print(nodes_list)

A = nx.adjacency_matrix(G, nodes_list, weight=0)
A.toarray()

nx.draw(nx.subgraph(G, choosed_nodes), with_labels=True)
    """
    text = """
G = nx.karate_club_graph()
pos = nx.spring_layout(G)
mrhi = [node for node, attr in G.nodes(data=True) if attr["club"]=="Mr. Hi"]
officer = [node for node, attr in G.nodes(data=True) if attr["club"]=="Officer"]

random.seed(2797)
mrhi_4 = random.sample(mrhi, 4)
officer_4 = random.sample(officer, 4)

choosed_nodes = mrhi_4 + officer_4
choosed_nodes

nodes = list(G.nodes())

for node in choosed_nodes:
    if node in nodes:
        nodes.remove(node)

nodes_list = choosed_nodes + nodes
print(nodes_list)

A = nx.adjacency_matrix(G, nodes_list, weight=0)
A.toarray()

nx.draw(nx.subgraph(G, choosed_nodes), with_labels=True)
    """
    pc.copy(text)
    
    
def eight_rwlpa_2():
    """
2. Получите блочную матрицу  𝐏
  и матрицу начального состояния меток  𝐘0∈{0,1}𝑁×2
𝐏=(𝐏𝑙𝑙𝐏𝑢𝑙𝐏𝑙𝑢𝐏𝑢𝑢)=(𝐈𝐏𝑢𝑙0𝐏𝑢𝑢)
𝐘0=(𝐘𝐥𝐘𝐮)=(𝐘𝐥0)
Выведите на экран след матрицы  𝐏
 . Выведите на экран количество ненулевых элементов матрицы  𝐘0
# Процесс случайного блуждания с ловушками, попав в них при блуждании мы остаемся в них во время всех последующих итераций
# состояние узлов-ловушек всегда остается неизменным
# траектории случайных блажданий из узлов, относящихся к u, будут завершаться на
# изначально помеченных узлах

l = 8

# из ловушек больше никуда нельзя попасть, кроме самих ловушек
A[:l] = 0
A[:l, :l] = np.eye(l)

A.toarray()

целиком
degrees = np.sum(A, axis=1)
D = np.diag(degrees)
P = np.linalg.inv(D) @ A
P

# след P
np.trace(P)

types_of_labels = list(set([attr["club"] for node, attr in G.nodes(data=True)]))
types_of_labels

Y_0 = np.zeros((A.shape[0], len(types_of_labels)))
Y_l = [[1,0] if G.nodes()[node]["club"] == "Mr. Hi" else [0, 1] for i, node in enumerate(choosed_nodes)]
Y_0[:l] = Y_l
Y_0
    """
    text = """
l = 8

# из ловушек больше никуда нельзя попасть, кроме самих ловушек
A[:l] = 0
A[:l, :l] = np.eye(l)

A.toarray()

целиком
degrees = np.sum(A, axis=1)
D = np.diag(degrees)
P = np.linalg.inv(D) @ A
P

# след P
np.trace(P)

types_of_labels = list(set([attr["club"] for node, attr in G.nodes(data=True)]))
types_of_labels

Y_0 = np.zeros((A.shape[0], len(types_of_labels)))
Y_l = [[1,0] if G.nodes()[node]["club"] == "Mr. Hi" else [0, 1] for i, node in enumerate(choosed_nodes)]
Y_0[:l] = Y_l
Y_0
    """
    pc.copy(text)
    
    
def eight_rwlpa_3():
    """
3. Известно, что для блочной матрицы справедливо:
𝐏∞=(𝐈(𝐈−𝐏𝑢𝑢)−1𝐏𝑢𝑙00)
Получите оценку  𝐏∞
  путем возведения матрицы в достаточно большую степень. Свяжите блоки матрицы с отдельными переменными P_ll, P_lu, P_ul, P_uu и продемонстрируйте, что каждый блок полученной матрицы удовлетворяет формуле выше при помощи функции np.allclose.
  
P_inf = np.linalg.matrix_power(P, 10**16)

P_ll = P[:l, :l]
P_ul = P[l:, :l]
P_lu = P[:l, l:]
P_uu = P[l:, l:]

P_inf_ll = P_inf[:l, :l]
P_inf_ul = P_inf[l:, :l]
P_inf_lu = P_inf[:l, l:]
P_inf_uu = P_inf[l:, l:]

# P_ll
np.allclose(np.eye(P_inf_ll.shape[0]), P_inf_ll)

# P_lu
np.allclose(np.zeros(P_inf_lu.shape), P_inf_lu)

# P_ul
np.allclose(np.linalg.inv(np.eye(P_uu.shape[0]) - P_uu) @ P_ul, P_inf_ul)

# P_uu
np.allclose(np.zeros(P_inf_uu.shape), P_inf_uu)
    """
    text = """
P_inf = np.linalg.matrix_power(P, 10**16)

P_ll = P[:l, :l]
P_ul = P[l:, :l]
P_lu = P[:l, l:]
P_uu = P[l:, l:]

P_inf_ll = P_inf[:l, :l]
P_inf_ul = P_inf[l:, :l]
P_inf_lu = P_inf[:l, l:]
P_inf_uu = P_inf[l:, l:]

# P_ll
np.allclose(np.eye(P_inf_ll.shape[0]), P_inf_ll)

# P_lu
np.allclose(np.zeros(P_inf_lu.shape), P_inf_lu)

# P_ul
np.allclose(np.linalg.inv(np.eye(P_uu.shape[0]) - P_uu) @ P_ul, P_inf_ul)

# P_uu
np.allclose(np.zeros(P_inf_uu.shape), P_inf_uu)
    """
    pc.copy(text)
    
    
def eight_rwlpa_4():
    """
4. Используя Базовый вариант RW-LPA , расставьте метки для всех узлов. Визуализируйте сеть, показав цветом контура вокруг узла принадлежность узла к одному из сообществ, а цветом цветом заливки узла - сообщество, к которому узел был отнесен алгоритмом.

Y = copy.deepcopy(Y_0)
eps = 1e-16
iters = 0

while True:
    iters += 1
    Y_new = P @ Y
    Y_new[:l] = Y[:l]
    if abs(Y - Y_new).max() < eps:
        Y_ = Y_new
        break
    Y = Y_new
print(iters)

Y_

colors_contour = ["red" if attr["club"] == 'Mr. Hi' else "green" for node, attr in G.nodes(data=True)]

for i, node in enumerate(nodes_list):
    if Y_[i][0] > Y_[i][1]:
        G.nodes[node]["pred_club"] = 'Mr. Hi'
    else:
        G.nodes[node]["pred_club"] = 'Officer'
        
colors_nodes = ["red" if attr["pred_club"] == 'Mr. Hi' else "green" for node, attr in G.nodes(data=True)]

nx.draw(G, with_labels=True, edgecolors=colors_contour, node_color=colors_nodes, pos=pos, linewidths=2)
    """
    text = """
Y = copy.deepcopy(Y_0)
eps = 1e-16
iters = 0

while True:
    iters += 1
    Y_new = P @ Y
    Y_new[:l] = Y[:l]
    if abs(Y - Y_new).max() < eps:
        Y_ = Y_new
        break
    Y = Y_new
print(iters)

Y_

colors_contour = ["red" if attr["club"] == 'Mr. Hi' else "green" for node, attr in G.nodes(data=True)]

for i, node in enumerate(nodes_list):
    if Y_[i][0] > Y_[i][1]:
        G.nodes[node]["pred_club"] = 'Mr. Hi'
    else:
        G.nodes[node]["pred_club"] = 'Officer'
        
colors_nodes = ["red" if attr["pred_club"] == 'Mr. Hi' else "green" for node, attr in G.nodes(data=True)]

nx.draw(G, with_labels=True, edgecolors=colors_contour, node_color=colors_nodes, pos=pos, linewidths=2)
    """
    pc.copy(text)
    
    
def nine_cuts_1():
    """
1. Задан граф G. Определите два разреза:

𝐶(1)=(𝑁(1)1,𝑁(1)2)
 ,  𝑁(1)1={0,1,2,3,4}
 ,  𝑁(1)2={5,6,7,8,9}
 
𝐶(2)=(𝑁(2)1,𝑁(2)2)
 ,  𝑁(2)1={0,1,2,3,4,5,6,7,8}
 ,  𝑁(2)2={9}
 
Для каждого из разрезов вычислите величину  𝑄(𝑘)
  двумя способами: явным образом просмотрев все ребра и при помощи функции из пакета networkx. Выведите величины разрезов на экран.

𝑄(𝑘)=𝑐𝑢𝑡(𝑁(𝑘)1,𝑁(𝑘)2)=∑𝑖∈𝑁(𝑘)1,𝑗∈𝑁(𝑘)2𝑙𝑖𝑗

G = nx.Graph()
G.add_edges_from(combinations(range(5), 2))
G.add_edges_from(combinations(range(5, 9), 2))
G.add_edges_from([(8, 3), (6, 1), (7, 9)])
pos = nx.spring_layout(G, seed=30)
nx.draw(G, pos=pos, with_labels=True)

C1 = G.copy()
edges_for_remove_1 = [(3,8), (6,1)]
C1.remove_edges_from(edges_for_remove_1)
nx.draw(C1, pos=pos, with_labels=True)

C2 = G.copy()
edges_for_remove_2 = [(7,9)]
C2.remove_edges_from(edges_for_remove_2)
nx.draw(C2, pos=pos, with_labels=True)

# первый способ
Q1_1 = len(edges_for_remove_1)
Q2_1 = len(edges_for_remove_2)
print(Q1_1, Q2_1)

# второй способ
S1 = {0,1,2,3,4}
T1 = {5,6,7,8,9}
Q1_2 = nx.cut_size(G, S1, T1)

S2 = {0,1,2,3,4,5,6,7,8}
T2 = {9}
Q2_2 = nx.cut_size(G, S2, T2)

print(Q1_2, Q2_2)
    """
    text = """
G = nx.Graph()
G.add_edges_from(combinations(range(5), 2))
G.add_edges_from(combinations(range(5, 9), 2))
G.add_edges_from([(8, 3), (6, 1), (7, 9)])
pos = nx.spring_layout(G, seed=30)
nx.draw(G, pos=pos, with_labels=True)

C1 = G.copy()
edges_for_remove_1 = [(3,8), (6,1)]
C1.remove_edges_from(edges_for_remove_1)
nx.draw(C1, pos=pos, with_labels=True)

C2 = G.copy()
edges_for_remove_2 = [(7,9)]
C2.remove_edges_from(edges_for_remove_2)
nx.draw(C2, pos=pos, with_labels=True)

# первый способ
Q1_1 = len(edges_for_remove_1)
Q2_1 = len(edges_for_remove_2)
print(Q1_1, Q2_1)

# второй способ
S1 = {0,1,2,3,4}
T1 = {5,6,7,8,9}
Q1_2 = nx.cut_size(G, S1, T1)

S2 = {0,1,2,3,4,5,6,7,8}
T2 = {9}
Q2_2 = nx.cut_size(G, S2, T2)

print(Q1_2, Q2_2)
    """
    pc.copy(text)
    
    
def nine_cuts_2():
    """
2. Для разрезов из предыдущего задания вычислите

𝑉𝑜𝑙(𝑁(𝑘)𝑡)=∑𝑖∈𝑁(𝑘)𝑡,𝑗∈𝑁𝑙𝑖𝑗=∑𝑖∈𝑁(𝑘)𝑡𝑘𝑖,𝑡=1,2
 

𝑄(𝑘)𝑛𝑜𝑟𝑚𝑎𝑙𝑖𝑧𝑒𝑑=𝑐𝑢𝑡(𝑁(𝑘)1,𝑁(𝑘)2)𝑉𝑜𝑙(𝑁(𝑘)1)+𝑐𝑢𝑡(𝑁(𝑘)1,𝑁(𝑘)2)𝑉𝑜𝑙(𝑁(𝑘)2)
 
и
𝑄(𝑘)𝑞𝑢𝑜𝑡𝑖𝑒𝑛𝑡=𝑐𝑢𝑡(𝑁(𝑘)1,𝑁(𝑘)2)min(𝑉𝑜𝑙(𝑁(𝑘)1),𝑉𝑜𝑙(𝑁(𝑘)2))
 

Для каждого разреза выведите четыре величины  𝑉𝑜𝑙(𝑁(𝑘)1)
 ,  𝑉𝑜𝑙(𝑁(𝑘)2)
 ,  𝑄(𝑘)𝑛𝑜𝑟𝑚𝑎𝑙𝑖𝑧𝑒𝑑
  и  𝑄(𝑘)𝑞𝑢𝑜𝑡𝑖𝑒𝑛𝑡
  на экран.

# первый разрез
vol_1_1 = nx.volume(G, S1)
vol_2_1 = nx.volume(G, T1)
Q_norm_1 = nx.normalized_cut_size(G, S1, T1)
Q_quot_1 = Q1_1 / (min(vol_1_1, vol_2_1))

vol_1_1, vol_2_1, Q_norm_1, Q_quot_1

# второй разрез
vol_1_2 = nx.volume(G, S2)
vol_2_2 = nx.volume(G, T2)
Q_norm_2 = nx.normalized_cut_size(G, S2, T2)
Q_quot_2 = Q2_1 / (min(vol_1_2, vol_2_2))

vol_1_2, vol_2_2, Q_norm_2, Q_quot_2
    """
    text = """
# первый разрез
vol_1_1 = nx.volume(G, S1)
vol_2_1 = nx.volume(G, T1)
Q_norm_1 = nx.normalized_cut_size(G, S1, T1)
Q_quot_1 = Q1_1 / (min(vol_1_1, vol_2_1))

vol_1_1, vol_2_1, Q_norm_1, Q_quot_1

# второй разрез
vol_1_2 = nx.volume(G, S2)
vol_2_2 = nx.volume(G, T2)
Q_norm_2 = nx.normalized_cut_size(G, S2, T2)
Q_quot_2 = Q2_1 / (min(vol_1_2, vol_2_2))

vol_1_2, vol_2_2, Q_norm_2, Q_quot_2
    """
    pc.copy(text)
    
    
def nine_cuts_3():
    """
3. Для графа G из задания 1 вычислите матрицу Лапласа двумя способами:

по формуле  𝐋=𝐃−𝐀
 ;
используя готовую функцию из networkx.
Выведите полученные матрицы на экран и покажите, что они равны.

A = nx.adjacency_matrix(G)
D = np.diag(A.sum(axis=1))
L = D - A
L

L_nx = nx.laplacian_matrix(G).toarray()
L_nx

np.array_equal(L, L_nx)
    """
    text = """
A = nx.adjacency_matrix(G)
D = np.diag(A.sum(axis=1))
L = D - A
L

L_nx = nx.laplacian_matrix(G).toarray()
L_nx

np.array_equal(L, L_nx)
    """
    pc.copy(text)
    

def nine_cuts_4():
    """
4. Для каждого из разрезов первого задания вычислите величины  𝑄(𝑘)
 , воспользовавшись матрицей Лапласа.
 
s1 = np.array([1 if node in S1 else -1 for node in G.nodes()])
Q1_L = int((s1.T @ L @ s1) / 4)
Q1_L

s2 = np.array([1 if node in S2 else -1 for node in G.nodes()])
Q2_L = int((s2.T @ L @ s2) / 4)
Q2_L

    """
    text = """
s1 = np.array([1 if node in S1 else -1 for node in G.nodes()])
Q1_L = int((s1.T @ L @ s1) / 4)
Q1_L

s2 = np.array([1 if node in S2 else -1 for node in G.nodes()])
Q2_L = int((s2.T @ L @ s2) / 4)
Q2_L
    """
    pc.copy(text)
    
    
def nine_cuts_5():
    """
5. Найдите собственные значения и собственные векторы матрицы Лапласа. Выведите на экран кратность нулевого собственного значения матрицы Лапласа. Найдите и выведите на экран количество компонент связности в графе G.

Создайте копию графа G с удаленными ребрами в соответствии с разрезом 𝐶(1)
. Повторите решение задачи для получившегося несвязного графа.

 Проверено на семинаре
Лемма: для неориентированной сети с неотрицательными весами кратность нулевого собственного значения матрицы лапласа равна количеству связных компонент сети , … , . Собственные пространство с.з. 0, над собственными векторами, отвечающимим с.з. 0 является оболочкой над индикаторными векторами этих компонент.

eigenvalues, eigenvectors = np.linalg.eig(L)
len_con_comp = len([comp for comp in nx.connected_components(G)])
print("Количество компонент связности:", len_con_comp)

eigenvalues_rounded = np.round(eigenvalues, 6)
eigenvalues_rounded

k_0 = Counter(eigenvalues_rounded)[0]
print("Кратность нулевого собственного значения:", k_0)

A_C1 = nx.adjacency_matrix(C1)
D_C1 = np.diag(A_C1.sum(axis=1))
L_C1 = D_C1 - A_C1

eigenvalues_C1, eigenvectors_C1 = np.linalg.eig(L_C1)
len_con_comp_C1 = len([comp for comp in nx.connected_components(C1)])
print("Количество компонент связности:", len_con_comp_C1)

eigenvalues_rounded_C1 = np.round(eigenvalues_C1, 6)
eigenvalues_rounded_C1

k_0 = Counter(eigenvalues_rounded_C1)[0]
print("Кратность нулевого собственного значения:", k_0)
    """
    text = """
eigenvalues, eigenvectors = np.linalg.eig(L)
len_con_comp = len([comp for comp in nx.connected_components(G)])
print("Количество компонент связности:", len_con_comp)

eigenvalues_rounded = np.round(eigenvalues, 6)
eigenvalues_rounded

k_0 = Counter(eigenvalues_rounded)[0]
print("Кратность нулевого собственного значения:", k_0)

A_C1 = nx.adjacency_matrix(C1)
D_C1 = np.diag(A_C1.sum(axis=1))
L_C1 = D_C1 - A_C1

eigenvalues_C1, eigenvectors_C1 = np.linalg.eig(L_C1)
len_con_comp_C1 = len([comp for comp in nx.connected_components(C1)])
print("Количество компонент связности:", len_con_comp_C1)

eigenvalues_rounded_C1 = np.round(eigenvalues_C1, 6)
eigenvalues_rounded_C1

k_0 = Counter(eigenvalues_rounded_C1)[0]
print("Кратность нулевого собственного значения:", k_0)
    """
    pc.copy(text)
    
    
def ten_cluster_1():
    """
1. Загрузите граф карате клуба. Вычислите матрицу Лапласа  𝐋=𝐃−𝐀
  при помощи готовой функции. Найдите собственные значения и собственные векторы матрицы Лапласа. При использовании функции явно укажите, что используется невзвешенная матрица смежности.

Выделите собственный вектор  𝐮
 , отвечающий минимальному ненулевому с.з.  𝜆𝑖≠0
  и выведите его на экран. Преобразуйте  𝐮
  в индикаторный вектор  𝐟
  по правилу  𝐟𝑖
  = sign( 𝐮𝑖)
 .

Визуализируйте  𝐮
 : по горизонтали откладывается номер узла, по вертикали значение соответствующей координаты вектора  𝐮
 

Визуализируйте граф, обозначив цветом узла компоненту, в которую попадает узел в соответствии с найденным разрезом.

G = nx.karate_club_graph()
L = nx.laplacian_matrix(G, weight=0).toarray()
L

eigenvalues, eigenvectors = np.linalg.eig(L)
eigenvalues, eigenvectors

nonzero_eigenvalues = eigenvalues[eigenvalues > 1e-8]
min_eig_val = list(eigenvalues).index([sorted(nonzero_eigenvalues)[0]])
u = eigenvectors[:, min_eig_val]
u, min_eig_val

f = np.sign(u)
f

plt.scatter(range(len(u)), u);
colors = ["blue" if f_i > 0 else "red" for f_i in f]
nx.draw(G, with_labels=True, node_color=colors)
    """
    text = """
G = nx.karate_club_graph()
L = nx.laplacian_matrix(G, weight=0).toarray()
L

eigenvalues, eigenvectors = np.linalg.eig(L)
eigenvalues, eigenvectors

nonzero_eigenvalues = eigenvalues[eigenvalues > 1e-8]
min_eig_val = list(eigenvalues).index([sorted(nonzero_eigenvalues)[0]])
u = eigenvectors[:, min_eig_val]
u, min_eig_val

f = np.sign(u)
f

plt.scatter(range(len(u)), u);
colors = ["blue" if f_i > 0 else "red" for f_i in f]
nx.draw(G, with_labels=True, node_color=colors)
    """
    pc.copy(text)
    
    
def ten_cluster_2():
    """
2. Повторите предыдущую задачу, используя нормализованную матрицу Лапласа  𝐋𝑠𝑦𝑚=𝐃−12𝐋𝐃−12
 . При расчете матрицы Лапласа явно укажите, что используется взвешенная матрица смежности  𝐖
 .
L_sym = nx.normalized_laplacian_matrix(G, weight="weight").toarray()
L_sym

eigenvalues, eigenvectors = np.linalg.eig(L_sym)
nonzero_eigenvalues = eigenvalues[eigenvalues > 1e-8]
min_eig_val = list(eigenvalues).index([sorted(nonzero_eigenvalues)[0]])
u = eigenvectors[:, min_eig_val]
min_eig_val

f = np.sign(u)
f

plt.scatter(range(len(u)), u);

colors = ["blue" if f_i > 0 else "red" for f_i in f]
nx.draw(G, with_labels=True, node_color=colors)

    """
    text = """
L_sym = nx.normalized_laplacian_matrix(G, weight="weight").toarray()
L_sym

eigenvalues, eigenvectors = np.linalg.eig(L_sym)
nonzero_eigenvalues = eigenvalues[eigenvalues > 1e-8]
min_eig_val = list(eigenvalues).index([sorted(nonzero_eigenvalues)[0]])
u = eigenvectors[:, min_eig_val]
min_eig_val

f = np.sign(u)
f

plt.scatter(range(len(u)), u);

colors = ["blue" if f_i > 0 else "red" for f_i in f]
nx.draw(G, with_labels=True, node_color=colors)
    """
    pc.copy(text)
    
    
def ten_cluster_3():
    """
3. Дан набор данных (X, y). Визуализируйте набор данных, отрисовав точки на плоскости и раскрасив в цвета, соответствующие меткам объектов y. Решите задачу кластеризации данных на 3 кластера при помощи алгоритма KMeans из sklearn. Визуализируйте полученный результат.

from sklearn.datasets import make_blobs
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=300, random_state=170)
X = np.dot(X, [[0.6, -0.6], [-0.4, 0.8]])

plt.scatter(X[:, 0], X[:, 1], c=y);

kmeans = KMeans(n_clusters=3, random_state=1, n_init="auto").fit(X)
labels = kmeans.labels_
plt.scatter(X[:, 0], X[:, 1], c=labels);
    """
    text = """
    from sklearn.datasets import make_blobs
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=300, random_state=170)
X = np.dot(X, [[0.6, -0.6], [-0.4, 0.8]])

plt.scatter(X[:, 0], X[:, 1], c=y);

kmeans = KMeans(n_clusters=3, random_state=1, n_init="auto").fit(X)
labels = kmeans.labels_
plt.scatter(X[:, 0], X[:, 1], c=labels);
    """
    pc.copy(text)
    
    
def ten_cluster_4():
    """
4. На основе датасета из предыдущего задания создайте граф при помощи функции radius_neighbors_graph из sklearn (укажите аргумент radius=1). Получите матрицу  𝑈∈ℝ300×3
 , состояющую из собственных векторов матрицы Лапласа, соответствующих трем наименьшим ненулевым собственным значениям. Решите задачу кластеризации при помощи алгоритма KMeans из sklearn на основе матрицы  𝑈
 . Визуализируйте полученный результат.
 
RNG = nx.from_numpy_array(radius_neighbors_graph(X, radius=1))
L_rng = nx.laplacian_matrix(RNG).toarray()
L_rng

eigenvalues, eigenvectors = np.linalg.eig(L_rng)
nonzero_eigenvalues = eigenvalues[eigenvalues > 1e-8]
indexes = [list(eigenvalues).index(i) for i in sorted(nonzero_eigenvalues)[:3]]
indexes

U = eigenvectors[:, indexes].real
U.shape

kmeans = KMeans(n_clusters=3, random_state=1).fit(U)
labels = kmeans.labels_
plt.scatter(X[:, 0], X[:, 1], c=labels);
    """
    text = """
RNG = nx.from_numpy_array(radius_neighbors_graph(X, radius=1))
L_rng = nx.laplacian_matrix(RNG).toarray()
L_rng

eigenvalues, eigenvectors = np.linalg.eig(L_rng)
nonzero_eigenvalues = eigenvalues[eigenvalues > 1e-8]
indexes = [list(eigenvalues).index(i) for i in sorted(nonzero_eigenvalues)[:3]]
indexes

U = eigenvectors[:, indexes].real
U.shape

kmeans = KMeans(n_clusters=3, random_state=1).fit(U)
labels = kmeans.labels_
plt.scatter(X[:, 0], X[:, 1], c=labels);
    """
    pc.copy(text)
    
    
def ten_cluster_5():
    """
5. Решите задачу кластеризации данных на 3 кластера при помощи алгоритма SpectralClustering из sklearn. Визуализируйте полученный результат. При создании модели кластеризации укажите значение аргумента affinity='nearest_neighbors'.

clustering = SpectralClustering(n_clusters=3,
                                affinity='nearest_neighbors',
                                random_state=1).fit(X)
clustering

labels = clustering.labels_
plt.scatter(X[:, 0], X[:, 1], c=labels);
    """
    text = """
clustering = SpectralClustering(n_clusters=3,
                                affinity='nearest_neighbors',
                                random_state=1).fit(X)
clustering

labels = clustering.labels_
plt.scatter(X[:, 0], X[:, 1], c=labels);
    """
    pc.copy(text)
    
    
def eleven_gcnn_mpf_1():
    """
1. Загрузите граф из файла karate.graphml. Визуализируйте граф, раскрасив в цвета классов (атрибут label у узлов).

Получите невзвешенную матрицу смежности  𝐀
  в виде тензора torch и тензор меток для каждого узла. Выведите полученные тензоры на экран.
  
G = nx.read_graphml("data/karate.graphml")
pos = nx.spring_layout(G)
labels = [attr["label"] for node, attr in G.nodes(data=True)]
nx.draw(G, pos=pos, with_labels=True, node_color=labels)

A = th.tensor(nx.adjacency_matrix(G, weight=0).toarray(), dtype=th.float32)
A

th.tensor([[attr["label"]] for node, attr in G.nodes(data=True)], dtype=th.float32)
    """
    text = """
G = nx.read_graphml("data/karate.graphml")
pos = nx.spring_layout(G)
labels = [attr["label"] for node, attr in G.nodes(data=True)]
nx.draw(G, pos=pos, with_labels=True, node_color=labels)

A = th.tensor(nx.adjacency_matrix(G, weight=0).toarray(), dtype=th.float32)
A

th.tensor([[attr["label"]] for node, attr in G.nodes(data=True)], dtype=th.float32)
    """
    pc.copy(text)
    
    
def eleven_gcnn_mpf_2():
    """
2. Найдите стохастическую матрицу  𝐀̂ 
 .

𝐀̂ =𝐃−1𝐀
 

Сгенерируйте тензор  𝐗
  размера Nx5 с признаками узлов, используя torch.rand. Выполните один шаг рассылки сообщений при помощи матричного умножения.

𝐗̂ =𝐀̂ 𝐗
 

Выполните этот же шаг рассылки сообщений, явно проитерировавшись по узлам графа и усреднив атрибуты соседей. Сравните два полученных тензора и покажите, что они равны.

degrees = th.sum(A, axis=1)
D = th.diag(degrees)
D

degrees = th.sum(A, axis=1)
D = th.diag(degrees)
A_hat = th.linalg.inv(D) @ A
A_hat

N = len(G)
X = th.rand((N, 5))
X.shape
X_hat = A_hat @ X
X_hat
X_hat_2 = th.zeros_like(X_hat)
for i in range(N):
    neib_ind = th.where(A[i] == 1)[0].tolist()
    X_hat_2[i] = th.mean(X[neib_ind], dim=0)
    
th.allclose(X_hat, X_hat_2)
    """
    text = """
degrees = th.sum(A, axis=1)
D = th.diag(degrees)
D

degrees = th.sum(A, axis=1)
D = th.diag(degrees)
A_hat = th.linalg.inv(D) @ A
A_hat

N = len(G)
X = th.rand((N, 5))
X.shape
X_hat = A_hat @ X
X_hat
X_hat_2 = th.zeros_like(X_hat)
for i in range(N):
    neib_ind = th.where(A[i] == 1)[0].tolist()
    X_hat_2[i] = th.mean(X[neib_ind], dim=0)
    
th.allclose(X_hat, X_hat_2)
    """
    pc.copy(text)
    
    
def eleven_gcnn_mpf_3():
    """
3. Опишите слой GCNLayer графовой сверточной нейронной сети. Создайте слой с n_outputs=3 и пропустите через этот слой матрицу смежности графа и тензор признаков  𝐗=𝐄
 . Выведите форму полученного тензора на экран.
 
import torch.nn as nn
import torch as th
from typing import Callable

class GCNLayer(nn.Module):
    def __init__(self, n_inputs: int, n_outputs: int, activation: Callable | None = None) -> None:
        super().__init__()

        # self.W - веса слоя
        W = th.empty(n_inputs, n_outputs, dtype=th.float32)
        nn.init.xavier_uniform_(W)
        self.W = nn.Parameter(W, requires_grad=True)
        
        # self.activation - функция активации, применяется после рассылки сообщений, может отсутствовать
        self.activation = activation

    def normalize_matrix(self, A: th.Tensor) -> th.Tensor:
        #A - исходная матрица смежности графа
        degrees = th.sum(A, axis=1)
        D = th.diag(degrees)
        A_hat = th.linalg.inv(D) @ A
        return A_hat

    def forward(self, A: th.Tensor, X: th.Tensor) -> th.Tensor:
        #A - исходная матрица смежности графа
        #X - матрица признаков узлов
        
        out = self.normalize_matrix(A) @ X @ self.W
        if self.activation:
            out = self.activation(out)
        return out
        
model = GCNLayer(n_inputs=len(G), n_outputs=3)
out = model(A, th.eye(len(G)))
out
out.shape
    """
    text = """
import torch.nn as nn
import torch as th
from typing import Callable

class GCNLayer(nn.Module):
    def __init__(self, n_inputs: int, n_outputs: int, activation: Callable | None = None) -> None:
        super().__init__()

        # self.W - веса слоя
        W = th.empty(n_inputs, n_outputs, dtype=th.float32)
        nn.init.xavier_uniform_(W)
        self.W = nn.Parameter(W, requires_grad=True)
        
        # self.activation - функция активации, применяется после рассылки сообщений, может отсутствовать
        self.activation = activation

    def normalize_matrix(self, A: th.Tensor) -> th.Tensor:
        #A - исходная матрица смежности графа
        degrees = th.sum(A, axis=1)
        D = th.diag(degrees)
        A_hat = th.linalg.inv(D) @ A
        return A_hat

    def forward(self, A: th.Tensor, X: th.Tensor) -> th.Tensor:
        #A - исходная матрица смежности графа
        #X - матрица признаков узлов
        
        out = self.normalize_matrix(A) @ X @ self.W
        if self.activation:
            out = self.activation(out)
        return out
        
model = GCNLayer(n_inputs=len(G), n_outputs=3)
out = model(A, th.eye(len(G)))
out
out.shape
    """
    pc.copy(text)
    
    
def eleven_gcnn_mpf_4():
    """
4. Используя графовую сверточную нейронную сеть, решите задачу классификации узлов. Создайте модель, состоящую из двух последовательно идущих слоев GCNLayer. Первый слой имеет 2 нейрона и гиперболический тангенс в качестве функции активации. Количество нейронов во втором слое определяется количеством классов в задаче. Настройте модель (в качестве признаков узлов используйте единичную матрицу  𝐗=𝐄
 ).

После завершения процесса обучения пропустите данные через первый слой и визуализируйте граф, используя полученные векторные представления в качестве координат. Раскрасьте узлы в цвет соответствующих классов.

class GCNModel(nn.Module):
    def __init__(self, n_inputs, n_outputs, n_hidden=2):
        super().__init__()
        self.act = nn.Tanh()
        self.layer_1 = GCNLayer(n_inputs=n_inputs, n_outputs=n_hidden, activation=self.act)
        self.layer_2 = GCNLayer(n_inputs=n_hidden, n_outputs=n_outputs)
        
    def forward(self, A, X):
        out = self.layer_1(A, X)
        out = self.layer_2(A, out)
        return out
        
gcn_model = GCNModel(n_inputs=len(G), n_outputs=len(np.unique(labels)))
loss_func = nn.CrossEntropyLoss()
optimizer = th.optim.Adam(gcn_model.parameters(), lr=0.005)
labels = th.tensor([attr["label"] for node, attr in G.nodes(data=True)], dtype=th.long)

EPOCHS = 1300

for i in tqdm(range(EPOCHS)):
    output = gcn_model(A, th.eye(len(G)))
    loss = loss_func(output, labels)
    pred = th.argmax(output, dim=1)
    acc = (pred == labels).float().mean().item()
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    if i % 200 == 0:
        print(f"Epoch: {i}, Loss: {round(loss.item(), 5)}, Accuracy: {acc}")
        
print(f"Epoch: {i}, Loss: {round(loss.item(), 5)}, Accuracy: {acc}")

positions = gcn_model.layer_1(A, th.eye(len(G))).tolist()
positions

new_labels = th.argmax(output, dim=1)
new_labels

pos = {node:positions[ind] for ind, node in enumerate(G.nodes())}
nx.draw(G, with_labels=True, pos=pos, node_color=new_labels)
    """
    text = """
class GCNModel(nn.Module):
    def __init__(self, n_inputs, n_outputs, n_hidden=2):
        super().__init__()
        self.act = nn.Tanh()
        self.layer_1 = GCNLayer(n_inputs=n_inputs, n_outputs=n_hidden, activation=self.act)
        self.layer_2 = GCNLayer(n_inputs=n_hidden, n_outputs=n_outputs)
        
    def forward(self, A, X):
        out = self.layer_1(A, X)
        out = self.layer_2(A, out)
        return out
        
gcn_model = GCNModel(n_inputs=len(G), n_outputs=len(np.unique(labels)))
loss_func = nn.CrossEntropyLoss()
optimizer = th.optim.Adam(gcn_model.parameters(), lr=0.005)
labels = th.tensor([attr["label"] for node, attr in G.nodes(data=True)], dtype=th.long)

EPOCHS = 1300

for i in tqdm(range(EPOCHS)):
    output = gcn_model(A, th.eye(len(G)))
    loss = loss_func(output, labels)
    pred = th.argmax(output, dim=1)
    acc = (pred == labels).float().mean().item()
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    if i % 200 == 0:
        print(f"Epoch: {i}, Loss: {round(loss.item(), 5)}, Accuracy: {acc}")
        
print(f"Epoch: {i}, Loss: {round(loss.item(), 5)}, Accuracy: {acc}")

positions = gcn_model.layer_1(A, th.eye(len(G))).tolist()
positions

new_labels = th.argmax(output, dim=1)
new_labels

pos = {node:positions[ind] for ind, node in enumerate(G.nodes())}
nx.draw(G, with_labels=True, pos=pos, node_color=new_labels)
    """
    pc.copy(text)
    
    
def eleven_gcnn_mpf_5():
    """
5. Повторите решение предыдущей задачи, используя в качестве признаков  𝐗
  следующие характеристики узлов (каждый узел описывается четырьмя характеристиками):

количество треугольников, в которые входит узел;
степень узла;
эксцентриситет узла;
центральность по собственному вектору для узла.
Для ускорения процесса обучения стандартизуйте тензор признаков.

zer = th.zeros_like(A)
zer[:, 0] = th.tensor(list(nx.triangles(G).values()), dtype=th.float32)
zer[:, 1] = th.tensor(list(dict(G.degree).values()), dtype=th.float32)
zer[:, 2] = th.tensor(list(nx.eccentricity(G).values()), dtype=th.float32)
zer[:, 3] = th.tensor(list(nx.eigenvector_centrality(G).values()), dtype=th.float32)

scaler = StandardScaler()
stand_X = th.tensor(scaler.fit_transform(zer), dtype=th.float32)
stand_X[:5]

gcn_model = GCNModel(n_inputs=stand_X.shape[1], n_outputs=len(np.unique(labels)))
loss_func = nn.CrossEntropyLoss()
optimizer = th.optim.Adam(gcn_model.parameters(), lr=0.005)

EPOCHS = 1500

for i in tqdm(range(EPOCHS)):
    output = gcn_model(A, stand_X)
    loss = loss_func(output, labels)
    pred = th.argmax(output, dim=1)
    acc = (pred == labels).float().mean().item()
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    if i % 200 == 0:
        print(f"Epoch: {i}, Loss: {round(loss.item(), 5)}, Accuracy: {acc}")
        
print(f"Epoch: {i}, Loss: {round(loss.item(), 5)}, Accuracy: {acc}")

positions = gcn_model.layer_1(A, stand_X).tolist()
positions

new_labels = th.argmax(output, dim=1)
new_labels

pos = {node:positions[ind] for ind, node in enumerate(G.nodes())}
nx.draw(G, with_labels=True, pos=pos, node_color=new_labels)
    """
    text = """
zer = th.zeros_like(A)
zer[:, 0] = th.tensor(list(nx.triangles(G).values()), dtype=th.float32)
zer[:, 1] = th.tensor(list(dict(G.degree).values()), dtype=th.float32)
zer[:, 2] = th.tensor(list(nx.eccentricity(G).values()), dtype=th.float32)
zer[:, 3] = th.tensor(list(nx.eigenvector_centrality(G).values()), dtype=th.float32)

scaler = StandardScaler()
stand_X = th.tensor(scaler.fit_transform(zer), dtype=th.float32)
stand_X[:5]

gcn_model = GCNModel(n_inputs=stand_X.shape[1], n_outputs=len(np.unique(labels)))
loss_func = nn.CrossEntropyLoss()
optimizer = th.optim.Adam(gcn_model.parameters(), lr=0.005)

EPOCHS = 1500

for i in tqdm(range(EPOCHS)):
    output = gcn_model(A, stand_X)
    loss = loss_func(output, labels)
    pred = th.argmax(output, dim=1)
    acc = (pred == labels).float().mean().item()
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    if i % 200 == 0:
        print(f"Epoch: {i}, Loss: {round(loss.item(), 5)}, Accuracy: {acc}")
        
print(f"Epoch: {i}, Loss: {round(loss.item(), 5)}, Accuracy: {acc}")

positions = gcn_model.layer_1(A, stand_X).tolist()
positions

new_labels = th.argmax(output, dim=1)
new_labels

pos = {node:positions[ind] for ind, node in enumerate(G.nodes())}
nx.draw(G, with_labels=True, pos=pos, node_color=new_labels)
    """
    pc.copy(text)
    
    
def twelve_gnn_nodes_1():
    """
1. Загрузите граф Planetoid/Cora из torch_geometric.datasets.

Выведите на экран:

количество узлов графа;
количество ребер графа;
размерность признаков узлов;
количество узлов для обучения, валидации и тестирования.
Решите задачу классификации узлов графа, используя только полносвязные слои torch.nn.Linear (создайте модель из двух слоев). Для обучения используйте пакетный градиентный спуск (не разбивайте на батчи). Обратите внимание, что настройка весов модели должна проводиться только на основе примеров из обучающей выборки. Посчитайте и выведите на экран значение accuracy на тестовой выборке.

dataset = Planetoid(name="Cora", root="./tmp")
data = dataset[0]
data
print("количество узлов графа:", dataset.x.shape[0])
print("количество ребер графа:", dataset.edge_index.shape[1])
print("размерность признаков узлов:", dataset.x.shape[1])
print("количество узлов для обучения:", data.train_mask.sum().item())
print("количество узлов для валидации:", data.val_mask.sum().item())
print("количество узлов для тестирования:", data.test_mask.sum().item())

class LinearModel(nn.Module):
    def __init__(self, n_inputs, n_outputs):
        super().__init__()
        self.layer_1 = nn.Linear(n_inputs, 128)
        self.layer_2 = nn.Linear(128, n_outputs)
        self.tanh = nn.Tanh()
        
    def forward(self, x):
        out = self.layer_1(x)
        out = self.tanh(out)
        out = self.layer_2(out)
        return out
        
model_linear = LinearModel(1433, 7)
optimizer = torch.optim.Adam(model_linear.parameters(), lr=0.001)
loss_func = nn.CrossEntropyLoss()

EPOCHS = 40

for epoch in tqdm(range(EPOCHS), desc="Training Linear Model"):
    model_linear.train()
    optimizer.zero_grad()
    
    outputs = model_linear(data.x)
    loss = loss_func(outputs[data.train_mask], data.y[data.train_mask])
    
    loss.backward()
    optimizer.step()
    
    if (epoch % 20 == 0) or (epoch == EPOCHS-1):
        model_linear.eval()
        with torch.no_grad():
            train_outputs = outputs[data.train_mask]
            train_acc = accuracy_score(train_outputs.argmax(dim=1), 
                                     data.y[data.train_mask])
            
            val_outputs = model_linear(data.x)[data.val_mask]
            val_loss = loss_func(val_outputs, data.y[data.val_mask])
            val_acc = accuracy_score(val_outputs.argmax(dim=1), 
                                   data.y[data.val_mask])
            
            print("Epoch", epoch + 1)
            print(f"Train Loss {loss.item():.4f}, Train Accuracy {train_acc:.4f}")
            print(f"Val Loss {val_loss.item():.4f}, Val Accuracy {val_acc:.4f}", end="\n\n")
            
test_outputs = model_linear(data.x)[data.test_mask]
test_acc = accuracy_score(test_outputs.argmax(dim=1), 
                       data.y[data.test_mask])


print(f"Test Accuracy {test_acc}")
    """
    text = """
dataset = Planetoid(name="Cora", root="./tmp")
data = dataset[0]
data
print("количество узлов графа:", dataset.x.shape[0])
print("количество ребер графа:", dataset.edge_index.shape[1])
print("размерность признаков узлов:", dataset.x.shape[1])
print("количество узлов для обучения:", data.train_mask.sum().item())
print("количество узлов для валидации:", data.val_mask.sum().item())
print("количество узлов для тестирования:", data.test_mask.sum().item())

class LinearModel(nn.Module):
    def __init__(self, n_inputs, n_outputs):
        super().__init__()
        self.layer_1 = nn.Linear(n_inputs, 128)
        self.layer_2 = nn.Linear(128, n_outputs)
        self.tanh = nn.Tanh()
        
    def forward(self, x):
        out = self.layer_1(x)
        out = self.tanh(out)
        out = self.layer_2(out)
        return out
        
model_linear = LinearModel(1433, 7)
optimizer = torch.optim.Adam(model_linear.parameters(), lr=0.001)
loss_func = nn.CrossEntropyLoss()

EPOCHS = 40

for epoch in tqdm(range(EPOCHS), desc="Training Linear Model"):
    model_linear.train()
    optimizer.zero_grad()
    
    outputs = model_linear(data.x)
    loss = loss_func(outputs[data.train_mask], data.y[data.train_mask])
    
    loss.backward()
    optimizer.step()
    
    if (epoch % 20 == 0) or (epoch == EPOCHS-1):
        model_linear.eval()
        with torch.no_grad():
            train_outputs = outputs[data.train_mask]
            train_acc = accuracy_score(train_outputs.argmax(dim=1), 
                                     data.y[data.train_mask])
            
            val_outputs = model_linear(data.x)[data.val_mask]
            val_loss = loss_func(val_outputs, data.y[data.val_mask])
            val_acc = accuracy_score(val_outputs.argmax(dim=1), 
                                   data.y[data.val_mask])
            
            print("Epoch", epoch + 1)
            print(f"Train Loss {loss.item():.4f}, Train Accuracy {train_acc:.4f}")
            print(f"Val Loss {val_loss.item():.4f}, Val Accuracy {val_acc:.4f}", end="\n\n")
            
test_outputs = model_linear(data.x)[data.test_mask]
test_acc = accuracy_score(test_outputs.argmax(dim=1), 
                       data.y[data.test_mask])


print(f"Test Accuracy {test_acc}")
    """
    pc.copy(text)
    
    
def twelve_gnn_nodes_2():
    """
2. Решите задачу 1, используя два слоя torch_geometric.nn.GCNConv вместо полносвязных слоев torch.nn.Linear.

class GCNConvModel(nn.Module):
    def __init__(self, n_inputs, n_outputs):
        super().__init__()
        self.layer_1 = GCNConv(n_inputs, 128)
        self.layer_2 = GCNConv(128, n_outputs)
        self.tanh = nn.Tanh()
        
    def forward(self, x, edge_index):
        out = self.layer_1(x, edge_index)
        out = self.tanh(out)
        out = self.layer_2(out, edge_index)
        return out
        
model_gcn = GCNConvModel(1433, 7)
optimizer = torch.optim.Adam(model_gcn.parameters(), lr=0.001)
loss_func = nn.CrossEntropyLoss()

EPOCHS = 40

for epoch in tqdm(range(EPOCHS), desc="Training GCN Model"):
    model_gcn.train()
    optimizer.zero_grad()
    
    outputs = model_gcn(data.x, data.edge_index)
    loss = loss_func(outputs[data.train_mask], data.y[data.train_mask])
    
    loss.backward()
    optimizer.step()
    
    if (epoch % 20 == 0) or (epoch == EPOCHS-1):
        model_gcn.eval()
        with torch.no_grad():
            train_outputs = outputs[data.train_mask]
            train_acc = accuracy_score(train_outputs.argmax(dim=1), 
                                     data.y[data.train_mask])
            
            val_outputs = model_gcn(data.x, data.edge_index)[data.val_mask]
            val_loss = loss_func(val_outputs, data.y[data.val_mask])
            val_acc = accuracy_score(val_outputs.argmax(dim=1), 
                                   data.y[data.val_mask])
            
            print("Epoch", epoch + 1)
            print(f"Train Loss {loss.item():.4f}, Train Accuracy {train_acc:.4f}")
            print(f"Val Loss {val_loss.item():.4f}, Val Accuracy {val_acc:.4f}", end="\n\n")
            
test_outputs = model_gcn(data.x, data.edge_index)[data.test_mask]
test_acc = accuracy_score(test_outputs.argmax(dim=1), data.y[data.test_mask])

print(f"Test Accuracy {test_acc}")
    """
    text = """
class GCNConvModel(nn.Module):
    def __init__(self, n_inputs, n_outputs):
        super().__init__()
        self.layer_1 = GCNConv(n_inputs, 128)
        self.layer_2 = GCNConv(128, n_outputs)
        self.tanh = nn.Tanh()
        
    def forward(self, x, edge_index):
        out = self.layer_1(x, edge_index)
        out = self.tanh(out)
        out = self.layer_2(out, edge_index)
        return out
        
model_gcn = GCNConvModel(1433, 7)
optimizer = torch.optim.Adam(model_gcn.parameters(), lr=0.001)
loss_func = nn.CrossEntropyLoss()

EPOCHS = 40

for epoch in tqdm(range(EPOCHS), desc="Training GCN Model"):
    model_gcn.train()
    optimizer.zero_grad()
    
    outputs = model_gcn(data.x, data.edge_index)
    loss = loss_func(outputs[data.train_mask], data.y[data.train_mask])
    
    loss.backward()
    optimizer.step()
    
    if (epoch % 20 == 0) or (epoch == EPOCHS-1):
        model_gcn.eval()
        with torch.no_grad():
            train_outputs = outputs[data.train_mask]
            train_acc = accuracy_score(train_outputs.argmax(dim=1), 
                                     data.y[data.train_mask])
            
            val_outputs = model_gcn(data.x, data.edge_index)[data.val_mask]
            val_loss = loss_func(val_outputs, data.y[data.val_mask])
            val_acc = accuracy_score(val_outputs.argmax(dim=1), 
                                   data.y[data.val_mask])
            
            print("Epoch", epoch + 1)
            print(f"Train Loss {loss.item():.4f}, Train Accuracy {train_acc:.4f}")
            print(f"Val Loss {val_loss.item():.4f}, Val Accuracy {val_acc:.4f}", end="\n\n")
            
test_outputs = model_gcn(data.x, data.edge_index)[data.test_mask]
test_acc = accuracy_score(test_outputs.argmax(dim=1), data.y[data.test_mask])

print(f"Test Accuracy {test_acc}")
    """
    pc.copy(text)
    
    
def twelve_gnn_nodes_3():
    """
3. Воспользовавшись необученной моделью из предыдущего задания, получите прогнозы для всех узлов графа. Уменьшите размерность полученных прогнозов до 2 при помощи алгоритма t-SNE (sklearn.manifold.TSNE). Визуализируйте точки на плоскости, используя полученные значения в качестве координат. Раскрасьте точки в цвета, соответствующим меткам узлов.

Повторите данную процедуру, используя обученную модель. Сравните результаты и сделайте выводы.

model_gcn_new = GCNConvModel(1433, 7)
outputs = model_gcn_new(data.x, data.edge_index)
outputs.shape

tsne_outputs = TSNE(n_components=2, random_state=42).fit_transform(outputs.detach().numpy())
tsne_outputs

plt.figure(figsize=(12, 6))
plt.scatter(tsne_outputs[:, 0], tsne_outputs[:, 1], c=data.y.numpy(), alpha=0.8)
plt.title('t-SNE Visualization (Untrained Model)')
plt.colorbar()
plt.show()

outputs = model_gcn(data.x, data.edge_index)
tsne_outputs = TSNE(n_components=2, random_state=42).fit_transform(outputs.detach().numpy())

plt.figure(figsize=(12, 6))
plt.scatter(tsne_outputs[:, 0], tsne_outputs[:, 1], c=data.y.numpy(), alpha=0.8)
plt.title('t-SNE Visualization (Train Model)')
plt.colorbar()
plt.show()
    """
    text = """
model_gcn_new = GCNConvModel(1433, 7)
outputs = model_gcn_new(data.x, data.edge_index)
outputs.shape

tsne_outputs = TSNE(n_components=2, random_state=42).fit_transform(outputs.detach().numpy())
tsne_outputs

plt.figure(figsize=(12, 6))
plt.scatter(tsne_outputs[:, 0], tsne_outputs[:, 1], c=data.y.numpy(), alpha=0.8)
plt.title('t-SNE Visualization (Untrained Model)')
plt.colorbar()
plt.show()

outputs = model_gcn(data.x, data.edge_index)
tsne_outputs = TSNE(n_components=2, random_state=42).fit_transform(outputs.detach().numpy())

plt.figure(figsize=(12, 6))
plt.scatter(tsne_outputs[:, 0], tsne_outputs[:, 1], c=data.y.numpy(), alpha=0.8)
plt.title('t-SNE Visualization (Train Model)')
plt.colorbar()
plt.show()
    """
    pc.copy(text)
    
    
def twelve_gnn_nodes_4():
    """
4. Предыдущие решения не используют узлы, находящиеся в валидационном множестве. Решите задачу 2, используя валидационное множество для выполнения ранней остановки.

model_gcn = GCNConvModel(1433, 7)
optimizer = torch.optim.Adam(model_gcn.parameters(), lr=0.001)
loss_func = nn.CrossEntropyLoss()

EPOCHS = 100
best_val_acc = 0
patience = 5
no_improve_epochs = 0
stop_training = False

for epoch in tqdm(range(EPOCHS), desc="Training GCN Model with early stopping"):
    if stop_training:
        print(f"Early stopping epoch {epoch + 1}")
        break
    
    model_gcn.train()
    optimizer.zero_grad()
    
    outputs = model_gcn(data.x, data.edge_index)
    loss = loss_func(outputs[data.train_mask], data.y[data.train_mask])
    
    loss.backward()
    optimizer.step()
    
    model_gcn.eval()
    with torch.no_grad():
        val_outputs = model_gcn(data.x, data.edge_index)[data.val_mask]
        val_acc = accuracy_score(val_outputs.argmax(dim=1), data.y[data.val_mask])
        
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            no_improve_epochs = 0
            torch.save(model_gcn.state_dict(), 'data/gcn_best_model.pth')
        else:
            no_improve_epochs += 1
            
        if no_improve_epochs >= patience:
            stop_training = True
    
    if (epoch % 5 == 0) or (epoch == EPOCHS-1) or stop_training:
        with torch.no_grad():
            train_outputs = outputs[data.train_mask]
            train_acc = accuracy_score(train_outputs.argmax(dim=1), data.y[data.train_mask])
            val_loss = loss_func(val_outputs, data.y[data.val_mask])
            
            print("Epoch", epoch + 1)
            print(f"Train Loss {loss.item():.4f}, Train Accuracy {train_acc:.4f}")
            print(f"Val Loss {val_loss.item():.4f}, Val Accuracy {val_acc:.4f}")
            print(f"Best Val Accuracy: {best_val_acc:.4f}")
            print(f"No improve {no_improve_epochs} epochs", end="\n\n")

            
if stop_training:
    model_gcn.load_state_dict(torch.load('data/gcn_best_model.pth'))
    
test_outputs = model_gcn(data.x, data.edge_index)[data.test_mask]
test_acc = accuracy_score(test_outputs.argmax(dim=1), data.y[data.test_mask])

print(f"Test Accuracy {test_acc}")
    """
    text = """
model_gcn = GCNConvModel(1433, 7)
optimizer = torch.optim.Adam(model_gcn.parameters(), lr=0.001)
loss_func = nn.CrossEntropyLoss()

EPOCHS = 100
best_val_acc = 0
patience = 5
no_improve_epochs = 0
stop_training = False

for epoch in tqdm(range(EPOCHS), desc="Training GCN Model with early stopping"):
    if stop_training:
        print(f"Early stopping epoch {epoch + 1}")
        break
    
    model_gcn.train()
    optimizer.zero_grad()
    
    outputs = model_gcn(data.x, data.edge_index)
    loss = loss_func(outputs[data.train_mask], data.y[data.train_mask])
    
    loss.backward()
    optimizer.step()
    
    model_gcn.eval()
    with torch.no_grad():
        val_outputs = model_gcn(data.x, data.edge_index)[data.val_mask]
        val_acc = accuracy_score(val_outputs.argmax(dim=1), data.y[data.val_mask])
        
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            no_improve_epochs = 0
            torch.save(model_gcn.state_dict(), 'data/gcn_best_model.pth')
        else:
            no_improve_epochs += 1
            
        if no_improve_epochs >= patience:
            stop_training = True
    
    if (epoch % 5 == 0) or (epoch == EPOCHS-1) or stop_training:
        with torch.no_grad():
            train_outputs = outputs[data.train_mask]
            train_acc = accuracy_score(train_outputs.argmax(dim=1), data.y[data.train_mask])
            val_loss = loss_func(val_outputs, data.y[data.val_mask])
            
            print("Epoch", epoch + 1)
            print(f"Train Loss {loss.item():.4f}, Train Accuracy {train_acc:.4f}")
            print(f"Val Loss {val_loss.item():.4f}, Val Accuracy {val_acc:.4f}")
            print(f"Best Val Accuracy: {best_val_acc:.4f}")
            print(f"No improve {no_improve_epochs} epochs", end="\n\n")

            
if stop_training:
    model_gcn.load_state_dict(torch.load('data/gcn_best_model.pth'))
    
test_outputs = model_gcn(data.x, data.edge_index)[data.test_mask]
test_acc = accuracy_score(test_outputs.argmax(dim=1), data.y[data.test_mask])

print(f"Test Accuracy {test_acc}")
    """
    pc.copy(text)
    
    
def twelve_gnn_nodes_5():
    """
5. Повторите решение задачи 4, сравнив несколько различных слоев:

GCNConv
SAGEConv (укажите aggr="mean")
GATConv (выберите для первого слоя heads > 1)
Выведите результат в виде таблицы:

Модель	Loss на обучении	Acc на обучении	Acc на тесте	Кол-во эпох до ранней остановк

class SAGEConvModel(nn.Module):
    def __init__(self, n_inputs, n_outputs):
        super().__init__()
        self.layer_1 = SAGEConv(n_inputs, 128, aggr="mean")
        self.layer_2 = SAGEConv(128, n_outputs, aggr="mean")
        self.tanh = nn.Tanh()
        
    def forward(self, x, edge_index):
        out = self.layer_1(x, edge_index)
        out = self.tanh(out)
        out = self.layer_2(out, edge_index)
        return out
        
class GATConvModel(nn.Module):
    def __init__(self, n_inputs, n_outputs, first_heads=2):
        super().__init__()
        self.layer_1 = GATConv(n_inputs, 128, heads=first_heads)
        self.layer_2 = GATConv(128*first_heads, n_outputs)
        self.tanh = nn.Tanh()
        
    def forward(self, x, edge_index):
        out = self.layer_1(x, edge_index)
        out = self.tanh(out)
        out = self.layer_2(out, edge_index)
        return out
        
def train_with_early_stopping(data, model, lr=0.001, max_epochs=100, patience=5):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_func = nn.CrossEntropyLoss()
    
    best_val_acc = 0
    no_improve_epochs = 0
    stop_training = False
    best_model_state = None
    metrics = {}
    
    model_name = model.__class__.__name__
    
    for epoch in tqdm(range(max_epochs), desc=f"Training Model {model_name}"):
        if stop_training:
            print(f"Early stopping epoch {epoch + 1}")
            break
        
        model.train()
        optimizer.zero_grad()
        
        outputs = model(data.x, data.edge_index)
        train_loss = loss_func(outputs[data.train_mask], data.y[data.train_mask])
        train_loss.backward()
        optimizer.step()
        
        model.eval()
        with torch.no_grad():
            val_outputs = model(data.x, data.edge_index)[data.val_mask]
            val_acc = accuracy_score(val_outputs.argmax(dim=1), data.y[data.val_mask])
            
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                no_improve_epochs = 0
                best_model_state = model.state_dict()
            else:
                no_improve_epochs += 1
                
            if no_improve_epochs >= patience:
                stop_training = True
        
        if (epoch % 5 == 0) or (epoch == max_epochs-1) or stop_training:
            with torch.no_grad():
                train_outputs = outputs[data.train_mask]
                train_acc = accuracy_score(train_outputs.argmax(dim=1), data.y[data.train_mask])
                
                test_outputs = model(data.x, data.edge_index)[data.test_mask]
                test_acc = accuracy_score(test_outputs.argmax(dim=1), data.y[data.test_mask])
                
                print(f"Epoch {epoch + 1}")
                print(f"Train Loss: {train_loss.item():.4f} | Train Acc: {train_acc:.4f}")
                print(f"Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}")
                print(f"Best Val Accuracy: {best_val_acc:.4f}")
                print(f"No improve {no_improve_epochs} epochs", end="\n\n")
    
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
    
    model.eval()
    with torch.no_grad():
        train_outputs = model(data.x, data.edge_index)[data.train_mask]
        final_train_loss = loss_func(train_outputs, data.y[data.train_mask]).item()
        final_train_acc = accuracy_score(train_outputs.argmax(dim=1), data.y[data.train_mask])
        
        test_outputs = model(data.x, data.edge_index)[data.test_mask]
        final_test_acc = accuracy_score(test_outputs.argmax(dim=1), data.y[data.test_mask])
    
    return {
        'model_name': model_name,
        'train_loss': final_train_loss,
        'train_acc': final_train_acc,
        'test_acc': final_test_acc,
        'epochs': epoch + 1
    }
model_gcn = GCNConvModel(1433, 7)
model_sage = SAGEConvModel(1433, 7)
model_gat = GATConvModel(1433, 7)

gcn = train_with_early_stopping(model=model_gcn, data=data)
sage = train_with_early_stopping(model=model_sage, data=data)
gat = train_with_early_stopping(model=model_gat, data=data)


all_results = [gcn, sage, gat]

results_df = pd.DataFrame({
    'Модель': [res['model_name'] for res in all_results],
    'Loss на обучении': [f"{res['train_loss']:.4f}" for res in all_results],
    'Acc на обучении': [f"{res['train_acc']:.4f}" for res in all_results],
    'Acc на тесте': [f"{res['test_acc']:.4f}" for res in all_results],
    'Кол-во эпох до ранней остановки': [res['epochs'] for res in all_results],
})
results_df
    """
    text = """
class SAGEConvModel(nn.Module):
    def __init__(self, n_inputs, n_outputs):
        super().__init__()
        self.layer_1 = SAGEConv(n_inputs, 128, aggr="mean")
        self.layer_2 = SAGEConv(128, n_outputs, aggr="mean")
        self.tanh = nn.Tanh()
        
    def forward(self, x, edge_index):
        out = self.layer_1(x, edge_index)
        out = self.tanh(out)
        out = self.layer_2(out, edge_index)
        return out
        
class GATConvModel(nn.Module):
    def __init__(self, n_inputs, n_outputs, first_heads=2):
        super().__init__()
        self.layer_1 = GATConv(n_inputs, 128, heads=first_heads)
        self.layer_2 = GATConv(128*first_heads, n_outputs)
        self.tanh = nn.Tanh()
        
    def forward(self, x, edge_index):
        out = self.layer_1(x, edge_index)
        out = self.tanh(out)
        out = self.layer_2(out, edge_index)
        return out
        
def train_with_early_stopping(data, model, lr=0.001, max_epochs=100, patience=5):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_func = nn.CrossEntropyLoss()
    
    best_val_acc = 0
    no_improve_epochs = 0
    stop_training = False
    best_model_state = None
    metrics = {}
    
    model_name = model.__class__.__name__
    
    for epoch in tqdm(range(max_epochs), desc=f"Training Model {model_name}"):
        if stop_training:
            print(f"Early stopping epoch {epoch + 1}")
            break
        
        model.train()
        optimizer.zero_grad()
        
        outputs = model(data.x, data.edge_index)
        train_loss = loss_func(outputs[data.train_mask], data.y[data.train_mask])
        train_loss.backward()
        optimizer.step()
        
        model.eval()
        with torch.no_grad():
            val_outputs = model(data.x, data.edge_index)[data.val_mask]
            val_acc = accuracy_score(val_outputs.argmax(dim=1), data.y[data.val_mask])
            
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                no_improve_epochs = 0
                best_model_state = model.state_dict()
            else:
                no_improve_epochs += 1
                
            if no_improve_epochs >= patience:
                stop_training = True
        
        if (epoch % 5 == 0) or (epoch == max_epochs-1) or stop_training:
            with torch.no_grad():
                train_outputs = outputs[data.train_mask]
                train_acc = accuracy_score(train_outputs.argmax(dim=1), data.y[data.train_mask])
                
                test_outputs = model(data.x, data.edge_index)[data.test_mask]
                test_acc = accuracy_score(test_outputs.argmax(dim=1), data.y[data.test_mask])
                
                print(f"Epoch {epoch + 1}")
                print(f"Train Loss: {train_loss.item():.4f} | Train Acc: {train_acc:.4f}")
                print(f"Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}")
                print(f"Best Val Accuracy: {best_val_acc:.4f}")
                print(f"No improve {no_improve_epochs} epochs", end="\n\n")
    
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
    
    model.eval()
    with torch.no_grad():
        train_outputs = model(data.x, data.edge_index)[data.train_mask]
        final_train_loss = loss_func(train_outputs, data.y[data.train_mask]).item()
        final_train_acc = accuracy_score(train_outputs.argmax(dim=1), data.y[data.train_mask])
        
        test_outputs = model(data.x, data.edge_index)[data.test_mask]
        final_test_acc = accuracy_score(test_outputs.argmax(dim=1), data.y[data.test_mask])
    
    return {
        'model_name': model_name,
        'train_loss': final_train_loss,
        'train_acc': final_train_acc,
        'test_acc': final_test_acc,
        'epochs': epoch + 1
    }
model_gcn = GCNConvModel(1433, 7)
model_sage = SAGEConvModel(1433, 7)
model_gat = GATConvModel(1433, 7)

gcn = train_with_early_stopping(model=model_gcn, data=data)
sage = train_with_early_stopping(model=model_sage, data=data)
gat = train_with_early_stopping(model=model_gat, data=data)


all_results = [gcn, sage, gat]

results_df = pd.DataFrame({
    'Модель': [res['model_name'] for res in all_results],
    'Loss на обучении': [f"{res['train_loss']:.4f}" for res in all_results],
    'Acc на обучении': [f"{res['train_acc']:.4f}" for res in all_results],
    'Acc на тесте': [f"{res['test_acc']:.4f}" for res in all_results],
    'Кол-во эпох до ранней остановки': [res['epochs'] for res in all_results],
})
results_df
    """
    pc.copy(text)
    
    
def thirteen_gnn_graphs_1():
    """
1. Реализуйте все описанные методы класса GraphsDataset. Используя данный класс, создайте датасет на основе файлов архива graphs.zip (можно разархивировать вручную или программно). Выведите на экран количество объектов в датасете. Выведите на экран значения признаков узлов для графа с индексом 0.

path_to_zip_file = "data/graphs.zip"
directory_to_extract_to = "data"

with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:
    zip_ref.extractall(directory_to_extract_to)
    
class GraphsDataset(Dataset):
    def __init__(self, path):
        super().__init__()
        self.graphs = []
        self.ys = []
        
        for idx, raw_path in enumerate(os.listdir(path)):
            for p in Path(path, raw_path).glob("*.gml"):
                graph = nx.read_gml(p)
                
                for node in graph.nodes:
                    graph.nodes[node].pop("block", None)
                
                self.graphs.append(
                    from_networkx(graph, group_node_attrs=("degree_centrality", "clustering_coefficient"))
                )
                self.ys.append(idx)
        
        self.ys = torch.tensor(self.ys)

    def __getitem__(self, idx):
        return self.graphs[idx], self.ys[idx]

    def __len__(self):
        return len(self.graphs)
        
dataset = GraphsDataset("data/graphs")
dataset[0]

print("количество объектов в датасете:", len(dataset))
display(dataset[0])
display(dataset[0][0].x)
    """
    text = """
path_to_zip_file = "data/graphs.zip"
directory_to_extract_to = "data"

with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:
    zip_ref.extractall(directory_to_extract_to)
    
class GraphsDataset(Dataset):
    def __init__(self, path):
        super().__init__()
        self.graphs = []
        self.ys = []
        
        for idx, raw_path in enumerate(os.listdir(path)):
            for p in Path(path, raw_path).glob("*.gml"):
                graph = nx.read_gml(p)
                
                for node in graph.nodes:
                    graph.nodes[node].pop("block", None)
                
                self.graphs.append(
                    from_networkx(graph, group_node_attrs=("degree_centrality", "clustering_coefficient"))
                )
                self.ys.append(idx)
        
        self.ys = torch.tensor(self.ys)

    def __getitem__(self, idx):
        return self.graphs[idx], self.ys[idx]

    def __len__(self):
        return len(self.graphs)
        
dataset = GraphsDataset("data/graphs")
dataset[0]

print("количество объектов в датасете:", len(dataset))
display(dataset[0])
display(dataset[0][0].x)
    """
    pc.copy(text)
    
    
def thirteen_gnn_graphs_2():
    """
2. Используя датасет из предыдущего задания, создайте объект torch_geometric.loader.DataLodaer. Получите один батч размера 128 при помощи этого объекта и выведите на экран (используйте соответствующие атрибуты и методы):

количество узлов в графе-батче;
количество связей в графе-батче;
количество графов в батче;
количество узлов в каждом графе батча;
Выполните readout для графа на основе атрибута x. Выведите размерность полученного тензора на экран.

loader = DataLoader(dataset, batch_size=128, shuffle=True)
batch, labels = next(iter(loader))
batch, labels

print("количество узлов в графе-батче:", batch.num_nodes)
print("количество связей в графе-батче:", batch.num_edges)
print("количество графов в батче:", len(batch))
print("количество узлов в графе-батче:", [batch[i].num_nodes for i in range(len(batch))])

sc = scatter(batch.x, index=batch.batch, dim=0)
display(sc)

display(sc.shape)
    """
    text = """
loader = DataLoader(dataset, batch_size=128, shuffle=True)
batch, labels = next(iter(loader))
batch, labels

print("количество узлов в графе-батче:", batch.num_nodes)
print("количество связей в графе-батче:", batch.num_edges)
print("количество графов в батче:", len(batch))
print("количество узлов в графе-батче:", [batch[i].num_nodes for i in range(len(batch))])

sc = scatter(batch.x, index=batch.batch, dim=0)
display(sc)

display(sc.shape)
    """
    pc.copy(text)
    
    
def thirteen_gnn_graphs_3():
    """
3. Решите задачу классификации графа, используя слои SAGEConv и операцию усреднения для процедуры readout. Для обучения используйте стохастический градиентный спуск с размером батча 128. Во время обучения выводите значение функции потерь по эпохам (используйте torchmetrics). Вычислите матрицу несоответствий прогнозов и точность обученной модели (используйте torchmetrics).

class SAGEConvModel(nn.Module):
    def __init__(self, n_inputs, n_outputs, readout):
        super().__init__()
        self.readout = readout
        self.layer_1 = tgnn.SAGEConv(n_inputs, 128, aggr="mean")
        self.layer_2 = tgnn.SAGEConv(128, n_outputs, aggr="mean")
        self.relu = nn.ReLU()
        
    def forward(self, batch, reduce="mean"):
        out = self.layer_1(batch.x, batch.edge_index)
        out = self.relu(out)
        out = self.layer_2(out, batch.edge_index)
        out = scatter(out, batch.batch, dim=0, reduce=self.readout)
        return out
        
def train(data_loader, lr=0.5, max_epochs=100, reduce="mean"):
    device = "cuda"
    model = SAGEConvModel(2, 3, reduce).to(device)
    optimizer = torch.optim.SGD(model.parameters(), lr=lr)
    loss_func = nn.CrossEntropyLoss().to(device)

    train_loss = MeanMetric().to(device)
    test_acc = Accuracy(task="multiclass", num_classes=3).to(device)
    confusion_matrix = ConfusionMatrix(task="multiclass", num_classes=3).to(device)

    train_dataset, test_dataset = random_split(data_loader.dataset, lengths=[0.8, 0.2])
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=128)

    for epoch in tqdm(range(max_epochs), desc=f"Train Model, readout reduce {reduce}"):
        model.train()
        train_loss.reset()

        for batch, labels in train_loader:
            batch = batch.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            outputs = model(batch)
            loss = loss_func(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss.update(loss)

        model.eval()
        test_acc.reset()
        with torch.no_grad():
            for batch, labels in test_loader:
                batch = batch.to(device)
                labels = labels.to(device)
                outputs = model(batch)
                preds = outputs.argmax(dim=1)
                test_acc.update(preds, labels)

        print({
            "Epoch": f"{epoch + 1}/{max_epochs}",
            "Train Loss": f"{train_loss.compute():.4f}",
            "Test Acc": f"{test_acc.compute():.4f}"
        })

    model.eval()
    with torch.no_grad():
        for batch, labels in test_loader:
            batch = batch.to(device)
            labels = labels.to(device)
            outputs = model(batch)
            preds = outputs.argmax(dim=1)
            confusion_matrix.update(preds, labels)

    return {
        'Loss': train_loss.compute().item(),
        'Acc': test_acc.compute().item(),
        'Confusion Matrix': confusion_matrix.compute()
    }
    
results = train(loader)
results
    """
    text = """
class SAGEConvModel(nn.Module):
    def __init__(self, n_inputs, n_outputs, readout):
        super().__init__()
        self.readout = readout
        self.layer_1 = tgnn.SAGEConv(n_inputs, 128, aggr="mean")
        self.layer_2 = tgnn.SAGEConv(128, n_outputs, aggr="mean")
        self.relu = nn.ReLU()
        
    def forward(self, batch, reduce="mean"):
        out = self.layer_1(batch.x, batch.edge_index)
        out = self.relu(out)
        out = self.layer_2(out, batch.edge_index)
        out = scatter(out, batch.batch, dim=0, reduce=self.readout)
        return out
        
def train(data_loader, lr=0.5, max_epochs=100, reduce="mean"):
    device = "cuda"
    model = SAGEConvModel(2, 3, reduce).to(device)
    optimizer = torch.optim.SGD(model.parameters(), lr=lr)
    loss_func = nn.CrossEntropyLoss().to(device)

    train_loss = MeanMetric().to(device)
    test_acc = Accuracy(task="multiclass", num_classes=3).to(device)
    confusion_matrix = ConfusionMatrix(task="multiclass", num_classes=3).to(device)

    train_dataset, test_dataset = random_split(data_loader.dataset, lengths=[0.8, 0.2])
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=128)

    for epoch in tqdm(range(max_epochs), desc=f"Train Model, readout reduce {reduce}"):
        model.train()
        train_loss.reset()

        for batch, labels in train_loader:
            batch = batch.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            outputs = model(batch)
            loss = loss_func(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss.update(loss)

        model.eval()
        test_acc.reset()
        with torch.no_grad():
            for batch, labels in test_loader:
                batch = batch.to(device)
                labels = labels.to(device)
                outputs = model(batch)
                preds = outputs.argmax(dim=1)
                test_acc.update(preds, labels)

        print({
            "Epoch": f"{epoch + 1}/{max_epochs}",
            "Train Loss": f"{train_loss.compute():.4f}",
            "Test Acc": f"{test_acc.compute():.4f}"
        })

    model.eval()
    with torch.no_grad():
        for batch, labels in test_loader:
            batch = batch.to(device)
            labels = labels.to(device)
            outputs = model(batch)
            preds = outputs.argmax(dim=1)
            confusion_matrix.update(preds, labels)

    return {
        'Loss': train_loss.compute().item(),
        'Acc': test_acc.compute().item(),
        'Confusion Matrix': confusion_matrix.compute()
    }
    
results = train(loader)
results
    """
    pc.copy(text)
    
    
def thirteen_gnn_graphs_4():
    """
4. Повторите решение задачи 3, сравнив разные функции агрегации для проведения операции readout. Выведите результаты в виде таблицы:

Выведите результат в виде таблицы:

Readout op	Loss	Acc
sum		
mean		
max		
min		

readout_sum = train(loader, lr=0.05, max_epochs=500, reduce="sum")
readout_mean = train(loader, lr=0.5, max_epochs=200, reduce="mean")
readout_max = train(loader, lr=0.05, max_epochs=500, reduce="max")
readout_min = train(loader,lr=0.05, max_epochs=500, reduce="min")

all_results = [readout_sum, readout_mean, readout_max, readout_min]

results_df = pd.DataFrame({
    'Readout op': [r_op for r_op in ["sum", "mean", "max", "min"]],
    'Loss': [f"{res['Loss']:.4f}" for res in all_results],
    'Acc': [f"{res['Acc']:.4f}" for res in all_results],
})
results_df
    """
    text = """
readout_sum = train(loader, lr=0.05, max_epochs=500, reduce="sum")
readout_mean = train(loader, lr=0.5, max_epochs=200, reduce="mean")
readout_max = train(loader, lr=0.05, max_epochs=500, reduce="max")
readout_min = train(loader,lr=0.05, max_epochs=500, reduce="min")

all_results = [readout_sum, readout_mean, readout_max, readout_min]

results_df = pd.DataFrame({
    'Readout op': [r_op for r_op in ["sum", "mean", "max", "min"]],
    'Loss': [f"{res['Loss']:.4f}" for res in all_results],
    'Acc': [f"{res['Acc']:.4f}" for res in all_results],
})
results_df
    """
    pc.copy(text)
    
    
def fourteen_kg_trans_1():
    """
1. Загрузите датасет CoDExSmall. Выведите на экран (обратитесь к соответстующим полям) количество сущностей в обучающем (<dataset>.training), валидационном (<dataset>.validation) и тестовом (<dataset>.testing) множестве.

Выведите на экран одну произвольную тройку  (ℎ,𝑟,𝑡)
  в оригинальном виде (<dataset>.triples) и эту же тройку, но с использованием числового представления сущностей/отношений (<dataset>.mapped_triples).
  
dataset = CoDExSmall()
print(dataset.training.num_triples)
print(dataset.validation.num_triples)
print(dataset.testing.num_triples)

dataset.training.triples[0]

dataset.training.mapped_triples[0]
    """
    text = """
  
dataset = CoDExSmall()
print(dataset.training.num_triples)
print(dataset.validation.num_triples)
print(dataset.testing.num_triples)

dataset.training.triples[0]

dataset.training.mapped_triples[0]
    """
    pc.copy(text)
    
    
def fourteen_kg_trans_2():
    """
2. Обучите модель TransE на датасете CoDExSmall при помощи pipeline. Задайте количество эпох обучения (num_epochs в training_kwargs), равное 100, и размерность эмбеддингов (embedding_dim в  model_kwargs), равную 64.

Изобразите график значений функции потерь в зависимости от номера эпохи (<pipeline_result>.plot_losses).

Выведите на экран значение метрик в виде pd.DataFrame (<pipeline_result>.metric_results.to_df())

pipe_transe = pipeline(
    dataset="CoDExSmall",
    model="TransE",
    training_kwargs={
        "num_epochs": 100,
    },
    model_kwargs={
        "embedding_dim": 64,
    }
)
pipe_transe.plot_losses()
metrics_df = pipe_transe.metric_results.to_df()
metrics_df
    """
    text = """
pipe_transe = pipeline(
    dataset="CoDExSmall",
    model="TransE",
    training_kwargs={
        "num_epochs": 100,
    },
    model_kwargs={
        "embedding_dim": 64,
    }
)
pipe_transe.plot_losses()
metrics_df = pipe_transe.metric_results.to_df()
metrics_df
    """
    pc.copy(text)
    
    
def fourteen_kg_trans_3():
    """
3. Выберите случайным образом одну тройку  (ℎ,𝑟,𝑡∗)
  из тестового множества. Используя обученную модель, рассчитайте оценки для всех троек  (ℎ,𝑟,𝑡),𝑡∈𝐾
 .

Найдите позицию (ранг) истинного значения объекта тройки  𝑡∗
  в списке прогнозов, отсортированном в порядке убывания величины score.

random_idx = random.randint(0, dataset.testing.num_triples - 1)

true_triple = dataset.testing.triples[random_idx]
true_map = dataset.testing.mapped_triples[random_idx]
h, r, t = true_triple.tolist()
h_id, r_id, t_id = true_map.tolist()
print(h, r, t)
print(h_id, r_id, t_id)

prediction_df = predict_target(
    model=pipe_transe.model,
    head=h,
    relation=r,
    tail=None,
    triples_factory=dataset.testing
).df
prediction_df
sorted_df = prediction_df.sort_values("score", ascending=False).reset_index(drop=True)
sorted_df

true_rank = (sorted_df[sorted_df['tail_id'] == t_id].index).to_numpy()[0] + 1
print(f'Ранг истинного объекта t*: {true_rank}')
    """
    text = """
random_idx = random.randint(0, dataset.testing.num_triples - 1)

true_triple = dataset.testing.triples[random_idx]
true_map = dataset.testing.mapped_triples[random_idx]
h, r, t = true_triple.tolist()
h_id, r_id, t_id = true_map.tolist()
print(h, r, t)
print(h_id, r_id, t_id)

prediction_df = predict_target(
    model=pipe_transe.model,
    head=h,
    relation=r,
    tail=None,
    triples_factory=dataset.testing
).df
prediction_df
sorted_df = prediction_df.sort_values("score", ascending=False).reset_index(drop=True)
sorted_df

true_rank = (sorted_df[sorted_df['tail_id'] == t_id].index).to_numpy()[0] + 1
print(f'Ранг истинного объекта t*: {true_rank}')
    """
    pc.copy(text)
    
    
def fourteen_kg_trans_4():
    """
4. Обучите модели TransE, TransH и TransD на датасете CoDExSmall.

Сравните качество полученных моделей по следующим метрикам (side=both и type=realistic):

Adjusted Arithmetic Mean Rank (AAMR): adjusted_arithmetic_mean_rank
Mean Reciprocal Rank (MRR): inverse_harmonic_mean_rank
Hits @ 10: hits_at_10
Визуализируйте результаты в виде столбчатой диаграммы

models = ["TransE", "TransH", "TransD"]
needed_metrics = [
    "adjusted_arithmetic_mean_rank",
    "inverse_harmonic_mean_rank",
    "hits_at_10",
    ]
    
metrics = defaultdict(list)

for model in models:
  pipe_transe = pipeline(
      dataset="CoDExSmall",
      model=model,
      training_kwargs={
          "num_epochs": 100,
      },
      model_kwargs={
          "embedding_dim": 64,
      }
  )
  metrics_df = pipe_transe.metric_results.to_df()
  metrics_df = metrics_df[
      (metrics_df["Side"]=="both") &
       (metrics_df["Rank_type"]=="realistic")
       ]

  metrics["models"].append(model)
  for need_metric in needed_metrics:
    metrics[need_metric].append(
        metrics_df[metrics_df["Metric"]==need_metric]["Value"].values[0]
        )
        
results_df = pd.DataFrame(metrics)

results_df.set_index("models")[needed_metrics].plot(
    kind="bar",
    figsize=(10, 6),
    title="Визуализация результатов"
)
plt.ylabel("Значение метрики")
plt.xlabel("Модель")
plt.xticks(rotation=0)
plt.grid(True, linestyle='--', alpha=0.5)
plt.legend(title="Метрика")
plt.tight_layout()
plt.show()
    """
    text = """
models = ["TransE", "TransH", "TransD"]
needed_metrics = [
    "adjusted_arithmetic_mean_rank",
    "inverse_harmonic_mean_rank",
    "hits_at_10",
    ]
    
metrics = defaultdict(list)

for model in models:
  pipe_transe = pipeline(
      dataset="CoDExSmall",
      model=model,
      training_kwargs={
          "num_epochs": 100,
      },
      model_kwargs={
          "embedding_dim": 64,
      }
  )
  metrics_df = pipe_transe.metric_results.to_df()
  metrics_df = metrics_df[
      (metrics_df["Side"]=="both") &
       (metrics_df["Rank_type"]=="realistic")
       ]

  metrics["models"].append(model)
  for need_metric in needed_metrics:
    metrics[need_metric].append(
        metrics_df[metrics_df["Metric"]==need_metric]["Value"].values[0]
        )
        
results_df = pd.DataFrame(metrics)

results_df.set_index("models")[needed_metrics].plot(
    kind="bar",
    figsize=(10, 6),
    title="Визуализация результатов"
)
plt.ylabel("Значение метрики")
plt.xlabel("Модель")
plt.xticks(rotation=0)
plt.grid(True, linestyle='--', alpha=0.5)
plt.legend(title="Метрика")
plt.tight_layout()
plt.show()
    """
    pc.copy(text)
Metadata-Version: 2.3
Name: jieba_pyfast
Version: 3.13.3
Summary: Tokenize Chinese characters
License: MIT
Author: snapADDY GmbH
Author-email: info@snapaddy.com
Requires-Python: >=3.11,<4.0
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Description-Content-Type: text/markdown

# jieba_pyfast

A Chinese text segmentation module, from `jieba_fast`, with wheels for python3.9 & python3.10

## Installation

You can install the latest stable version via:

`$ pip install jieba_pyfast`

## Main Functions

For details, see https://github.com/fxsjy/jieba

## Usage

```python
>>> import jieba_pyfast as jieba
>>> jieba.lcut('下雨天留客天留我不留')
['下雨天', '留客', '天留', '我', '不留']
```


Metadata-Version: 2.4
Name: pyhub-llm
Version: 0.7.0
Summary: Standalone LLM library with support for multiple providers
Project-URL: Homepage, https://github.com/pyhub-kr/pyhub-llm
Project-URL: Documentation, https://github.com/pyhub-kr/pyhub-llm#readme
Project-URL: Repository, https://github.com/pyhub-kr/pyhub-llm
Project-URL: Issues, https://github.com/pyhub-kr/pyhub-llm/issues
Author-email: PyHub Team <me@pyhub.kr>
License: MIT
License-File: LICENSE
Keywords: agent,ai,anthropic,google,llm,mcp,ollama,openai,react
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.10
Requires-Dist: aiofiles>=23.0.0
Requires-Dist: httpx>=0.24.0
Requires-Dist: jinja2>=3.1.0
Requires-Dist: pillow>=10.0.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: rich>=13.0.0
Requires-Dist: toml>=0.10.0
Requires-Dist: typer>=0.9.0
Provides-Extra: all
Requires-Dist: anthropic>=0.52.0; extra == 'all'
Requires-Dist: fastmcp; extra == 'all'
Requires-Dist: google-genai>=1.19.0; extra == 'all'
Requires-Dist: mcp; extra == 'all'
Requires-Dist: ollama>=0.5.0; extra == 'all'
Requires-Dist: openai>=1.84.0; extra == 'all'
Requires-Dist: pymupdf; extra == 'all'
Requires-Dist: pymupdf>=1.23.0; extra == 'all'
Requires-Dist: pyyaml>=6.0.0; extra == 'all'
Requires-Dist: uvicorn; extra == 'all'
Provides-Extra: anthropic
Requires-Dist: anthropic>=0.52.0; extra == 'anthropic'
Provides-Extra: build
Requires-Dist: build; extra == 'build'
Requires-Dist: setuptools; extra == 'build'
Requires-Dist: twine; extra == 'build'
Requires-Dist: wheel; extra == 'build'
Provides-Extra: dev
Requires-Dist: black>=23.0.0; extra == 'dev'
Requires-Dist: mypy>=1.0.0; extra == 'dev'
Requires-Dist: pytest-asyncio>=0.21.0; extra == 'dev'
Requires-Dist: pytest-cov>=4.0.0; extra == 'dev'
Requires-Dist: pytest>=7.0.0; extra == 'dev'
Requires-Dist: ruff>=0.1.0; extra == 'dev'
Provides-Extra: docs
Requires-Dist: mkdocs; extra == 'docs'
Requires-Dist: mkdocs-glightbox; extra == 'docs'
Requires-Dist: mkdocs-material; extra == 'docs'
Requires-Dist: pymdown-extensions; extra == 'docs'
Provides-Extra: google
Requires-Dist: google-genai>=1.19.0; extra == 'google'
Provides-Extra: mcp
Requires-Dist: fastmcp; extra == 'mcp'
Requires-Dist: mcp; extra == 'mcp'
Requires-Dist: pyyaml>=6.0.0; extra == 'mcp'
Requires-Dist: uvicorn; extra == 'mcp'
Provides-Extra: ollama
Requires-Dist: ollama>=0.5.0; extra == 'ollama'
Requires-Dist: pymupdf; extra == 'ollama'
Provides-Extra: openai
Requires-Dist: openai>=1.84.0; extra == 'openai'
Provides-Extra: pdf
Requires-Dist: pymupdf>=1.23.0; extra == 'pdf'
Provides-Extra: upstage
Requires-Dist: openai>=1.84.0; extra == 'upstage'
Description-Content-Type: text/markdown

# pyhub-llm

Îã§ÏñëÌïú LLM Ï†úÍ≥µÏóÖÏ≤¥Î•º ÏúÑÌïú ÌÜµÌï© Python ÎùºÏù¥Î∏åÎü¨Î¶¨ÏûÖÎãàÎã§. OpenAI, Anthropic, Google, Ollama Îì±Ïùò APIÎ•º ÏùºÍ¥ÄÎêú Ïù∏ÌÑ∞ÌéòÏù¥Ïä§Î°ú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.

## üìö Î¨∏ÏÑú

### [‚ú® CHEATSHEET.md](./CHEATSHEET.md) - Î™®Îì† Í∏∞Îä•Í≥º ÏòàÏ†úÍ∞Ä Ìè¨Ìï®Îêú ÏôÑÏ†ÑÌïú Í∞ÄÏù¥Îìú

pyhub-llmÏùò Î™®Îì† Í∏∞Îä•ÏùÑ ÏûêÏÑ∏Ìûà ÏïåÏïÑÎ≥¥Î†§Î©¥ CHEATSHEET.mdÎ•º Ï∞∏Í≥†ÌïòÏÑ∏Ïöî:
- Îã§ÏñëÌïú LLM ÌîÑÎ°úÎ∞îÏù¥Îçî ÏÇ¨Ïö©Î≤ï
- Ïä§Ìä∏Î¶¨Î∞ç, Ï∫êÏã±, Ï≤¥Ïù¥Îãù Îì± Í≥†Í∏â Í∏∞Îä•
- Ïõπ ÌîÑÎ†àÏûÑÏõåÌÅ¨ ÌÜµÌï© (FastAPI, Django, Streamlit)
- MCP(Model Context Protocol) ÌÜµÌï©
- Ïã§Ïö©Ï†ÅÏù∏ ÏòàÏ†úÏôÄ Î≤†Ïä§Ìä∏ ÌîÑÎûôÌã∞Ïä§

## Ï£ºÏöî Í∏∞Îä•

- üîå **ÌÜµÌï© Ïù∏ÌÑ∞ÌéòÏù¥Ïä§**: Î™®Îì† LLM Ï†úÍ≥µÏóÖÏ≤¥Î•º ÎèôÏùºÌïú Î∞©ÏãùÏúºÎ°ú ÏÇ¨Ïö©
- üöÄ **Í∞ÑÌé∏Ìïú Ï†ÑÌôò**: ÏΩîÎìú Î≥ÄÍ≤Ω ÏóÜÏù¥ Î™®Îç∏ Ï†ÑÌôò Í∞ÄÎä•
- üíæ **Ï∫êÏã± ÏßÄÏõê**: ÏùëÎãµ Ï∫êÏã±ÏúºÎ°ú ÎπÑÏö© Ï†àÍ∞ê Î∞è ÏÑ±Îä• Ìñ•ÏÉÅ
- üîÑ **Ïä§Ìä∏Î¶¨Î∞ç ÏßÄÏõê**: Ïã§ÏãúÍ∞Ñ ÏùëÎãµ Ïä§Ìä∏Î¶¨Î∞ç
- üõ†Ô∏è **ÎèÑÍµ¨/Ìï®Ïàò Ìò∏Ï∂ú**: Function calling ÏßÄÏõê
- üì∑ **Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨**: Ïù¥ÎØ∏ÏßÄ ÏÑ§Î™Ö Î∞è Î∂ÑÏÑù Í∏∞Îä•
- ‚ö° **ÎπÑÎèôÍ∏∞ ÏßÄÏõê**: ÎèôÍ∏∞/ÎπÑÎèôÍ∏∞ Î™®Îëê ÏßÄÏõê
- üîó **Ï≤¥Ïù¥Îãù**: Ïó¨Îü¨ LLMÏùÑ Ïó∞Í≤∞ÌïòÏó¨ Î≥µÏû°Ìïú ÏõåÌÅ¨ÌîåÎ°úÏö∞ Íµ¨ÏÑ±

## ÏÑ§Ïπò

### Ï†ÑÏ≤¥ ÏÑ§Ïπò

```bash
pip install 'pyhub-llm[all]'
```

### ÌäπÏ†ï Ï†úÍ≥µÏóÖÏ≤¥Îßå ÏÑ§Ïπò

```bash
# OpenAIÎßå
pip install "pyhub-llm[openai]"

# AnthropicÎßå
pip install "pyhub-llm[anthropic]"

# GoogleÎßå (google-genai ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÇ¨Ïö©)
pip install "pyhub-llm[google]"

# OllamaÎßå
pip install "pyhub-llm[ollama]"

# Î™®Îì† Ï†úÍ≥µÏóÖÏ≤¥
pip install "pyhub-llm[all]"
```

### Í∞úÎ∞ú ÌôòÍ≤Ω ÏÑ§Ïπò

```bash
# Ï†ÄÏû•ÏÜå ÌÅ¥Î°†
git clone https://github.com/pyhub-kr/pyhub-llm.git
cd pyhub-llm

# Í∞úÎ∞ú ÌôòÍ≤Ω ÏÑ§Ïπò
pip install -e ".[dev,all]"
# ÌòπÏùÄ make install
```

## Îπ†Î•∏ ÏãúÏûë

### ÌôòÍ≤ΩÎ≥ÄÏàò ÏÑ§Ï†ï

Í∞Å ÌîÑÎ°úÎ∞îÏù¥ÎçîÎ•º ÏÇ¨Ïö©ÌïòÎ†§Î©¥ Ìï¥Îãπ API ÌÇ§Î•º ÌôòÍ≤ΩÎ≥ÄÏàòÎ°ú ÏÑ§Ï†ïÌï¥Ïïº Ìï©ÎãàÎã§:

#### Linux/macOS (Bash)
```bash
export OPENAI_API_KEY="your-openai-api-key"
export ANTHROPIC_API_KEY="your-anthropic-api-key"
export GOOGLE_API_KEY="your-google-api-key"
export UPSTAGE_API_KEY="your-upstage-api-key"
```

#### Windows (PowerShell)
```powershell
$env:OPENAI_API_KEY="your-openai-api-key"
$env:ANTHROPIC_API_KEY="your-anthropic-api-key"
$env:GOOGLE_API_KEY="your-google-api-key"
$env:UPSTAGE_API_KEY="your-upstage-api-key"
```

> **Ï∞∏Í≥†**: 
> + API ÌÇ§Îäî Í∞Å ÌîÑÎ°úÎ∞îÏù¥ÎçîÏùò ÏõπÏÇ¨Ïù¥Ìä∏ÏóêÏÑú Î∞úÍ∏âÎ∞õÏùÑ Ïàò ÏûàÏäµÎãàÎã§ (API ÌÇ§ ÏÑ§Ï†ï ÏÑπÏÖò Ï∞∏Ï°∞)
> + OllamaÎäî Î°úÏª¨ÏóêÏÑú Ïã§ÌñâÎêòÎØÄÎ°ú API ÌÇ§Í∞Ä ÌïÑÏöî ÏóÜÏäµÎãàÎã§
> + OllamaÎäî ÎîîÌè¥Ìä∏Î°ú `http://localhost:11434` Ï£ºÏÜåÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§. `UPSTAGE_BASE_URL` ÌôòÍ≤ΩÎ≥ÄÏàòÎÇò `OllamaLLM(base_url="...")` Ïù∏ÏûêÎ•º ÌÜµÌï¥ Î≥ÄÍ≤ΩÌïòÏã§ Ïàò ÏûàÏäµÎãàÎã§.

### Î™®Îç∏Î≥Ñ ÏßÅÏ†ë ÏÇ¨Ïö©

Í∞Å ÌîÑÎ°úÎ∞îÏù¥ÎçîÎ•º ÏÇ¨Ïö©ÌïòÎ†§Î©¥ Ìï¥Îãπ ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º Î®ºÏ†Ä ÏÑ§ÏπòÌï¥Ïïº Ìï©ÎãàÎã§:

```bash
# OpenAI ÏÇ¨Ïö©Ïãú
pip install "pyhub-llm[openai]"

# Anthropic ÏÇ¨Ïö©Ïãú
pip install "pyhub-llm[anthropic]"

# Google ÏÇ¨Ïö©Ïãú
pip install "pyhub-llm[google]"

# Ollama ÏÇ¨Ïö©Ïãú (Î°úÏª¨ Ïã§Ìñâ)
pip install "pyhub-llm[ollama]"
```

```python
from pyhub.llm import OpenAILLM, AnthropicLLM, GoogleLLM, OllamaLLM

# OpenAI (OPENAI_API_KEY ÌôòÍ≤ΩÎ≥ÄÏàò ÌïÑÏöî)
openai_llm = OpenAILLM(model="gpt-4o-mini")
reply = openai_llm.ask("ÏïàÎÖïÌïòÏÑ∏Ïöî!")

# API ÌÇ§ ÏßÅÏ†ë Ï†ÑÎã¨
openai_llm = OpenAILLM(model="gpt-4o-mini", api_key="your-api-key")

# Anthropic (ANTHROPIC_API_KEY ÌôòÍ≤ΩÎ≥ÄÏàò ÌïÑÏöî)
claude_llm = AnthropicLLM(model="claude-3-5-haiku-latest")
reply = claude_llm.ask("ÏïàÎÖïÌïòÏÑ∏Ïöî!")

# Google (GOOGLE_API_KEY ÌôòÍ≤ΩÎ≥ÄÏàò ÌïÑÏöî)
gemini_llm = GoogleLLM(model="gemini-1.5-flash")
reply = gemini_llm.ask("ÏïàÎÖïÌïòÏÑ∏Ïöî!")

# Ollama (Î°úÏª¨ Ïã§Ìñâ, API ÌÇ§ Î∂àÌïÑÏöî, Í∏∞Î≥∏ URL: http://localhost:11434)
ollama_llm = OllamaLLM(model="mistral")
reply = ollama_llm.ask("ÏïàÎÖïÌïòÏÑ∏Ïöî!")
```

### Í∏∞Î≥∏ ÏÇ¨Ïö©Î≤ï

```python
from pyhub.llm import LLM

# LLM Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±
llm = LLM.create("gpt-4o-mini")

# ÏßàÎ¨∏ÌïòÍ∏∞
reply = llm.ask("PythonÏùò Ïû•Ï†êÏùÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?")
print(reply.text)
```

> Îçî ÎßéÏùÄ ÏòàÏãúÏôÄ Í≥†Í∏â ÏÇ¨Ïö©Î≤ïÏùÄ [CHEATSHEET.md](./CHEATSHEET.md#Í∏∞Î≥∏-ÏÇ¨Ïö©Î≤ï)Î•º Ï∞∏Í≥†ÌïòÏÑ∏Ïöî.

## Ollama Î°úÏª¨ Î™®Îç∏ ÏÇ¨Ïö©

OllamaÎäî Î°úÏª¨ÏóêÏÑú LLMÏùÑ Ïã§ÌñâÌï† Ïàò ÏûàÎäî Ïò§ÌîàÏÜåÏä§ ÎèÑÍµ¨ÏûÖÎãàÎã§. API ÌÇ§Í∞Ä ÌïÑÏöî ÏóÜÍ≥†, Îç∞Ïù¥ÌÑ∞Í∞Ä Ïô∏Î∂ÄÎ°ú Ï†ÑÏÜ°ÎêòÏßÄ ÏïäÏïÑ Í∞úÏù∏Ï†ïÎ≥¥ Î≥¥Ìò∏Ïóê Ïú†Î¶¨Ìï©ÎãàÎã§.

### Ollama ÏÑ§Ïπò

#### macOS
```bash
# Homebrew ÏÇ¨Ïö©
brew install ollama

# ÎòêÎäî Í≥µÏãù ÏÑ§Ïπò ÌîÑÎ°úÍ∑∏Îû® Îã§Ïö¥Î°úÎìú
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Linux
```bash
# ÏÑ§Ïπò Ïä§ÌÅ¨Î¶ΩÌä∏ Ïã§Ìñâ
curl -fsSL https://ollama.ai/install.sh | sh

# ÎòêÎäî Docker ÏÇ¨Ïö©
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

#### Windows
```bash
# PowerShellÏóêÏÑú Ïã§Ìñâ
iex (irm https://ollama.ai/install.ps1)

# ÎòêÎäî Í≥µÏãù ÏõπÏÇ¨Ïù¥Ìä∏ÏóêÏÑú ÏÑ§Ïπò ÌîÑÎ°úÍ∑∏Îû® Îã§Ïö¥Î°úÎìú
# https://ollama.ai/download/windows
```

### Î™®Îç∏ Îã§Ïö¥Î°úÎìú Î∞è Ïã§Ìñâ

```bash
# Ollama ÏÑúÎπÑÏä§ ÏãúÏûë (ÌïÑÏöîÌïú Í≤ΩÏö∞)
ollama serve

# Mistral Î™®Îç∏ Îã§Ïö¥Î°úÎìú
ollama pull mistral

# Îã§Î•∏ Ïù∏Í∏∞ Î™®Îç∏Îì§
ollama pull llama3.3
ollama pull gemma2
ollama pull qwen2

# Î™®Îç∏ Î™©Î°ù ÌôïÏù∏
ollama list

# Î™®Îç∏ ÏßÅÏ†ë Ïã§Ìñâ (ÌÖåÏä§Ìä∏Ïö©)
ollama run mistral
```

### pyhub-llmÏóêÏÑú Ollama ÏÇ¨Ïö©

```python
from pyhub.llm import OllamaLLM

# Í∏∞Î≥∏ ÏÇ¨Ïö©Î≤ï
llm = OllamaLLM(model="mistral")
reply = llm.ask("PythonÏúºÎ°ú Ïõπ Ïä§ÌÅ¨ÎûòÌïëÌïòÎäî Î∞©Î≤ïÏùÑ ÏïåÎ†§Ï£ºÏÑ∏Ïöî")
print(reply.text)

# Ïä§Ìä∏Î¶¨Î∞çÏúºÎ°ú Ïã§ÏãúÍ∞Ñ ÏùëÎãµ Î∞õÍ∏∞
for chunk in llm.ask("Í∏¥ Ïù¥ÏïºÍ∏∞Î•º Îì§Î†§Ï£ºÏÑ∏Ïöî", stream=True):
    print(chunk.text, end="", flush=True)

# Ïù¥ÎØ∏ÏßÄÏôÄ Ìï®Íªò ÏßàÎ¨∏ÌïòÍ∏∞
reply = llm.ask(
    "Ïù¥ Ïù¥ÎØ∏ÏßÄÏóê Î¨¥ÏóáÏù¥ Î≥¥Ïù¥ÎÇòÏöî?",
    files=["image.jpg"]
)

# PDF ÌååÏùº Ï≤òÎ¶¨ (ÏûêÎèôÏúºÎ°ú Ïù¥ÎØ∏ÏßÄÎ°ú Î≥ÄÌôòÎê®)
reply = llm.ask(
    "Ïù¥ PDF Î¨∏ÏÑúÎ•º ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî",
    files=["document.pdf"]  # ÏûêÎèôÏúºÎ°ú Í≥†ÌíàÏßà Ïù¥ÎØ∏ÏßÄÎ°ú Î≥ÄÌôò
)

# ÎπÑÎèôÍ∏∞ ÏÇ¨Ïö©
async def async_example():
    reply = await llm.ask_async("ÎπÑÎèôÍ∏∞Î°ú ÏßàÎ¨∏Ìï©ÎãàÎã§")
    return reply.text

# Ïª§Ïä§ÌÖÄ ÏÑ§Ï†ï
llm = OllamaLLM(
    model="mistral",
    temperature=0.7,
    max_tokens=2000,
    base_url="http://localhost:11434"  # Ïª§Ïä§ÌÖÄ Ollama ÏÑúÎ≤Ñ
)
```

### Ollama Ïû•Ï†ê

- **üîí Í∞úÏù∏Ï†ïÎ≥¥ Î≥¥Ìò∏**: Î™®Îì† Îç∞Ïù¥ÌÑ∞Í∞Ä Î°úÏª¨ÏóêÏÑú Ï≤òÎ¶¨
- **üí∞ ÎπÑÏö© Ï†àÍ∞ê**: API Ìò∏Ï∂ú ÎπÑÏö© ÏóÜÏùå
- **‚ö° Îπ†Î•∏ ÏùëÎãµ**: ÎÑ§Ìä∏ÏõåÌÅ¨ ÏßÄÏó∞ ÏóÜÏùå  
- **üåê Ïò§ÌîÑÎùºÏù∏ ÏÇ¨Ïö©**: Ïù∏ÌÑ∞ÎÑ∑ Ïó∞Í≤∞ Î∂àÌïÑÏöî
- **üéõÔ∏è ÏôÑÏ†ÑÌïú Ï†úÏñ¥**: Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞ ÏûêÏú† Ï°∞Ï†ï

### ÏßÄÏõê Î™®Îç∏

- **Llama Í≥ÑÏó¥**: llama3.3, llama3.1, llama3.2
- **Mistral**: mistral, mixtral
- **Gemma**: gemma2, gemma3  
- **Qwen**: qwen2, qwen2.5
- **Í∏∞ÌÉÄ**: phi3, codellama, vicuna Îì±

> **Ï∞∏Í≥†**: PDF ÌååÏùº Ï≤òÎ¶¨ Ïãú OllamaÎäî ÏûêÎèôÏúºÎ°ú Í≥†ÌíàÏßà Ïù¥ÎØ∏ÏßÄÎ°ú Î≥ÄÌôòÌïòÏó¨ Ï≤òÎ¶¨Ìï©ÎãàÎã§. ÌïúÍµ≠Ïñ¥ ÌÖçÏä§Ìä∏ Î≥¥Ï°¥ÏùÑ ÏúÑÌï¥ 600 DPIÎ°ú Î≥ÄÌôòÎê©ÎãàÎã§.

## Ï£ºÏöî Í∏∞Îä• ÏòàÏ†ú

### 1. Ïä§Ìä∏Î¶¨Î∞ç ÏùëÎãµ

```python
# Ïã§ÏãúÍ∞ÑÏúºÎ°ú ÏùëÎãµ Î∞õÍ∏∞
for chunk in llm.ask("Í∏¥ Ïù¥ÏïºÍ∏∞Î•º Îì§Î†§Ï£ºÏÑ∏Ïöî", stream=True):
    print(chunk.text, end="", flush=True)
```

### 2. ÌååÏùº Ï≤òÎ¶¨ (Ïù¥ÎØ∏ÏßÄ Î∞è PDF)

```python
# Ïù¥ÎØ∏ÏßÄ ÏÑ§Î™Ö
reply = llm.ask("Ïù¥ Ïù¥ÎØ∏ÏßÄÎ•º ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî", files=["photo.jpg"])

# PDF ÏöîÏïΩ
reply = llm.ask("Ïù¥ Î¨∏ÏÑúÎ•º ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî", files=["document.pdf"])
```

### 3. ÎèÑÍµ¨/Ìï®Ïàò Ìò∏Ï∂ú

```python
# Í∞ÑÎã®Ìïú Ìï®ÏàòÎ•º ÎèÑÍµ¨Î°ú ÏÇ¨Ïö©
def get_weather(city: str) -> str:
    """ÎèÑÏãúÏùò ÎÇ†Ïî® Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏ÏòµÎãàÎã§."""
    return f"{city}Ïùò ÎÇ†Ïî®Îäî ÎßëÏùåÏûÖÎãàÎã§."

reply = llm.ask("ÏÑúÏö∏ ÎÇ†Ïî® ÏïåÎ†§Ï§ò", tools=[get_weather])
```

> üìñ Îçî ÎßéÏùÄ Í≥†Í∏â Í∏∞Îä•Í≥º ÏÉÅÏÑ∏Ìïú ÏòàÏãúÎäî [CHEATSHEET.md](./CHEATSHEET.md)Î•º Ï∞∏Í≥†ÌïòÏÑ∏Ïöî:
> - ÎåÄÌôî ÌûàÏä§ÌÜ†Î¶¨ Í¥ÄÎ¶¨
> - Íµ¨Ï°∞ÌôîÎêú Ï∂úÎ†• (Pydantic)
> - LLM Ï≤¥Ïù¥Îãù
> - Ï∫êÏã± Ï†ÑÎûµ
> - MCP ÌÜµÌï©
> - Ïõπ ÌîÑÎ†àÏûÑÏõåÌÅ¨ ÌÜµÌï© (FastAPI, Django)
> - ÏóêÎü¨ Ï≤òÎ¶¨ Î∞è Ïû¨ÏãúÎèÑ

## API ÌÇ§ ÏÑ§Ï†ï

### ÌïÑÏöîÌïú API ÌÇ§

Í∞Å ÌîÑÎ°úÎ∞îÏù¥ÎçîÎ•º ÏÇ¨Ïö©ÌïòÎ†§Î©¥ Ìï¥Îãπ API ÌÇ§Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§:

- **OpenAI**: `OPENAI_API_KEY` - [API ÌÇ§ Î∞úÍ∏â](https://platform.openai.com/api-keys)
- **Anthropic**: `ANTHROPIC_API_KEY` - [API ÌÇ§ Î∞úÍ∏â](https://console.anthropic.com/settings/keys)
- **Google**: `GOOGLE_API_KEY` - [API ÌÇ§ Î∞úÍ∏â](https://makersuite.google.com/app/apikey)
- **Upstage**: `UPSTAGE_API_KEY` - [API ÌÇ§ Î∞úÍ∏â](https://console.upstage.ai/)

### ÏÑ§Ï†ï Î∞©Î≤ï

#### 1. ÌôòÍ≤Ω Î≥ÄÏàòÎ°ú ÏÑ§Ï†ï
```bash
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
export GOOGLE_API_KEY="your-google-key"
```

#### 2. ÏΩîÎìúÏóêÏÑú ÏßÅÏ†ë Ï†ÑÎã¨
```python
from pyhub.llm import OpenAILLM, AnthropicLLM, GoogleLLM

# API ÌÇ§Î•º ÏßÅÏ†ë Ï†ÑÎã¨
llm = OpenAILLM(api_key="your-api-key")
llm = AnthropicLLM(api_key="your-api-key")
llm = GoogleLLM(api_key="your-api-key")
```

## CLI ÏÇ¨Ïö©Î≤ï

```bash
# ÎåÄÌôîÌòï Ï±ÑÌåÖ
pyhub-llm chat --model gpt-4o-mini

# Îã®Ïùº ÏßàÎ¨∏
pyhub-llm ask "PythonÍ≥º GoÏùò Ï∞®Ïù¥Ï†êÏùÄ?"

# ÌååÏùºÍ≥º Ìï®Íªò ÏßàÎ¨∏
pyhub-llm ask "Ïù¥ ÏΩîÎìúÎ•º Î¶¨Î∑∞Ìï¥Ï£ºÏÑ∏Ïöî" --file main.py
```

> üîß Îçî ÎßéÏùÄ CLI ÏòµÏÖòÍ≥º ÏÇ¨Ïö©Î≤ïÏùÄ [CHEATSHEET.md](./CHEATSHEET.md#cli-ÏÇ¨Ïö©Î≤ï)Î•º Ï∞∏Í≥†ÌïòÏÑ∏Ïöî.


## Í∞úÎ∞ú

### ÌÖåÏä§Ìä∏ Ïã§Ìñâ

```bash
# Î™®Îì† ÌÖåÏä§Ìä∏
make test

# ÌäπÏ†ï ÌÖåÏä§Ìä∏
make test tests/test_openai.py

# Ïª§Î≤ÑÎ¶¨ÏßÄ Ìè¨Ìï® ÌÖåÏä§Ìä∏
make test-cov
# ÎòêÎäî
make cov

# Ïª§Î≤ÑÎ¶¨ÏßÄ HTML Î¶¨Ìè¨Ìä∏ Î≥¥Í∏∞
make test-cov-report

# ÌäπÏ†ï ÌååÏùºÎßå Ïª§Î≤ÑÎ¶¨ÏßÄ ÌÖåÏä§Ìä∏
make cov tests/test_optional_dependencies.py

# pytest ÏßÅÏ†ë Ïã§Ìñâ
pytest --cov=src/pyhub/llm --cov-report=term --cov-report=html
```

### ÏΩîÎìú ÌíàÏßà Í≤ÄÏÇ¨

```bash
# Ìè¨Îß∑ÌåÖ Î∞è Î¶∞ÌåÖ
make format
make lint

# ÌÉÄÏûÖ Ï≤¥ÌÅ¨
mypy src/
```

### ÎπåÎìú Î∞è Î∞∞Ìè¨

```bash
# Ìå®ÌÇ§ÏßÄ ÎπåÎìú
make build

# PyPI Î∞∞Ìè¨ (Í∂åÌïú ÌïÑÏöî)
make release
```

## Í∏∞Ïó¨ÌïòÍ∏∞

1. Ïù¥ Ï†ÄÏû•ÏÜåÎ•º Ìè¨ÌÅ¨Ìï©ÎãàÎã§
2. Í∏∞Îä• Î∏åÎûúÏπòÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§ (`git checkout -b feature/amazing-feature`)
3. Î≥ÄÍ≤ΩÏÇ¨Ìï≠ÏùÑ Ïª§Î∞ãÌï©ÎãàÎã§ (`git commit -m 'Add amazing feature'`)
4. Î∏åÎûúÏπòÏóê Ìë∏ÏãúÌï©ÎãàÎã§ (`git push origin feature/amazing-feature`)
5. Pull RequestÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§

### Í∏∞Ïó¨ Í∞ÄÏù¥ÎìúÎùºÏù∏

- Î™®Îì† ÏÉà Í∏∞Îä•ÏóêÎäî ÌÖåÏä§Ìä∏Î•º Ìè¨Ìï®Ìï¥Ï£ºÏÑ∏Ïöî
- ÏΩîÎìú Ïä§ÌÉÄÏùºÏùÄ BlackÍ≥º RuffÎ•º Îî∞Î¶ÖÎãàÎã§
- ÌÉÄÏûÖ ÌûåÌä∏Î•º ÏÇ¨Ïö©Ìï¥Ï£ºÏÑ∏Ïöî
- Î¨∏ÏÑúÎ•º ÏóÖÎç∞Ïù¥Ìä∏Ìï¥Ï£ºÏÑ∏Ïöî

## ÎùºÏù¥ÏÑ†Ïä§

Ïù¥ ÌîÑÎ°úÏ†ùÌä∏Îäî MIT ÎùºÏù¥ÏÑ†Ïä§Î•º Îî∞Î¶ÖÎãàÎã§. ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ [LICENSE](LICENSE) ÌååÏùºÏùÑ Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî.

## Î¨∏Ï†ú Ìï¥Í≤∞

### ÏùºÎ∞òÏ†ÅÏù∏ Î¨∏Ï†ú

**Q: API ÌÇ§ Ïò§Î•òÍ∞Ä Î∞úÏÉùÌï©ÎãàÎã§**

```python
# Ìï¥Í≤∞ Î∞©Î≤ï 1: ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï
import os
os.environ["OPENAI_API_KEY"] = "your-key"

# Ìï¥Í≤∞ Î∞©Î≤ï 2: ÏßÅÏ†ë Ï†ÑÎã¨
llm = OpenAILLM(api_key="your-key")
```

**Q: ÏÜçÎèÑÍ∞Ä ÎäêÎ¶ΩÎãàÎã§**

```python
# Ï∫êÏãú Ïù∏Ï†ùÏÖòÏúºÎ°ú Ï∫êÏã± ÌôúÏÑ±Ìôî
from pyhub.llm.cache import MemoryCache
cache = MemoryCache()
llm = LLM.create("gpt-4o-mini", cache=cache)
reply = llm.ask("...")

# Îçî Îπ†Î•∏ Î™®Îç∏ ÏÇ¨Ïö©
llm = LLM.create("gpt-3.5-turbo")
```

**Q: Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏù¥ ÎÜíÏäµÎãàÎã§**

```python
# ÎåÄÌôî ÌûàÏä§ÌÜ†Î¶¨ Ï†úÌïú
llm = LLM.create(
    "gpt-4o-mini",
    initial_messages=[]  # ÌûàÏä§ÌÜ†Î¶¨ ÏóÜÏù¥ ÏãúÏûë
)

# Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú ÌûàÏä§ÌÜ†Î¶¨ Ï†ïÎ¶¨
if len(llm) > 10:
    llm.clear()
```

## ÎßÅÌÅ¨

- [Î¨∏ÏÑú](https://pyhub-llm.readthedocs.io)
- [PyPI](https://pypi.org/project/pyhub-llm)
- [GitHub](https://github.com/pyhub-kr/pyhub-llm)
- [Ïù¥Ïäà Ìä∏ÎûòÏª§](https://github.com/pyhub-kr/pyhub-llm/issues)

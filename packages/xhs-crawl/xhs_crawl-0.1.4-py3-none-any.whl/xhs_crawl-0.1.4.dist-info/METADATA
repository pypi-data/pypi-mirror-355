Metadata-Version: 2.1
Name: xhs-crawl
Version: 0.1.4
Summary: 一个异步的小红书爬虫工具，支持笔记内容和图片的批量下载
License: MIT
Keywords: spider,crawler,xiaohongshu,download
Author: LGrok
Author-email: luolin.work@gmail.com
Requires-Python: >=3.9,<4.0
Classifier: Development Status :: 4 - Beta
Classifier: Environment :: Console
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Topic :: Internet :: WWW/HTTP :: Dynamic Content
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Dist: aiofiles (>=23.2.1,<24.0.0)
Requires-Dist: beautifulsoup4 (>=4.12.3,<5.0.0)
Requires-Dist: fake-useragent (>=1.4.0,<2.0.0)
Requires-Dist: httpx (>=0.25.2,<0.26.0)
Requires-Dist: loguru (>=0.7.2,<0.8.0)
Requires-Dist: pydantic (>=2.6.1,<3.0.0)
Requires-Dist: tenacity (>=8.2.3,<9.0.0)
Project-URL: Bug Tracker, https://github.com/LinLL/xhs-crawl/issues
Project-URL: Homepage, https://github.com/LinLL/xhs-crawl
Description-Content-Type: text/markdown

# XHS Crawl

小红书内容爬虫工具

## 功能特点

- 支持帖子内容抓取
- 支持图片下载
- 异步处理
- 自动重试机制

## 安装

```bash
pip install xhs-crawl
```

## 使用说明

### 命令行工具

安装完成后，你可以直接使用命令行工具下载小红书帖子内容：

```bash
xhs-crawl "https://www.xiaohongshu.com/explore/[POST_ID]" -d "./downloads"
```

参数说明：
- 第一个参数为小红书帖子URL（必填）
- `-d` 或 `--dir`：指定图片保存目录，默认为 `./downloads`

### Python代码调用

你也可以在Python代码中调用：

```python
import asyncio
from xhs_crawl import XHSSpider

async def main():
    # 初始化爬虫
    spider = XHSSpider()
    
    try:
        # 获取帖子数据
        url = "https://www.xiaohongshu.com/explore/[POST_ID]"
        post = await spider.get_post_data(url)
        
        if post:
            print(f"标题: {post.title}")
            print(f"内容: {post.content}")
            print(f"发现 {len(post.images)} 张图片")
            
            # 下载图片
            await spider.download_images(post, "./downloads")
    finally:
        # 关闭客户端连接
        await spider.close()

if __name__ == "__main__":
    asyncio.run(main())
```

### 返回数据结构

`get_post_data` 方法返回的 `post` 对象包含以下属性：

- `post_id`: 帖子ID
- `title`: 帖子标题
- `content`: 帖子正文内容
- `images`: 帖子包含的图片URL列表

## 注意事项

1. 请确保提供的URL格式正确
2. 下载目录需要有写入权限
3. 建议合理控制爬取频率，避免对目标网站造成压力
4. 该工具仅用于学习和研究目的，请遵守相关法律法规

## 许可证

MIT License

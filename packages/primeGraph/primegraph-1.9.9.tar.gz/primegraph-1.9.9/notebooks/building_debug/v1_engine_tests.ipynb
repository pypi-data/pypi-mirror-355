{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Test - simple paralle graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from primeGraph.models import GraphState\n",
    "from primeGraph.buffer import History, Incremental\n",
    "from primeGraph import Graph, START, END\n",
    "import time\n",
    "\n",
    "from rich import print as rprint\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "class StateForTestWithHistory(GraphState):\n",
    "    execution_order: History[str]\n",
    "\n",
    "state = StateForTestWithHistory(execution_order=[])\n",
    "\n",
    "def test_graph(storage=None):\n",
    "    state = StateForTestWithHistory(execution_order=[])\n",
    "    graph = Graph(state=state)\n",
    "\n",
    "    @graph.node()\n",
    "    def task1(state):\n",
    "        return {\"execution_order\": \"task1\"}\n",
    "\n",
    "    @graph.node(interrupt=\"before\")\n",
    "    def task2(state):\n",
    "        return {\"execution_order\": \"task2\"}\n",
    "\n",
    "    @graph.node()\n",
    "    def task3(state):\n",
    "        return {\"execution_order\": \"task3\"}\n",
    "\n",
    "    @graph.node()\n",
    "    def task4(state):\n",
    "        return {\"execution_order\": \"task4\"}\n",
    "\n",
    "    # Create parallel paths\n",
    "    graph.add_edge(START, \"task1\")\n",
    "    graph.add_edge(\"task1\", \"task2\")\n",
    "    graph.add_edge(\"task1\", \"task3\")\n",
    "    graph.add_edge(\"task2\", \"task4\")\n",
    "    graph.add_edge(\"task3\", \"task4\")\n",
    "    graph.add_edge(\"task4\", END)\n",
    "    graph.compile()\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primeGraph.checkpoint.local_storage import LocalStorage\n",
    "\n",
    "graph = test_graph(storage=LocalStorage())\n",
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.edges_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(await graph.execute())\n",
    "rprint(graph.state)\n",
    "rprint(graph.execution_engine.get_full_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(await graph.resume())\n",
    "rprint(graph.state)\n",
    "rprint(graph.execution_engine.get_full_state())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Test - parellel test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from primeGraph.models import GraphState\n",
    "from primeGraph.buffer import History, Incremental, LastValue\n",
    "from primeGraph import Graph, START, END\n",
    "\n",
    "\n",
    "from rich import print as rprint\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "class StateForTestWithHistory(GraphState):\n",
    "        execution_order: History[str]\n",
    "        counter: Incremental[int]\n",
    "\n",
    "def test_graph(storage=None):\n",
    "    state = StateForTestWithHistory(execution_order=[], counter=0)\n",
    "    graph = Graph(state=state)\n",
    "\n",
    "    @graph.node()\n",
    "    def task1(state):\n",
    "        return {\"execution_order\": \"task1\", \"counter\": 1}\n",
    "\n",
    "    @graph.node(interrupt=\"after\")\n",
    "    def task2(state):\n",
    "        return {\"execution_order\": \"task2\"}\n",
    "\n",
    "    @graph.node()\n",
    "    def task3(state):\n",
    "        return {\"execution_order\": \"task3\"}\n",
    "\n",
    "    @graph.node()\n",
    "    def task4(state):\n",
    "        return {\"execution_order\": \"task4\"}\n",
    "    \n",
    "    @graph.node()\n",
    "    def task5(state):\n",
    "        return {\"execution_order\": \"task5\"}\n",
    "    \n",
    "    @graph.node()\n",
    "    def task6(state):\n",
    "        return {\"execution_order\": \"task6\"}\n",
    "\n",
    "    # Create parallel paths\n",
    "    graph.add_edge(START, \"task1\")\n",
    "    graph.add_edge(\"task1\", \"task2\")\n",
    "    graph.add_edge(\"task1\", \"task3\")\n",
    "    graph.add_edge(\"task2\", \"task4\")\n",
    "    graph.add_edge(\"task3\", \"task5\")\n",
    "    graph.add_edge(\"task4\", \"task6\")\n",
    "    graph.add_edge(\"task5\", \"task6\")\n",
    "    graph.add_edge(\"task6\", END)\n",
    "    graph.compile()\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primeGraph.checkpoint.local_storage import LocalStorage\n",
    "\n",
    "graph = test_graph(storage=LocalStorage())\n",
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(graph.edges_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await graph.execute()\n",
    "rprint(graph.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(await graph.resume())\n",
    "rprint(graph.state)\n",
    "rprint(graph.execution_engine.get_full_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "exec_task = asyncio.create_task(graph.execute())\n",
    "\n",
    "if exec_task.done():\n",
    "    print(\"Task is done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Test - parellel test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from primeGraph.models import GraphState\n",
    "from primeGraph.buffer import History, Incremental, LastValue\n",
    "from primeGraph import Graph, START, END\n",
    "\n",
    "\n",
    "from rich import print as rprint\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "class SubgraphState(GraphState):\n",
    "    execution_order: History[str]\n",
    "    counter: Incremental[int]\n",
    "    status: LastValue[str]\n",
    "\n",
    "def test_graph(storage=None):\n",
    "    state = SubgraphState(execution_order=[], counter=0, status=\"\")\n",
    "    main_graph = Graph(state=state)\n",
    "\n",
    "    # Create first subgraph\n",
    "    @main_graph.subgraph()\n",
    "    def subgraph_a():\n",
    "        subgraph = Graph(state=state)\n",
    "\n",
    "        @subgraph.node()\n",
    "        def process_1(state):\n",
    "            return {\"execution_order\": \"process_1\", \"counter\": 1}\n",
    "\n",
    "        @subgraph.node()\n",
    "        def process_2(state):\n",
    "            return {\"execution_order\": \"process_2\", \"counter\": 2}\n",
    "\n",
    "        subgraph.add_edge(START, \"process_1\")\n",
    "        subgraph.add_edge(\"process_1\", \"process_2\")\n",
    "        subgraph.add_edge(\"process_2\", END)\n",
    "\n",
    "        return subgraph\n",
    "\n",
    "    # Create second subgraph\n",
    "    @main_graph.subgraph()\n",
    "    def subgraph_b():\n",
    "        subgraph = Graph(state=state)\n",
    "\n",
    "        @subgraph.node()\n",
    "        def process_3(state):\n",
    "            return {\"execution_order\": \"process_3\", \"counter\": 3}\n",
    "\n",
    "        @subgraph.node()\n",
    "        def process_4(state):\n",
    "            return {\"execution_order\": \"process_4\", \"counter\": 4}\n",
    "\n",
    "        subgraph.add_edge(START, \"process_3\")\n",
    "        subgraph.add_edge(\"process_3\", \"process_4\")\n",
    "        subgraph.add_edge(\"process_4\", END)\n",
    "\n",
    "        return subgraph\n",
    "\n",
    "    # Main graph nodes\n",
    "    @main_graph.node()\n",
    "    def initialize(state):\n",
    "        return {\"execution_order\": \"initialize\", \"status\": \"started\"}\n",
    "\n",
    "    @main_graph.node()\n",
    "    def finalize(state):\n",
    "        return {\"execution_order\": \"finalize\", \"status\": \"completed\"}\n",
    "\n",
    "    # Connect main graph with parallel subgraphs\n",
    "    main_graph.add_edge(START, \"initialize\")\n",
    "    main_graph.add_edge(\"initialize\", \"subgraph_a\")\n",
    "    main_graph.add_edge(\"initialize\", \"subgraph_b\")\n",
    "    main_graph.add_edge(\"subgraph_a\", \"finalize\")\n",
    "    main_graph.add_edge(\"subgraph_b\", \"finalize\")\n",
    "    main_graph.add_edge(\"finalize\", END)\n",
    "\n",
    "    main_graph.compile()\n",
    "\n",
    "    return main_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primeGraph.checkpoint.local_storage import LocalStorage\n",
    "\n",
    "graph = test_graph(storage=LocalStorage())\n",
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(graph.edges_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await graph.execute()\n",
    "rprint(graph.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(await graph.resume())\n",
    "rprint(graph.state)\n",
    "rprint(graph.execution_engine.get_full_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "exec_task = asyncio.create_task(graph.execute())\n",
    "\n",
    "if exec_task.done():\n",
    "    print(\"Task is done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from primeGraph.models import GraphState\n",
    "from primeGraph.buffer import History, LastValue\n",
    "from primeGraph import Graph, START, END\n",
    "from primeGraph.graph.engine import GraphExecutor\n",
    "import time\n",
    "\n",
    "from rich import print as rprint\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "class RouterState(GraphState):\n",
    "    result: LastValue[dict]  # Store the result from routes\n",
    "    execution_order: History[str]  # Track execution order\n",
    "\n",
    "\n",
    "def test_graph(storage=None):\n",
    "    state = RouterState(result={}, execution_order=[])\n",
    "    graph = Graph(state=state, checkpoint_storage=storage)\n",
    "\n",
    "    @graph.node()\n",
    "    def process_data(state):\n",
    "        print(\"process_data\")\n",
    "        if True:\n",
    "            return \"route_a\"  # Router node returns next node name\n",
    "        else:\n",
    "            return \"route_b\"\n",
    "\n",
    "    @graph.node()\n",
    "    def route_a(state):\n",
    "        print(\"route_a\")\n",
    "        time.sleep(0.1)\n",
    "        return {\n",
    "            \"result\": {\"path\": \"A\"},\n",
    "            \"execution_order\": \"route_a\",\n",
    "        }\n",
    "\n",
    "    @graph.node()\n",
    "    def route_b(state):\n",
    "        print(\"route_b\")\n",
    "        time.sleep(0.1)\n",
    "        return {\n",
    "            \"result\": {\"path\": \"B\"},\n",
    "            \"execution_order\": \"route_b\",\n",
    "        }\n",
    "\n",
    "    # Add router edge and possible routes\n",
    "    \n",
    "    graph.add_edge(\"route_a\", END)\n",
    "    graph.add_edge(\"route_b\", END)\n",
    "    graph.add_router_edge(START, \"process_data\")\n",
    "\n",
    "    graph.compile()\n",
    "\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primeGraph.checkpoint.local_storage import LocalStorage\n",
    "\n",
    "graph = test_graph(storage=LocalStorage())\n",
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Add debug logging\n",
    "logging.getLogger('primeGraph.checkpoint.local_storage').setLevel(logging.DEBUG)\n",
    "logging.getLogger('primeGraph.graph.engine').setLevel(logging.DEBUG)\n",
    "\n",
    "executor = GraphExecutor(graph)\n",
    "task = asyncio.create_task(executor.execute())\n",
    "\n",
    "# Wait a bit and print engine state before checkpoint\n",
    "await asyncio.sleep(4)\n",
    "print(\"\\n=== Engine State BEFORE checkpoint ===\")\n",
    "rprint(executor.get_full_state())\n",
    "\n",
    "# Print checkpoint state right after it's saved\n",
    "await asyncio.sleep(0.1)  # Small delay to ensure checkpoint is saved\n",
    "last_checkpoint = list(graph.checkpoint_storage._storage[list(graph.checkpoint_storage._storage.keys())[0]].keys())[-1]\n",
    "checkpoint_state = graph.checkpoint_storage._storage[list(graph.checkpoint_storage._storage.keys())[0]][last_checkpoint]\n",
    "print(\"\\n=== Checkpoint State ===\")\n",
    "rprint(checkpoint_state.engine_state)\n",
    "\n",
    "# Wait a bit longer for task5 to complete\n",
    "await asyncio.sleep(2)\n",
    "print(\"\\n=== Engine State AFTER task5 completion ===\")\n",
    "rprint(executor.get_full_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(executor.get_full_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(graph.checkpoint_storage._storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_checkpoint = list(graph.checkpoint_storage._storage[list(graph.checkpoint_storage._storage.keys())[0]].keys())[-1]\n",
    "last_checkpoint_state = graph.checkpoint_storage._storage[list(graph.checkpoint_storage._storage.keys())[0]][last_checkpoint]\n",
    "\n",
    "executor = GraphExecutor(graph)\n",
    "\n",
    "executor.load_full_state(last_checkpoint_state.engine_state)\n",
    "rprint(executor.get_full_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor.resume()\n",
    "await executor.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.chain_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from primeGraph import Graph, START, END\n",
    "from pydantic import Field, BaseModel\n",
    "from primeGraph.models import GraphState\n",
    "from primeGraph.buffer import History, LastValue\n",
    "from rich import print as rprint\n",
    "from typing import Dict, Union, List, Any\n",
    "from primeGraph.checkpoint.postgresql import PostgreSQLStorage\n",
    "from primeGraph.graph.engine import GraphExecutor\n",
    "from rich import print as rprint\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "sys_prompt_start = \"\"\"\n",
    "You are a helpful assistant that is able to help the user with their goals.\n",
    "\n",
    "You are part of a workflow for the user to plan for something. This is the first step of the workflow.\n",
    "\n",
    "Give the user a welcome in a brief way and prepare them to start sharing their goal. Things you need to let the user know:\n",
    "\n",
    "- They should share their goal in a clear and concise manner\n",
    "- [OPTIONAL] they should share any relevant context or details about the goal\n",
    "- [OPTIONAL] They should express what success looks like for this goal\n",
    "- [OPTIONAL] They should share any constraints or requirements for the goal\n",
    "\"\"\"\n",
    "\n",
    "sys_prompt_process_message = \"\"\"\n",
    "\n",
    "==== OVERALL GUIDANCE =====\n",
    "You are in a planning workflow in this is the second (and more important) step.\n",
    "\n",
    "Your goal is to analyze the user's message and route them to the next step in the workflow.\n",
    "\n",
    "You will be given the user's message and the conversation history. Give more weight to the user's message than the conversation history.\n",
    "\n",
    "__ The main goal for this entire planning process: __\n",
    "\n",
    "- Break down the user goal in the planning steps that are:\n",
    "    - Clear\n",
    "    - Concise\n",
    "    - Easy to understand\n",
    "    - Easy to follow\n",
    "\n",
    "__ The main goal with this step is to: __\n",
    "\n",
    "- Analyze current information about the goal and the conversation history\n",
    "- Assess if the information gathered is enough to create a good plan\n",
    "- Route user to the next step in the workflow\n",
    "- Make sure you capture user's intent and route them to the correct step\n",
    "- Make the user experience seemless and seamless\n",
    "\n",
    "==== WORKFLOW STEPS =====\n",
    "\n",
    "Everything evolve around you capacity to create a good plan in the end. \n",
    "Based on the information gathered, evaluate the following:\n",
    "\n",
    "IF information about the goal, details, or anything that can help you create a good plan is needed:\n",
    "- [Follow up questions]\n",
    "    - Ask follow up questions to gather more information about the goal\n",
    "    - Analyze all the information gathered and judge if addional information is needed\n",
    "    - Be clear and concise with the follow up questions\n",
    "\n",
    "IF all the information is gathered and/or the user is ready to move forward, choose between:\n",
    "- [Summarize and ask permission]: \n",
    "    - Help the user visualize the high level plan\n",
    "    - Share your plan into macro steps with a brief summary of what each step entails\n",
    "    - Check if the user would like to proceed with the next step\n",
    "- [Finalize]: If the summary is approved, or the user is saying that they are ready to move forward, you should proceed to the next step\n",
    "\n",
    "Unrelated:\n",
    "- [Outside of the workflow]: The user is explicitly asking to move out of the plan workflow or explicitly saying that they are done or want to cancel.\n",
    "\n",
    "\n",
    "==== GUIDELINES ON HOW TO ACT =====\n",
    "\n",
    "- Pay extreme attention to the user's message\n",
    "- Try to understand the user's intent\n",
    "- Always pick only ONE of the options presented to you on WORKFLOW STEPS\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sys_prompt_followup = \"\"\"\n",
    "==== OVERALL GUIDANCE =====\n",
    "You are in a planning workflow and this is a follow up step.\n",
    "\n",
    "Your goal is to analyze the user's goal, the information gathered and the conversation history.\n",
    "Based on the information gathered, you should ask follow up questions to gather more information about the goal.\n",
    "\n",
    "\n",
    "__ The main goal for this entire planning process: __\n",
    "\n",
    "- Break down the user goal in the planning steps that are:\n",
    "    - Clear\n",
    "    - Concise\n",
    "    - Easy to understand\n",
    "    - Easy to follow\n",
    "\n",
    "__ The main goal with this step is to: __\n",
    "\n",
    "- Analyze current information\n",
    "- Make additional follow up questions ONLY IF NEEDED\n",
    "    - The act of gather more information should be to make sure that the planning process has its goals achieved\n",
    "\n",
    "==== GUIDELINES ON HOW TO ACT =====\n",
    "\n",
    "- Pay extreme attention to all the curren information gathered\n",
    "- Identify the gaps into a good planning to achieve the goal and the information needed to fill those gaps\n",
    "- Make follow up questions ONLY IF NEEDED\n",
    "    - The act of gather more information should be to make sure that the planning process has its goals achieved\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sys_prompt_summarize = \"\"\"\n",
    "==== OVERALL GUIDANCE =====\n",
    "You are in a planning workflow and this is a summarize step.\n",
    "\n",
    "Your goal is to share with the user a high level overview of the plan you are about to create.\n",
    "Make it short anc concise, but also include all the important details.\n",
    "Make it visually easy to understand and review (on the user's end). \n",
    "Ask if the user would like to proceed with the next step.\n",
    "\n",
    "\n",
    "__ The main goal for this entire planning process: __\n",
    "\n",
    "- Break down the user goal in the planning steps that are:\n",
    "    - Clear\n",
    "    - Concise\n",
    "    - Easy to understand\n",
    "    - Easy to follow\n",
    "\n",
    "__ The main goal with this step is to: __\n",
    "\n",
    "- Summarize the plan in a way that is easy to understand and review\n",
    "- Make it visually easy to understand and review (on the user's end)\n",
    "- Ask if the user would like to proceed with the next step\n",
    "\n",
    "==== GUIDELINES ON HOW TO ACT =====\n",
    "\n",
    "- Make sure you include all the important details\n",
    "- Make sure you have a decent break down of the plan\n",
    "- Make it visually easy to understand and review (on the user's end)\n",
    "- Ask if the user would like to proceed with the next step\n",
    "\"\"\"\n",
    "\n",
    "sys_prompt_finalize = \"\"\"\n",
    "==== OVERALL GUIDANCE =====\n",
    "You are in a planning workflow and this is a finalize step.\n",
    "\n",
    "Your goal is to say goodbye to the user and thank them for using your service.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class PlannerState(GraphState):\n",
    "    conversation: History[Dict[str, str]] = Field(default_factory=list)\n",
    "    model_message: LastValue[Union[str, None]] = Field(\n",
    "        default=None\n",
    "    )  # Should be consumed by the bot before moving forward interruptions\n",
    "    user_message: LastValue[Union[str, None]] = Field(\n",
    "        default=None\n",
    "    )  # Should be consumed by the bot before moving forward interruptions\n",
    "    is_followup: LastValue[Union[None, bool]] = Field(default=None)\n",
    "    is_summarize: LastValue[Union[bool, None]] = Field(default=None)\n",
    "    is_finalize: LastValue[Union[bool, None]] = Field(default=None)\n",
    "    is_outside_of_the_workflow: LastValue[Union[bool, None]] = Field(default=None)\n",
    "    plan_goal: LastValue[str] = Field(default=\"\")\n",
    "    plan_summary: LastValue[str] = Field(default=\"\")\n",
    "    plan_steps: LastValue[List[str]] = Field(default_factory=list)\n",
    "    plan_details: History[str] = Field(default_factory=list)\n",
    "    current_information_assessment: LastValue[str] = Field(default=\"\")\n",
    "    follow_up_questions: LastValue[List[str]] = Field(default_factory=list)\n",
    "    plan_summary: LastValue[str] = Field(default=\"\")\n",
    "    plan_steps: LastValue[List[str]] = Field(default_factory=list)\n",
    "\n",
    "def planner_graph(graph_state: PlannerState, checkpoint_storage: Union[PostgreSQLStorage, None]) -> Graph:\n",
    "    plan_graph = Graph(state=graph_state, verbose=True, checkpoint_storage=checkpoint_storage)\n",
    "    client = instructor.from_openai(OpenAI())\n",
    "\n",
    "    @plan_graph.node(interrupt=\"after\")\n",
    "    def start_conversation(state: PlannerState) -> dict[str, Any]:\n",
    "        class StartConversationResponse(BaseModel):\n",
    "            response: str = Field(description=\"Your response to the user\")\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            response_model=StartConversationResponse,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt_start},\n",
    "            ],\n",
    "        )\n",
    "        return {\"conversation\": {\"role\": \"assistant\", \"content\": completion.response}}\n",
    "\n",
    "    @plan_graph.node()\n",
    "    def process_user_message(state: PlannerState) -> dict[str, Any]:\n",
    "        class ProcessMessageResponse(BaseModel):\n",
    "            plan_goal: str = Field(description=\"The main goal of the plan\")\n",
    "            plan_details: List[str] = Field(description=\"Any additional relevant details of the plan\")\n",
    "            is_followup: bool # = Field(description=\"If you need to ask follow up questions\")\n",
    "            is_summarize: bool # = Field(description=\"If there are not follow up questions on your end.\")\n",
    "            is_finalize: bool # = Field(\n",
    "            #     description=\"If the user is explicitly asking to move out of the plan workflow\"\n",
    "            # )\n",
    "            is_outside_of_the_workflow: bool # = Field(\n",
    "            #     description=\"If the user is explicitly asking to move out of the plan workflow\"\n",
    "            # )\n",
    "\n",
    "        # add user message to the conversation\n",
    "        if state.user_message:\n",
    "            state.conversation.append({\"role\": \"user\", \"content\": state.user_message})\n",
    "\n",
    "        # Extract structured data from natural language\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            response_model=ProcessMessageResponse,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt_process_message},\n",
    "                *state.conversation,\n",
    "                {\"role\": \"user\", \"content\": state.user_message},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # unpack plan_details:\n",
    "        if completion.plan_details:\n",
    "            for detail in completion.plan_details:\n",
    "                state.plan_details.append(detail)\n",
    "\n",
    "        return {\n",
    "            \"plan_goal\": completion.plan_goal,\n",
    "            \"user_message\": None,\n",
    "            \"model_message\": None,\n",
    "            \"is_followup\": completion.is_followup,\n",
    "            \"is_summarize\": completion.is_summarize,\n",
    "            \"is_finalize\": completion.is_finalize,\n",
    "            \"is_outside_of_the_workflow\": completion.is_outside_of_the_workflow,\n",
    "        }\n",
    "\n",
    "    @plan_graph.node()\n",
    "    def response_router(state: PlannerState) -> str:\n",
    "        if state.is_finalize:\n",
    "            return \"finalize_plan\"\n",
    "        elif state.is_summarize:\n",
    "            return \"summarize_plan\"\n",
    "        elif state.is_followup:\n",
    "            return \"make_followup_questions\"\n",
    "        else:\n",
    "            return \"process_user_message\"\n",
    "                \n",
    "    @plan_graph.node(interrupt=\"after\")\n",
    "    def make_followup_questions(state: PlannerState):\n",
    "        class FollowupResponse(BaseModel):\n",
    "            plan_goal: str = Field(description=\"The main goal of the plan\")\n",
    "            plan_details: List[str] = Field(description=\"Any additional you got from your interactions with the user\")\n",
    "            current_information_assessment: str = Field(description=\"A brief assessment of the current information gathered\")\n",
    "            follow_up_questions: List[str] = Field(description=\"The follow up questions that you think are needed to gather more information about the goal\")\n",
    "            response: str = Field(description=\"Your response with the follow up questions\")\n",
    "\n",
    "    \n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            response_model=FollowupResponse,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt_followup},\n",
    "                *state.conversation,\n",
    "                {\"role\": \"user\", \"content\": state.user_message},\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        # unpack plan_details:\n",
    "        if completion.plan_details:\n",
    "            for detail in completion.plan_details:\n",
    "                state.plan_details.append(detail)\n",
    "\n",
    "        return {\n",
    "            \"conversation\": {\"role\": \"assistant\", \"content\": completion.response},\n",
    "            \"user_message\": None,\n",
    "            \"model_message\": completion.response,\n",
    "            \"current_information_assessment\": completion.current_information_assessment,\n",
    "            \"follow_up_questions\": completion.follow_up_questions,\n",
    "        }\n",
    "  \n",
    "    @plan_graph.node(interrupt=\"after\")\n",
    "    def summarize_plan(state: PlannerState):\n",
    "        class SummarizeResponse(BaseModel):\n",
    "            response: str = Field(description=\"Your response to the user\")\n",
    "            plan_summary: str = Field(description=\"A summary of the plan\")\n",
    "            plan_steps: List[str] = Field(description=\"A list of the steps that are part of the plan\")\n",
    "\n",
    "            \n",
    "        # Extract structured data from natural language\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            response_model=SummarizeResponse,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt_summarize},\n",
    "                *state.conversation,\n",
    "                {\"role\": \"user\", \"content\": state.user_message},\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "        return {\n",
    "            \"conversation\": {\"role\": \"assistant\", \"content\": completion.response},\n",
    "            \"user_message\": None,\n",
    "            \"model_message\": completion.response,\n",
    "            \"plan_summary\": completion.plan_summary,\n",
    "            \"plan_steps\": completion.plan_steps,\n",
    "        }\n",
    "    \n",
    "    @plan_graph.node()\n",
    "    def finalize_plan(state: PlannerState):\n",
    "        class FinalizeResponse(BaseModel):\n",
    "            response: str = Field(description=\"Your response to the user\")\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            response_model=FinalizeResponse,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt_finalize},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"conversation\": {\"role\": \"assistant\", \"content\": completion.response},\n",
    "            \"user_message\": None,\n",
    "            \"model_message\": completion.response,\n",
    "        }\n",
    "\n",
    "\n",
    "    plan_graph.add_edge(START, \"start_conversation\")\n",
    "    plan_graph.add_edge(\"start_conversation\", \"process_user_message\")\n",
    "    plan_graph.add_router_edge(\"process_user_message\", \"response_router\")\n",
    "\n",
    "    plan_graph.add_edge(\"summarize_plan\", \"process_user_message\")\n",
    "    plan_graph.add_edge(\"make_followup_questions\", \"process_user_message\")\n",
    "    plan_graph.add_edge(\"finalize_plan\", END)\n",
    "    \n",
    "    plan_graph.compile()\n",
    "\n",
    "    return plan_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primeGraph.checkpoint.local_storage import LocalStorage\n",
    "\n",
    "graph = planner_graph(checkpoint_storage=LocalStorage(), graph_state=PlannerState())\n",
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(graph.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = GraphExecutor(graph)\n",
    "\n",
    "await executor.execute()\n",
    "rprint(graph.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await executor.resume()\n",
    "rprint(graph.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await executor.resume()\n",
    "rprint(graph.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Add debug logging\n",
    "logging.getLogger('primeGraph.checkpoint.local_storage').setLevel(logging.DEBUG)\n",
    "logging.getLogger('primeGraph.graph.engine').setLevel(logging.DEBUG)\n",
    "\n",
    "executor = GraphExecutor(graph)\n",
    "task = asyncio.create_task(executor.execute())\n",
    "\n",
    "# Wait a bit and print engine state before checkpoint\n",
    "await asyncio.sleep(4)\n",
    "print(\"\\n=== Engine State BEFORE checkpoint ===\")\n",
    "rprint(executor.get_full_state())\n",
    "\n",
    "# Print checkpoint state right after it's saved\n",
    "await asyncio.sleep(0.1)  # Small delay to ensure checkpoint is saved\n",
    "last_checkpoint = list(graph.checkpoint_storage._storage[list(graph.checkpoint_storage._storage.keys())[0]].keys())[-1]\n",
    "checkpoint_state = graph.checkpoint_storage._storage[list(graph.checkpoint_storage._storage.keys())[0]][last_checkpoint]\n",
    "print(\"\\n=== Checkpoint State ===\")\n",
    "rprint(checkpoint_state.engine_state)\n",
    "\n",
    "# Wait a bit longer for task5 to complete\n",
    "await asyncio.sleep(2)\n",
    "print(\"\\n=== Engine State AFTER task5 completion ===\")\n",
    "rprint(executor.get_full_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(executor.get_full_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(graph.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(graph.checkpoint_storage._storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_checkpoint = list(graph.checkpoint_storage._storage[list(graph.checkpoint_storage._storage.keys())[0]].keys())[-1]\n",
    "last_checkpoint_state = graph.checkpoint_storage._storage[list(graph.checkpoint_storage._storage.keys())[0]][last_checkpoint]\n",
    "\n",
    "executor = GraphExecutor(graph)\n",
    "\n",
    "executor.load_full_state(last_checkpoint_state.engine_state)\n",
    "rprint(executor.get_full_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor.resume()\n",
    "await executor.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.chain_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from primeGraph import Graph, START, END\n",
    "from pydantic import Field, BaseModel\n",
    "from primeGraph.models import GraphState\n",
    "from primeGraph.buffer import History, LastValue\n",
    "from primeGraph.checkpoint.local_storage import LocalStorage\n",
    "from rich import print as rprint\n",
    "from typing import Dict, Union, List, Any\n",
    "from primeGraph.checkpoint.postgresql import PostgreSQLStorage\n",
    "from rich import print as rprint\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S.%f'\n",
    ")\n",
    "\n",
    "class StateForTestWithHistory(GraphState):\n",
    "  execution_order: History[str]\n",
    "\n",
    "storage = LocalStorage()\n",
    "\n",
    "def planner_graph():\n",
    "    state = StateForTestWithHistory(execution_order=[])\n",
    "    \n",
    "    graph = Graph(state=state, checkpoint_storage=storage)\n",
    "\n",
    "    @graph.node()\n",
    "    def task1(state):\n",
    "        print(\"task1\")\n",
    "        return {\"execution_order\": \"task1\"}\n",
    "\n",
    "    @graph.node()\n",
    "    def task2(state):\n",
    "        print(\"task2\")\n",
    "        return {\"execution_order\": \"task2\"}\n",
    "\n",
    "    @graph.node()\n",
    "    def task3(state):\n",
    "        print(\"task3\")\n",
    "        return {\"execution_order\": \"task3\"}\n",
    "\n",
    "    @graph.node()\n",
    "    def task4(state):\n",
    "        print(\"task4\")\n",
    "\n",
    "        return {\"execution_order\": \"task4\"}\n",
    "\n",
    "    @graph.node()\n",
    "    def task5(state):\n",
    "        print(\"task5\")\n",
    "        return {\"execution_order\": \"task5\"}\n",
    "\n",
    "    @graph.node(interrupt=\"before\")\n",
    "    def task6(state):\n",
    "        print(\"task6\")\n",
    "        return {\"execution_order\": \"task6\"}\n",
    "\n",
    "    graph.add_edge(START, \"task1\")\n",
    "    graph.add_edge(\"task1\", \"task2\")\n",
    "    graph.add_edge(\"task2\", \"task3\")\n",
    "    graph.add_edge(\"task2\", \"task4\")\n",
    "    graph.add_edge(\"task2\", \"task5\")\n",
    "    graph.add_edge(\"task4\", \"task6\")\n",
    "    graph.add_edge(\"task3\", \"task6\")\n",
    "    graph.add_edge(\"task5\", \"task6\")\n",
    "    graph.add_edge(\"task6\", END)\n",
    "    graph.compile()\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primeGraph.checkpoint.local_storage import LocalStorage\n",
    "\n",
    "graph = planner_graph()\n",
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"first execution\")\n",
    "chain_id = await graph.execute()\n",
    "# rprint(plan_graph_state)\n",
    "# rprint(graph.chain_status)\n",
    "rprint(graph.execution_engine.get_full_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await graph.resume()\n",
    "rprint(graph.execution_engine.get_full_state())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_id = await graph.execute()\n",
    "assert all(\n",
    "task in graph.state.execution_order for task in [\"task1\", \"task2\", \"task3\", \"task4\", \"task5\"]\n",
    "), \"tasks are not in there\"\n",
    "assert len(graph.checkpoint_storage.list_checkpoints(graph.chain_id)) == 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a new chain just to test the load from checkpoint\n",
    "new_chain_id = await graph.execute()\n",
    "\n",
    "# loading first chain state\n",
    "graph = planner_graph()\n",
    "graph.load_from_checkpoint(new_chain_id)\n",
    "\n",
    "# resuming execution\n",
    "# await graph.resume()\n",
    "# assert all(\n",
    "# task in graph.state.execution_order for task in [\"task1\", \"task2\", \"task3\", \"task4\", \"task5\", \"task6\"]\n",
    "# ), \"tasks are not in there\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.edges_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await graph.resume()\n",
    "rprint(graph.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await executor.resume()\n",
    "rprint(graph.state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

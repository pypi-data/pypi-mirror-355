{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "\n",
    "\n",
    "from primeGraph.buffer.factory import History, Incremental, LastValue\n",
    "from primeGraph.constants import END, START\n",
    "from primeGraph.graph.executable import Graph\n",
    "from primeGraph.models.state import GraphState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing execution plan conversion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "egraph = Graph()\n",
    "\n",
    "\n",
    "@egraph.node()\n",
    "def a():\n",
    "    time.sleep(1)\n",
    "    print(\"a \\n\", datetime.now())\n",
    "\n",
    "\n",
    "@egraph.node()\n",
    "def b():\n",
    "    time.sleep(1)\n",
    "    print(\"b \\n\", datetime.now())\n",
    "\n",
    "\n",
    "@egraph.node()\n",
    "def c():\n",
    "    time.sleep(1)\n",
    "    print(\"c \\n\", datetime.now())\n",
    "\n",
    "\n",
    "@egraph.node()\n",
    "def d():\n",
    "    time.sleep(1)\n",
    "    print(\"d \\n\", datetime.now())\n",
    "\n",
    "\n",
    "@egraph.node()\n",
    "def e():\n",
    "    time.sleep(1)\n",
    "    print(\"e \\n\", datetime.now())\n",
    "\n",
    "\n",
    "@egraph.node()\n",
    "def f():\n",
    "    time.sleep(1)\n",
    "    print(\"f\", datetime.now())\n",
    "\n",
    "\n",
    "@egraph.node()\n",
    "def g():\n",
    "    time.sleep(1)\n",
    "    print(\"g \\n\", datetime.now())\n",
    "\n",
    "\n",
    "egraph.add_edge(START, \"a\")\n",
    "egraph.add_edge(\"a\", \"b\")\n",
    "egraph.add_edge(\"b\", \"c\")\n",
    "egraph.add_edge(\"b\", \"d\")\n",
    "egraph.add_edge(\"d\", \"e\")\n",
    "egraph.add_edge(\"c\", \"f\")\n",
    "egraph.add_edge(\"f\", \"g\")\n",
    "egraph.add_edge(\"e\", \"g\")\n",
    "egraph.add_edge(\"g\", END)\n",
    "\n",
    "egraph.compile()\n",
    "egraph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egraph.execution_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egraph.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "egraph = Graph()\n",
    "\n",
    "\n",
    "@egraph.node()\n",
    "def a():\n",
    "    time.sleep(1)\n",
    "    print(\"a\", datetime.now())\n",
    "\n",
    "\n",
    "@egraph.node()\n",
    "def b():\n",
    "    time.sleep(1)\n",
    "    print(\"b\", datetime.now())\n",
    "\n",
    "\n",
    "@egraph.node()\n",
    "def c():\n",
    "    time.sleep(1)\n",
    "    print(\"c\", datetime.now())\n",
    "\n",
    "\n",
    "@egraph.node()\n",
    "def d():\n",
    "    time.sleep(1)\n",
    "    print(\"d\", datetime.now())\n",
    "\n",
    "\n",
    "@egraph.node()\n",
    "def e():\n",
    "    time.sleep(1)\n",
    "    print(\"e\", datetime.now())\n",
    "\n",
    "\n",
    "@egraph.node()\n",
    "def f():\n",
    "    time.sleep(1)\n",
    "    print(\"f\", datetime.now())\n",
    "\n",
    "\n",
    "@egraph.node()\n",
    "def g():\n",
    "    time.sleep(1)\n",
    "    time.sleep(1)\n",
    "    print(\"g\", datetime.now())\n",
    "\n",
    "\n",
    "egraph.add_edge(START, \"a\")\n",
    "egraph.add_edge(\"a\", \"b\")\n",
    "egraph.add_edge(\"b\", \"c\")\n",
    "egraph.add_edge(\"c\", \"d\")\n",
    "egraph.add_edge(\"d\", \"e\")\n",
    "egraph.add_edge(\"e\", \"f\")\n",
    "egraph.add_edge(\"f\", \"g\")\n",
    "egraph.add_edge(\"g\", END)\n",
    "\n",
    "egraph.compile()\n",
    "egraph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egraph.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "# Create a test graph with a mix of parallel and sequential paths\n",
    "test_graph = Graph()\n",
    "\n",
    "\n",
    "@test_graph.node()\n",
    "def start_task():\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S.%f')}] Starting workflow \\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "@test_graph.node()\n",
    "def parallel_task_1():\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S.%f')}] Starting parallel task 1 \\n\")\n",
    "    time.sleep(4)\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S.%f')}] Finished parallel task 1 \\n\")\n",
    "\n",
    "\n",
    "@test_graph.node()\n",
    "def parallel_task_2():\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S.%f')}] Starting parallel task 2 \\n\")\n",
    "    time.sleep(4)\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S.%f')}] Finished parallel task 2 \\n\")\n",
    "\n",
    "\n",
    "@test_graph.node()\n",
    "def final_task():\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S.%f')}] Running final task \\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "# Create a workflow with parallel execution\n",
    "test_graph.add_edge(START, \"start_task\")\n",
    "test_graph.add_edge(\"start_task\", \"parallel_task_1\")\n",
    "test_graph.add_edge(\"start_task\", \"parallel_task_2\")\n",
    "test_graph.add_edge(\"parallel_task_1\", \"final_task\")\n",
    "test_graph.add_edge(\"parallel_task_2\", \"final_task\")\n",
    "test_graph.add_edge(\"final_task\", END)\n",
    "\n",
    "# Compile and execute\n",
    "test_graph.compile()\n",
    "print(\"\\nExecution Plan:\")\n",
    "rprint(test_graph.execution_plan)\n",
    "print(\"\\nStarting execution:\")\n",
    "start_time = time.time()\n",
    "test_graph.execute()\n",
    "end_time = time.time()\n",
    "print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Define a state model with different buffer types\n",
    "class TestState(GraphState):\n",
    "    counter: Incremental[int]  # Will accumulate values\n",
    "    metrics: History[Dict[str, float]]  # Will keep history of all updates\n",
    "    current_status: LastValue[str]  # Will only keep last value\n",
    "\n",
    "\n",
    "# Initialize the graph with state\n",
    "state = TestState(counter=0, metrics={}, current_status=\"\")\n",
    "\n",
    "test_graph = Graph(state=state)\n",
    "\n",
    "\n",
    "# Test Incremental Buffer\n",
    "@test_graph.node()\n",
    "def add_to_counter(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"counter\": 5}\n",
    "\n",
    "\n",
    "@test_graph.node()\n",
    "def add_more_to_counter(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"counter\": 3}\n",
    "\n",
    "\n",
    "# Test History Buffer\n",
    "@test_graph.node()\n",
    "def add_metrics(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"metrics\": {\"accuracy\": 0.95, \"loss\": 0.1}}\n",
    "\n",
    "\n",
    "@test_graph.node()\n",
    "def update_metrics(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"metrics\": {\"loss\": 0.05, \"precision\": 0.88}}\n",
    "\n",
    "\n",
    "# Test LastValue Buffer\n",
    "@test_graph.node()\n",
    "def set_status_running(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"current_status\": \"running\"}\n",
    "\n",
    "\n",
    "@test_graph.node()\n",
    "def set_status_complete(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"current_status\": \"complete\"}\n",
    "\n",
    "\n",
    "# Create the workflow\n",
    "test_graph.add_edge(START, \"add_to_counter\")\n",
    "test_graph.add_edge(\"add_to_counter\", \"add_more_to_counter\")\n",
    "test_graph.add_edge(\"add_more_to_counter\", \"add_metrics\")\n",
    "test_graph.add_edge(\"add_metrics\", \"update_metrics\")\n",
    "test_graph.add_edge(\"update_metrics\", \"set_status_running\")\n",
    "test_graph.add_edge(\"set_status_running\", \"set_status_complete\")\n",
    "test_graph.add_edge(\"set_status_complete\", END)\n",
    "\n",
    "# Compile and execute\n",
    "test_graph.compile()\n",
    "test_graph.execute(timeout=10)\n",
    "\n",
    "# Print final state\n",
    "print(\"\\nFinal State:\")\n",
    "print(f\"Counter: {test_graph.state.counter}\")  # Should be 8 (5 + 3)\n",
    "print(\n",
    "    f\"Metrics History: {test_graph.state.metrics}\"\n",
    ")  # Should contain both metric updates\n",
    "print(f\"Current Status: {test_graph.state.current_status}\")  # Should be \"complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from tiny_graph.buffer.factory import History, Incremental, LastValue\n",
    "from tiny_graph.constants import END, START\n",
    "from tiny_graph.graph.executable import Graph\n",
    "from tiny_graph.models.state import GraphState\n",
    "\n",
    "\n",
    "# Define a state model with different buffer types\n",
    "class TestState(GraphState):\n",
    "    counter: Incremental[int]  # Will accumulate values\n",
    "    metrics: History[Dict[str, float]]  # Will keep history of all updates\n",
    "    current_status: LastValue[str]  # Will only keep last value\n",
    "\n",
    "\n",
    "# Initialize the graph with state\n",
    "state = TestState(counter=0, metrics={}, current_status=\"\")\n",
    "\n",
    "test_graph = Graph(state=state)\n",
    "\n",
    "\n",
    "# Define nodes that will run in parallel and update the same state\n",
    "@test_graph.node()\n",
    "def increment_counter(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"counter\": 1}\n",
    "\n",
    "\n",
    "@test_graph.node()\n",
    "def decrement_counter(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"counter\": -1}\n",
    "\n",
    "\n",
    "@test_graph.node()\n",
    "def update_status(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"current_status\": \"in_progress\"}\n",
    "\n",
    "\n",
    "# Create the workflow with parallel execution\n",
    "test_graph.add_edge(START, \"increment_counter\")\n",
    "test_graph.add_edge(START, \"decrement_counter\")\n",
    "test_graph.add_edge(START, \"update_status\")\n",
    "test_graph.add_edge(\"increment_counter\", END)\n",
    "test_graph.add_edge(\"decrement_counter\", END)\n",
    "test_graph.add_edge(\"update_status\", END)\n",
    "\n",
    "# Compile and execute\n",
    "test_graph.compile()\n",
    "test_graph.execute(timeout=10)\n",
    "\n",
    "# Print final state\n",
    "print(\"\\nFinal State:\")\n",
    "print(\n",
    "    f\"Counter: {test_graph.state.counter}\"\n",
    ")  # Should reflect the net effect of increments and decrements\n",
    "print(\n",
    "    f\"Metrics History: {test_graph.state.metrics}\"\n",
    ")  # Should be empty as no metrics are updated\n",
    "print(f\"Current Status: {test_graph.state.current_status}\")  # Should be \"in_progress\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph.execution_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph()\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def escape():\n",
    "    print(\"Starting workflow\")\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def process_data():\n",
    "    print(\"Processing data\")\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def validate():\n",
    "    print(\"Validating results\")\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def aa():\n",
    "    print(\"Validating results\")\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def bb():\n",
    "    print(\"Validating results\")\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def prep():\n",
    "    print(\"Workflow complete\")\n",
    "\n",
    "\n",
    "graph.add_edge(START, \"process_data\")\n",
    "graph.add_edge(\"process_data\", \"validate\")\n",
    "graph.add_edge(\"validate\", \"escape\")\n",
    "graph.add_edge(\"escape\", \"prep\")\n",
    "graph.add_edge(\"validate\", \"aa\")\n",
    "graph.add_edge(\"aa\", \"bb\")\n",
    "graph.add_edge(\"bb\", \"prep\")\n",
    "graph.add_edge(\"prep\", END)\n",
    "\n",
    "graph.compile()\n",
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.execution_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from rich import print as rprint\n",
    "from tiny_graph.buffer.factory import History, Incremental, LastValue\n",
    "from tiny_graph.graph.executable import Graph\n",
    "from tiny_graph.models.state import GraphState\n",
    "\n",
    "\n",
    "# Define a state model with buffer types\n",
    "class ComplexTestState(GraphState):\n",
    "    counter: Incremental[int]  # Will accumulate values\n",
    "    status: LastValue[str]  # Will only keep last value\n",
    "    metrics: History[Dict[str, float]]  # Will keep history of all updates\n",
    "\n",
    "\n",
    "# Initialize the graph with state\n",
    "state = ComplexTestState(counter=3, status=\"\", metrics={})\n",
    "graph = Graph(state=state)\n",
    "\n",
    "\n",
    "# Define nodes that will run in parallel and update the same state\n",
    "@graph.node()\n",
    "def increment_counter(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"counter\": 2}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def decrement_counter(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"counter\": -1}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def update_status_to_in_progress(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"status\": \"in_progress\"}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def update_status_to_complete(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"status\": \"complete\"}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def add_metrics(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"metrics\": {\"accuracy\": 0.9, \"loss\": 0.1}}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def update_metrics(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"metrics\": {\"loss\": 0.05, \"precision\": 0.85}}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def finalize_metrics(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"metrics\": {\"loss\": 0.01, \"precision\": 0.99}}\n",
    "\n",
    "\n",
    "# Create the workflow with multiple levels of execution\n",
    "graph.add_edge(START, \"increment_counter\")\n",
    "graph.add_edge(START, \"decrement_counter\")\n",
    "graph.add_edge(START, \"update_status_to_in_progress\")\n",
    "graph.add_edge(\"increment_counter\", \"add_metrics\")\n",
    "graph.add_edge(\"decrement_counter\", \"add_metrics\")\n",
    "graph.add_edge(\"add_metrics\", \"update_metrics\")\n",
    "graph.add_edge(\"update_metrics\", \"finalize_metrics\")\n",
    "graph.add_edge(\"update_status_to_in_progress\", \"update_status_to_complete\")\n",
    "graph.add_edge(\"update_status_to_complete\", \"finalize_metrics\")\n",
    "graph.add_edge(\"finalize_metrics\", END)\n",
    "\n",
    "# Compile and execute\n",
    "graph.compile()\n",
    "\n",
    "\n",
    "graph.start(timeout=10)\n",
    "\n",
    "rprint(graph.state)\n",
    "\n",
    "# Assert final state\n",
    "assert (\n",
    "    graph.state.counter == 1\n",
    "), \"Counter should reflect net effect of increments and decrements\"\n",
    "assert graph.state.status == \"complete\", \"Status should be 'complete'\"\n",
    "assert len(graph.state.metrics) == 3, \"Metrics should contain three updates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from rich import print as rprint\n",
    "from tiny_graph.buffer.factory import History, Incremental, LastValue\n",
    "from tiny_graph.graph.executable import Graph\n",
    "from tiny_graph.models.state import GraphState\n",
    "\n",
    "\n",
    "# Define a state model with buffer types\n",
    "class ComplexTestState(GraphState):\n",
    "    counter: Incremental[int]  # Will accumulate values\n",
    "    status: LastValue[str]  # Will only keep last value\n",
    "    metrics: History[Dict[str, float]]  # Will keep history of all updates\n",
    "\n",
    "\n",
    "# Initialize the graph with state\n",
    "state = ComplexTestState(counter=3, status=\"\", metrics={\"xablau\": 2.0})\n",
    "graph = Graph(state=state)\n",
    "\n",
    "\n",
    "# Define nodes that will run in parallel and update the same state\n",
    "@graph.node()\n",
    "def increment_counter(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"counter\": 2}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def decrement_counter(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"counter\": -1}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def update_status_to_in_progress(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"status\": \"in_progress\"}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def update_status_to_complete(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"status\": \"complete\"}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def add_metrics(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"metrics\": {\"accuracy\": 0.9, \"loss\": 0.1}}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def update_metrics(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"metrics\": {\"loss\": 0.05, \"precision\": 0.85}}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def finalize_metrics(state):\n",
    "    time.sleep(0.5)\n",
    "    return {\"metrics\": {\"loss\": 0.01, \"precision\": 0.99}}\n",
    "\n",
    "\n",
    "# Create the workflow with multiple levels of execution\n",
    "graph.add_edge(START, \"increment_counter\")\n",
    "graph.add_edge(START, \"decrement_counter\")\n",
    "graph.add_edge(START, \"update_status_to_in_progress\")\n",
    "graph.add_edge(\"increment_counter\", \"add_metrics\")\n",
    "graph.add_edge(\"decrement_counter\", \"add_metrics\")\n",
    "graph.add_edge(\"add_metrics\", \"update_metrics\")\n",
    "graph.add_edge(\"update_metrics\", \"finalize_metrics\")\n",
    "graph.add_edge(\"update_status_to_in_progress\", \"update_status_to_complete\")\n",
    "graph.add_edge(\"update_status_to_complete\", \"finalize_metrics\")\n",
    "graph.add_edge(\"finalize_metrics\", END)\n",
    "\n",
    "# Compile and execute\n",
    "graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.buffers[\"counter\"].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.buffers[\"metrics\"].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_graph = Graph()\n",
    "\n",
    "\n",
    "# Define some example actions\n",
    "@simple_graph.node(interrupt=\"before\")\n",
    "def escape():\n",
    "    print(\"Starting workflow\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def process_data():\n",
    "    print(\"Processing data\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def validate():\n",
    "    print(\"Validating results\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def aa():\n",
    "    print(\"Validating results\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def bb():\n",
    "    print(\"Validating results\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def dd():\n",
    "    print(\"Validating results\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def cc():\n",
    "    print(\"Validating results\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def hh():\n",
    "    print(\"Validating results\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def prep():\n",
    "    print(\"Workflow complete\")\n",
    "\n",
    "\n",
    "# Add edges to create workflow\n",
    "simple_graph.add_edge(START, \"process_data\")\n",
    "simple_graph.add_edge(\"process_data\", \"validate\")\n",
    "simple_graph.add_edge(\"validate\", \"escape\")\n",
    "simple_graph.add_edge(\"escape\", \"dd\")\n",
    "simple_graph.add_edge(\"escape\", \"cc\")\n",
    "simple_graph.add_edge(\"cc\", \"hh\")\n",
    "simple_graph.add_edge(\"dd\", \"hh\")\n",
    "simple_graph.add_edge(\"hh\", \"prep\")\n",
    "simple_graph.add_edge(\"validate\", \"aa\")\n",
    "simple_graph.add_edge(\"aa\", \"bb\")\n",
    "simple_graph.add_edge(\"bb\", \"prep\")\n",
    "simple_graph.add_edge(\"prep\", END)\n",
    "\n",
    "simple_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_graph.execution_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "rprint(simple_graph.execution_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(simple_graph._convert_execution_plan())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_execution_plan(plan, metadata):\n",
    "    def slice_list(lst, meta):\n",
    "        result = []\n",
    "        for item in lst:\n",
    "            if isinstance(item, list):\n",
    "                # Recursively slice nested lists\n",
    "                sliced_item = slice_list(item, meta)\n",
    "                if sliced_item:\n",
    "                    result.append(sliced_item)\n",
    "            else:\n",
    "                # Check metadata for slicing\n",
    "                if meta.get(item) == \"before\":\n",
    "                    break\n",
    "                result.append(item)\n",
    "                if meta.get(item) == \"after\":\n",
    "                    break\n",
    "        return result\n",
    "\n",
    "    return slice_list(plan, metadata)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "execution_plan = [\n",
    "    \"process_data\",\n",
    "    \"validate\",\n",
    "    [[\"escape\", [\"dd\", \"cc\"], \"hh\"], [\"aa\", \"bb\"]],\n",
    "    \"prep\",\n",
    "]\n",
    "metadata = {\n",
    "    # 'process_data': 'after',\n",
    "    # 'validate': 'before',\n",
    "    # 'escape': 'after',\n",
    "    \"dd\": \"before\",\n",
    "    # 'cc': 'after',\n",
    "    # 'hh': 'before',\n",
    "    # 'aa': 'after',\n",
    "    # 'bb': 'before',\n",
    "    # 'prep': 'after'\n",
    "}\n",
    "\n",
    "sliced_plan = slice_execution_plan(execution_plan, metadata)\n",
    "print(sliced_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_plan[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_graph.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem here is that in the case of update_status_to_complete,\n",
    "# it's not checking if the next node is a convergence point\n",
    "\n",
    "# it needs to have a function that will check if the next node\n",
    "# connector. find_convergence_point is not doing enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing interruption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from tiny_graph.buffer.factory import History, Incremental, LastValue\n",
    "from tiny_graph.models.state import GraphState\n",
    "\n",
    "\n",
    "# Define a state model with different buffer types\n",
    "class TestState(GraphState):\n",
    "    counter: Incremental[int]  # Will accumulate values\n",
    "    metrics: History[Dict[str, float]]  # Will keep history of all updates\n",
    "    current_status: LastValue[str]  # Will only keep last value\n",
    "\n",
    "\n",
    "# Initialize the graph with state\n",
    "state = TestState(counter=0, metrics={}, current_status=\"\")\n",
    "\n",
    "test_graph = Graph(state=state)\n",
    "\n",
    "\n",
    "# Test Incremental Buffer\n",
    "@test_graph.node()\n",
    "def add_to_counter(state):\n",
    "    time.sleep(0.5)\n",
    "    print(\"add_to_counter\")\n",
    "    return {\"counter\": 5}\n",
    "\n",
    "\n",
    "@test_graph.node()\n",
    "def add_more_to_counter(state):\n",
    "    print(\"add_more_to_counter\")\n",
    "    time.sleep(0.5)\n",
    "    return {\"counter\": 3}\n",
    "\n",
    "\n",
    "# Test History Buffer\n",
    "@test_graph.node(interrupt=\"before\")\n",
    "def add_metrics(state):\n",
    "    time.sleep(0.5)\n",
    "    print(\"add_metrics\")\n",
    "    return {\"metrics\": {\"accuracy\": 0.95, \"loss\": 0.1}}\n",
    "\n",
    "\n",
    "@test_graph.node()\n",
    "def update_metrics(state):\n",
    "    time.sleep(0.5)\n",
    "    print(\"update_metrics\")\n",
    "    return {\"metrics\": {\"loss\": 0.05, \"precision\": 0.88}}\n",
    "\n",
    "\n",
    "# Test LastValue Buffer\n",
    "@test_graph.node(interrupt=\"after\")\n",
    "def set_status_running(state):\n",
    "    time.sleep(0.5)\n",
    "    print(\"set_status_running\")\n",
    "    return {\"current_status\": \"running\"}\n",
    "\n",
    "\n",
    "@test_graph.node()\n",
    "def set_status_complete(state):\n",
    "    time.sleep(0.5)\n",
    "    print(\"set_status_complete\")\n",
    "    return {\"current_status\": \"complete\"}\n",
    "\n",
    "\n",
    "# Create the workflow\n",
    "test_graph.add_edge(START, \"add_to_counter\")\n",
    "test_graph.add_edge(\"add_to_counter\", \"add_more_to_counter\")\n",
    "test_graph.add_edge(\"add_more_to_counter\", \"add_metrics\")\n",
    "test_graph.add_edge(\"add_metrics\", \"update_metrics\")\n",
    "test_graph.add_edge(\"update_metrics\", \"set_status_running\")\n",
    "test_graph.add_edge(\"set_status_running\", \"set_status_complete\")\n",
    "test_graph.add_edge(\"set_status_complete\", END)\n",
    "\n",
    "# Compile and execute\n",
    "test_graph.compile()\n",
    "test_graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph.executed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "test_graph._convert_execution_plan()\n",
    "rprint(test_graph.execution_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = escape\n",
    "\n",
    "x.__metadata__[\"interrupt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_graph = Graph()\n",
    "\n",
    "\n",
    "# Define some example actions\n",
    "@simple_graph.node(interrupt=\"before\")\n",
    "def escape():\n",
    "    time.sleep(0.5)\n",
    "    print(\"escape\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def process_data():\n",
    "    time.sleep(1.5)\n",
    "    print(\"process_data\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def validate():\n",
    "    time.sleep(0.5)\n",
    "    print(\"validate\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def aa():\n",
    "    time.sleep(1.5)\n",
    "    print(\"aa\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def bb():\n",
    "    time.sleep(0.5)\n",
    "    print(\"bb\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def dd():\n",
    "    time.sleep(1.5)\n",
    "    print(\"dd\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def cc():\n",
    "    time.sleep(1.5)\n",
    "    print(\"cc\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def hh():\n",
    "    time.sleep(0.5)\n",
    "    print(\"hh\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def prep():\n",
    "    time.sleep(0.5)\n",
    "    print(\"prep\")\n",
    "\n",
    "\n",
    "# Add edges to create workflow\n",
    "simple_graph.add_edge(START, \"process_data\")\n",
    "simple_graph.add_edge(\"process_data\", \"validate\")\n",
    "simple_graph.add_edge(\"validate\", \"escape\")\n",
    "simple_graph.add_edge(\"escape\", \"dd\")\n",
    "simple_graph.add_edge(\"escape\", \"cc\")\n",
    "simple_graph.add_edge(\"cc\", \"hh\")\n",
    "simple_graph.add_edge(\"dd\", \"hh\")\n",
    "simple_graph.add_edge(\"hh\", \"prep\")\n",
    "simple_graph.add_edge(\"validate\", \"aa\")\n",
    "simple_graph.add_edge(\"aa\", \"bb\")\n",
    "simple_graph.add_edge(\"bb\", \"prep\")\n",
    "simple_graph.add_edge(\"prep\", END)\n",
    "\n",
    "simple_graph.compile()\n",
    "\n",
    "simple_graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "rprint(simple_graph.execution_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "rprint(simple_graph.detailed_execution_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "simple_graph._convert_execution_plan()\n",
    "rprint(simple_graph.execution_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "from rich import print as rprint\n",
    "from tiny_graph.graph.executable import ExecutableNode\n",
    "\n",
    "\n",
    "def add_item_to_obj_store(obj_store: Union[List, Tuple], item):\n",
    "    if isinstance(obj_store, list):\n",
    "        obj_store.append(item)\n",
    "        return obj_store  # Return the modified list\n",
    "    elif isinstance(obj_store, tuple):\n",
    "        return obj_store + (item,)  # Already returns the new tuple\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported object store type: {type(obj_store)}\")\n",
    "\n",
    "\n",
    "def extract_tasks_from_node(node, tasks=[]):\n",
    "    \"\"\"\n",
    "    Extracts tasks from a node, including nested nodes\n",
    "    returns lists for sequential execution and tuples for parallel execution\n",
    "    \"\"\"\n",
    "    tasks = [] if node.execution_type == \"sequential\" else tuple()\n",
    "    for task in node.task_list:\n",
    "        if isinstance(task, ExecutableNode):\n",
    "            if task.execution_type == \"sequential\":\n",
    "                tasks = add_item_to_obj_store(tasks, extract_tasks_from_node(task, []))\n",
    "            else:\n",
    "                tasks = add_item_to_obj_store(\n",
    "                    tasks, extract_tasks_from_node(task, tuple())\n",
    "                )\n",
    "        else:\n",
    "            tasks = add_item_to_obj_store(tasks, task)\n",
    "\n",
    "    return tasks\n",
    "\n",
    "\n",
    "rprint(extract_tasks_from_node(simple_graph.execution_plan[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_graph.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_graph.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_graph.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from rich import print as rprint\n",
    "from tiny_graph.buffer.factory import History, Incremental, LastValue\n",
    "from tiny_graph.graph.executable import Graph\n",
    "from tiny_graph.models.state import GraphState\n",
    "\n",
    "\n",
    "# Define a state model with buffer types\n",
    "class ComplexTestState(GraphState):\n",
    "    counter: Incremental[int]  # Will accumulate values\n",
    "    status: LastValue[str]  # Will only keep last value\n",
    "    metrics: History[Dict[str, float]]  # Will keep history of all updates\n",
    "\n",
    "\n",
    "# Initialize the graph with state\n",
    "state = ComplexTestState(counter=0, status=\"\", metrics={})\n",
    "graph = Graph(state=state)\n",
    "\n",
    "\n",
    "# Define nodes that will run in parallel and update the same state\n",
    "@graph.node()\n",
    "def increment_counter(state):\n",
    "    time.sleep(0.5)\n",
    "    print(\"increment_counter\")\n",
    "    return {\"counter\": 2}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def decrement_counter(state):\n",
    "    time.sleep(0.5)\n",
    "    print(\"decrement_counter\")\n",
    "    return {\"counter\": -1}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def update_status_to_in_progress(state):\n",
    "    time.sleep(0.5)\n",
    "    print(\"update_status_to_in_progress\")\n",
    "    return {\"status\": \"in_progress\"}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def update_status_to_complete(state):\n",
    "    time.sleep(0.5)\n",
    "    print(\"update_status_to_complete\")\n",
    "    return {\"status\": \"complete\"}\n",
    "\n",
    "\n",
    "@graph.node(interrupt=\"after\")\n",
    "def add_metrics(state):\n",
    "    time.sleep(0.5)\n",
    "    print(\"add_metrics\")\n",
    "    return {\"metrics\": {\"accuracy\": 0.9, \"loss\": 0.1}}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def update_metrics(state):\n",
    "    time.sleep(0.5)\n",
    "    print(\"update_metrics\")\n",
    "    return {\"metrics\": {\"loss\": 0.05, \"precision\": 0.85}}\n",
    "\n",
    "\n",
    "@graph.node(interrupt=\"before\")\n",
    "def finalize_metrics(state):\n",
    "    time.sleep(0.5)\n",
    "    print(\"finalize_metrics\")\n",
    "    return {\"metrics\": {\"loss\": 0.01, \"precision\": 0.99}}\n",
    "\n",
    "\n",
    "# Create the workflow with multiple levels of execution\n",
    "graph.add_edge(START, \"increment_counter\")\n",
    "graph.add_edge(START, \"decrement_counter\")\n",
    "graph.add_edge(START, \"update_status_to_in_progress\")\n",
    "graph.add_edge(\"increment_counter\", \"add_metrics\")\n",
    "graph.add_edge(\"decrement_counter\", \"add_metrics\")\n",
    "graph.add_edge(\"add_metrics\", \"update_metrics\")\n",
    "graph.add_edge(\"update_metrics\", \"finalize_metrics\")\n",
    "graph.add_edge(\"update_status_to_in_progress\", \"update_status_to_complete\")\n",
    "graph.add_edge(\"update_status_to_complete\", \"finalize_metrics\")\n",
    "graph.add_edge(\"finalize_metrics\", END)\n",
    "\n",
    "# Compile and execute\n",
    "graph.compile()\n",
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "rprint(graph.execution_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "graph._convert_execution_plan()\n",
    "rprint(graph.execution_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiny_graph.buffer.factory import History\n",
    "from tiny_graph.models.state import GraphState\n",
    "\n",
    "\n",
    "class StateForTestWithHistory(GraphState):\n",
    "    execution_order: History[str]\n",
    "\n",
    "\n",
    "state = StateForTestWithHistory(execution_order=[])\n",
    "graph = Graph(state=state)\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def task1(state):\n",
    "    return {\"execution_order\": \"task1\"}\n",
    "\n",
    "\n",
    "@graph.node(interrupt=\"after\")\n",
    "def task2(state):\n",
    "    return {\"execution_order\": \"task2\"}\n",
    "\n",
    "\n",
    "@graph.node(interrupt=\"before\")\n",
    "def task3(state):\n",
    "    return {\"execution_order\": \"task3\"}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def task4(state):\n",
    "    return {\"execution_order\": \"task4\"}\n",
    "\n",
    "\n",
    "graph.add_edge(START, \"task1\")\n",
    "graph.add_edge(\"task1\", \"task2\")\n",
    "graph.add_edge(\"task2\", \"task3\")\n",
    "graph.add_edge(\"task3\", \"task4\")\n",
    "graph.add_edge(\"task4\", END)\n",
    "graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[edge.id for edge in graph.edges if edge.end_node == \"__start__\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.detailed_execution_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Async Execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1 (testing non-async using execute_async)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from tiny_graph.buffer.factory import History, Incremental, LastValue\n",
    "from tiny_graph.models.state import GraphState\n",
    "\n",
    "\n",
    "# Define a state model with different buffer types\n",
    "class TestState(GraphState):\n",
    "    counter: Incremental[int]  # Will accumulate values\n",
    "    metrics: History[Dict[str, float]]  # Will keep history of all updates\n",
    "    current_status: LastValue[str]  # Will only keep last value\n",
    "\n",
    "\n",
    "# Initialize the graph with state\n",
    "state = TestState(counter=0, metrics=[], current_status=\"\")\n",
    "\n",
    "test_graph = Graph(state=state)\n",
    "\n",
    "\n",
    "# Test Incremental Buffer\n",
    "@test_graph.node()\n",
    "def add_to_counter(state):\n",
    "    time.sleep(0.5)\n",
    "    print(\"add_to_counter\")\n",
    "    return {\"counter\": 5}\n",
    "\n",
    "\n",
    "@test_graph.node()\n",
    "def add_more_to_counter(state):\n",
    "    print(\"add_more_to_counter\")\n",
    "    time.sleep(0.5)\n",
    "    return {\"counter\": 3}\n",
    "\n",
    "\n",
    "# Test History Buffer\n",
    "@test_graph.node(interrupt=\"before\")\n",
    "def add_metrics(state):\n",
    "    time.sleep(0.5)\n",
    "    print(\"add_metrics\")\n",
    "    return {\"metrics\": {\"accuracy\": 0.95, \"loss\": 0.1}}\n",
    "\n",
    "\n",
    "@test_graph.node()\n",
    "def update_metrics(state):\n",
    "    time.sleep(0.5)\n",
    "    print(\"update_metrics\")\n",
    "    return {\"metrics\": {\"loss\": 0.05, \"precision\": 0.88}}\n",
    "\n",
    "\n",
    "# Test LastValue Buffer\n",
    "@test_graph.node(interrupt=\"after\")\n",
    "def set_status_running(state):\n",
    "    time.sleep(0.5)\n",
    "    print(\"set_status_running\")\n",
    "    return {\"current_status\": \"running\"}\n",
    "\n",
    "\n",
    "@test_graph.node()\n",
    "def set_status_complete(state):\n",
    "    time.sleep(0.5)\n",
    "    print(\"set_status_complete\")\n",
    "    return {\"current_status\": \"complete\"}\n",
    "\n",
    "\n",
    "# Create the workflow\n",
    "test_graph.add_edge(START, \"add_to_counter\")\n",
    "test_graph.add_edge(\"add_to_counter\", \"add_more_to_counter\")\n",
    "test_graph.add_edge(\"add_more_to_counter\", \"add_metrics\")\n",
    "test_graph.add_edge(\"add_metrics\", \"update_metrics\")\n",
    "test_graph.add_edge(\"update_metrics\", \"set_status_running\")\n",
    "test_graph.add_edge(\"set_status_running\", \"set_status_complete\")\n",
    "test_graph.add_edge(\"set_status_complete\", END)\n",
    "\n",
    "# Compile and execute\n",
    "test_graph.compile()\n",
    "test_graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await test_graph.execute_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph.executed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "test_graph._convert_execution_plan()\n",
    "rprint(test_graph.execution_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_graph = Graph()\n",
    "\n",
    "\n",
    "# Define some example actions\n",
    "@simple_graph.node()\n",
    "def step_1():\n",
    "    time.sleep(1)\n",
    "    print(\"start\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def step_A():\n",
    "    time.sleep(2)\n",
    "    print(\"step_A\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def step_B():\n",
    "    time.sleep(2)\n",
    "    print(\"step_B\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "def step_2():\n",
    "    time.sleep(1)\n",
    "    print(\"step_2\")\n",
    "\n",
    "\n",
    "# Add edges to create workflow\n",
    "simple_graph.add_edge(START, \"step_1\")\n",
    "simple_graph.add_edge(\"step_1\", \"step_A\")\n",
    "simple_graph.add_edge(\"step_1\", \"step_B\")\n",
    "simple_graph.add_edge(\"step_A\", \"step_2\")\n",
    "simple_graph.add_edge(\"step_B\", \"step_2\")\n",
    "simple_graph.add_edge(\"step_2\", END)\n",
    "\n",
    "simple_graph.compile()\n",
    "\n",
    "simple_graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_graph.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await simple_graph.execute_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_graph.execution_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "simple_graph = Graph()\n",
    "\n",
    "\n",
    "# Define some example actions\n",
    "@simple_graph.node()\n",
    "async def step_1():\n",
    "    await asyncio.sleep(1)\n",
    "    print(\"start\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "async def step_A():\n",
    "    await asyncio.sleep(2)\n",
    "    print(\"step_A\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "async def step_B():\n",
    "    await asyncio.sleep(2)\n",
    "    print(\"step_B\")\n",
    "\n",
    "\n",
    "@simple_graph.node()\n",
    "async def step_2():\n",
    "    await asyncio.sleep(1)\n",
    "    print(\"step_2\")\n",
    "\n",
    "\n",
    "# Add edges to create workflow\n",
    "simple_graph.add_edge(START, \"step_1\")\n",
    "simple_graph.add_edge(\"step_1\", \"step_A\")\n",
    "simple_graph.add_edge(\"step_1\", \"step_B\")\n",
    "simple_graph.add_edge(\"step_A\", \"step_2\")\n",
    "simple_graph.add_edge(\"step_B\", \"step_2\")\n",
    "simple_graph.add_edge(\"step_2\", END)\n",
    "\n",
    "simple_graph.compile()\n",
    "\n",
    "simple_graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await simple_graph.execute_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiny_graph.buffer.factory import History\n",
    "from tiny_graph.models.state import GraphState\n",
    "\n",
    "\n",
    "class StateForTestWithHistory(GraphState):\n",
    "    execution_order: History[str]\n",
    "\n",
    "\n",
    "state = StateForTestWithHistory(execution_order=[])\n",
    "graph = Graph(state=state)\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "async def task1(state):\n",
    "    print(\"task1\")\n",
    "    return {\"execution_order\": \"task1\"}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "async def task2(state):\n",
    "    print(\"task2\")\n",
    "    return {\"execution_order\": \"task2\"}\n",
    "\n",
    "\n",
    "@graph.node(interrupt=\"before\")\n",
    "async def task3(state):\n",
    "    print(\"task3\")\n",
    "    return {\"execution_order\": \"task3\"}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "async def task4(state):\n",
    "    print(\"task4\")\n",
    "    return {\"execution_order\": \"task4\"}\n",
    "\n",
    "\n",
    "# Create parallel paths\n",
    "graph.add_edge(START, \"task1\")\n",
    "graph.add_edge(\"task1\", \"task2\")\n",
    "graph.add_edge(\"task1\", \"task3\")\n",
    "graph.add_edge(\"task2\", \"task4\")\n",
    "graph.add_edge(\"task3\", \"task4\")\n",
    "graph.add_edge(\"task4\", END)\n",
    "graph.compile()\n",
    "\n",
    "# First execution should execute task1 and task3, but pause before task2\n",
    "# await graph.start_async()\n",
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await graph.resume_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await graph.resume_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.state.execution_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiny_graph.buffer.factory import History, Incremental, LastValue\n",
    "from tiny_graph.models.state import GraphState\n",
    "\n",
    "\n",
    "class StateForTestWithHistory(GraphState):\n",
    "    execution_order: History[str]\n",
    "    counter: Incremental[int]\n",
    "\n",
    "\n",
    "StateForTestWithHistory(execution_order=[\"a\", 2], counter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "class ComplexTestState(GraphState):\n",
    "    counter: Incremental[int]  # Will accumulate values\n",
    "    status: LastValue[str]  # Will only keep last value\n",
    "    metrics: History[Dict[str, float]]  # Will keep history of all updates\n",
    "\n",
    "\n",
    "ComplexTestState(counter=0, status=\"\", metrics=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiny_graph.checkpoint.local_storage import LocalStorage\n",
    "\n",
    "\n",
    "class SimpleGraphState(GraphState):\n",
    "    messages: History[str]\n",
    "\n",
    "\n",
    "# Create state instance\n",
    "state = SimpleGraphState(messages=[])\n",
    "\n",
    "# Update graph with state\n",
    "storage = LocalStorage()\n",
    "graph1 = Graph(state=state, checkpoint_storage=storage)\n",
    "\n",
    "\n",
    "@graph1.node()\n",
    "def add_hello(state: GraphState):\n",
    "    return {\"messages\": \"Hello\"}\n",
    "\n",
    "\n",
    "@graph1.node()\n",
    "def add_world(state: GraphState):\n",
    "    return {\"messages\": \"World\"}\n",
    "\n",
    "\n",
    "@graph1.node()\n",
    "def add_exclamation(state: GraphState):\n",
    "    return {\"messages\": \"!\"}\n",
    "\n",
    "\n",
    "# Add edges\n",
    "graph1.add_edge(START, \"add_hello\")\n",
    "graph1.add_edge(\"add_hello\", \"add_world\")\n",
    "graph1.add_edge(\"add_world\", \"add_exclamation\")\n",
    "graph1.add_edge(\"add_exclamation\", END)\n",
    "\n",
    "# Add nodes and edges...\n",
    "graph1.compile()\n",
    "graph1.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await graph1.start_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage._storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Subgraph Execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_graph = Graph()\n",
    "\n",
    "\n",
    "@main_graph.node()\n",
    "def start_process():\n",
    "    print(\"Starting main process\")\n",
    "\n",
    "\n",
    "@main_graph.node()\n",
    "def end_process():\n",
    "    print(\"Ending main process\")\n",
    "\n",
    "\n",
    "# Create subgraph\n",
    "sub_graph = Graph()\n",
    "\n",
    "\n",
    "@sub_graph.node()\n",
    "def sub_task1():\n",
    "    print(\"Subtask 1\")\n",
    "\n",
    "\n",
    "@sub_graph.node()\n",
    "def sub_task2():\n",
    "    print(\"Subtask 2\")\n",
    "\n",
    "\n",
    "# Add edges to subgraph\n",
    "sub_graph.add_edge(START, \"sub_task1\")\n",
    "sub_graph.add_edge(\"sub_task1\", \"sub_task2\")\n",
    "sub_graph.add_edge(\"sub_task2\", END)\n",
    "\n",
    "\n",
    "# Add subgraph as a node to main graph\n",
    "@main_graph.subgraph(name=\"processing\")\n",
    "def processing_subgraph():\n",
    "    return sub_graph\n",
    "\n",
    "\n",
    "# Add edges to main graph including the subgraph\n",
    "main_graph.add_edge(START, \"start_process\")\n",
    "main_graph.add_edge(\"start_process\", \"processing\")  # Connect to subgraph\n",
    "main_graph.add_edge(\"processing\", \"end_process\")  # Connect from subgraph\n",
    "main_graph.add_edge(\"end_process\", END)\n",
    "\n",
    "# Compile and visualize\n",
    "main_graph.compile()\n",
    "main_graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_graph.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmain_graph.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubgraphState(GraphState):\n",
    "    execution_order: History[str]\n",
    "    counter: Incremental[int]\n",
    "    status: LastValue[str]\n",
    "\n",
    "\n",
    "state = SubgraphState(execution_order=[], counter=0, status=\"\")\n",
    "main_graph = Graph(state=state)\n",
    "\n",
    "\n",
    "# Create a subgraph\n",
    "@main_graph.subgraph()\n",
    "def processing_subgraph():\n",
    "    subgraph = Graph(state=state)\n",
    "\n",
    "    @subgraph.node()\n",
    "    def process_a(state):\n",
    "        return {\"execution_order\": \"process_a\", \"counter\": 1}\n",
    "\n",
    "    @subgraph.node()\n",
    "    def process_b(state):\n",
    "        return {\"execution_order\": \"process_b\", \"counter\": 2}\n",
    "\n",
    "    subgraph.add_edge(START, \"process_a\")\n",
    "    subgraph.add_edge(\"process_a\", \"process_b\")\n",
    "    subgraph.add_edge(\"process_b\", END)\n",
    "\n",
    "    return subgraph\n",
    "\n",
    "\n",
    "# Main graph nodes\n",
    "@main_graph.node()\n",
    "def start_task(state):\n",
    "    return {\"execution_order\": \"start_task\", \"status\": \"started\"}\n",
    "\n",
    "\n",
    "@main_graph.node()\n",
    "def end_task(state):\n",
    "    return {\"execution_order\": \"end_task\", \"status\": \"completed\"}\n",
    "\n",
    "\n",
    "# Connect main graph\n",
    "main_graph.add_edge(START, \"start_task\")\n",
    "main_graph.add_edge(\"start_task\", \"processing_subgraph\")\n",
    "main_graph.add_edge(\"processing_subgraph\", \"end_task\")\n",
    "main_graph.add_edge(\"end_task\", END)\n",
    "\n",
    "main_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_graph.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubgraphState(GraphState):\n",
    "    execution_order: History[str]\n",
    "    counter: Incremental[int]\n",
    "    status: LastValue[str]\n",
    "\n",
    "\n",
    "state = SubgraphState(execution_order=[], counter=0, status=\"\")\n",
    "main_graph = Graph(state=state)\n",
    "\n",
    "\n",
    "def nested_subgraph():\n",
    "    nested = Graph(state=state)\n",
    "\n",
    "    @nested.node()\n",
    "    def nested_task(state):\n",
    "        return {\"execution_order\": \"nested_task\", \"counter\": 1}\n",
    "\n",
    "    @nested.node()\n",
    "    def nested_task_2(state):\n",
    "        return {\"execution_order\": \"nested_task_2\", \"counter\": 2}\n",
    "\n",
    "    nested.add_edge(START, \"nested_task\")\n",
    "    nested.add_edge(\"nested_task\", \"nested_task_2\")\n",
    "    nested.add_edge(\"nested_task_2\", END)\n",
    "    return nested\n",
    "\n",
    "\n",
    "# Create parent subgraph containing nested subgraph\n",
    "@main_graph.subgraph()\n",
    "def parent_subgraph():\n",
    "    parent = Graph(state=state)\n",
    "\n",
    "    @parent.node()\n",
    "    def parent_task(state):\n",
    "        return {\"execution_order\": \"parent_task\", \"counter\": 2}\n",
    "\n",
    "    @parent.subgraph()\n",
    "    def inner_nested():\n",
    "        return nested_subgraph()\n",
    "\n",
    "    parent.add_edge(START, \"parent_task\")\n",
    "    parent.add_edge(\"parent_task\", \"inner_nested\")\n",
    "    parent.add_edge(\"inner_nested\", END)\n",
    "    return parent\n",
    "\n",
    "\n",
    "# Main graph setup\n",
    "@main_graph.node()\n",
    "def main_task(state):\n",
    "    return {\"execution_order\": \"main_task\", \"status\": \"running\"}\n",
    "\n",
    "\n",
    "main_graph.add_edge(START, \"main_task\")\n",
    "main_graph.add_edge(\"main_task\", \"parent_subgraph\")\n",
    "main_graph.add_edge(\"parent_subgraph\", END)\n",
    "\n",
    "main_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_graph.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_graph.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Repeated Nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph()\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def node_a():\n",
    "    print(\"Node A\")\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def node_b():\n",
    "    print(\"Node B\\n\")\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def node_c():\n",
    "    print(\"Node C\")\n",
    "\n",
    "\n",
    "# Add a repeat edge that runs node_b 3 times sequentially\n",
    "graph.add_edge(START, \"node_a\")\n",
    "graph.add_repeating_edge(\"node_a\", \"node_b\", \"node_c\", repeat=3, parallel=False)\n",
    "graph.add_edge(\"node_c\", END)\n",
    "\n",
    "# Or run in parallel\n",
    "# graph.add_repeat_edge(\"node_a\", \"node_b\", \"node_c\", repeat=3, parallel=True)\n",
    "\n",
    "graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.executed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.detailed_execution_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2 (async)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateWithHistory(GraphState):\n",
    "    execution_order: History[str]\n",
    "    execution_times: History[float]\n",
    "    counter: Incremental[int]\n",
    "\n",
    "\n",
    "state = StateWithHistory(execution_order=[], execution_times=[], counter=0)\n",
    "graph = Graph(state=state)\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "async def start_task(state):\n",
    "    return {}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "async def cpu_intensive_task(state):\n",
    "    # Simulate CPU-intensive work\n",
    "    await asyncio.sleep(0.1)\n",
    "    print(\"cpu_intensive_task_parallel\")\n",
    "    result = 0\n",
    "    for _ in range(1000000):\n",
    "        result += 1\n",
    "    current_time = time.time()\n",
    "    return {\n",
    "        \"execution_order\": f\"task_{len(state.execution_order)}\",\n",
    "        \"execution_times\": current_time,\n",
    "        \"counter\": 1,\n",
    "    }\n",
    "\n",
    "\n",
    "# Create parallel execution with many repetitions\n",
    "graph.add_edge(START, \"start_task\")\n",
    "graph.add_repeating_edge(\n",
    "    \"start_task\", \"cpu_intensive_task\", END, repeat=10, parallel=True\n",
    ")\n",
    "graph.compile()\n",
    "\n",
    "start_time = time.time()\n",
    "await graph.start_async()\n",
    "parallel_time = time.time() - start_time\n",
    "print(f\"Parallel execution time: {parallel_time}\")\n",
    "\n",
    "# Now test sequential execution\n",
    "graph2 = Graph(\n",
    "    state=StateWithHistory(execution_order=[], execution_times=[], counter=0)\n",
    ")\n",
    "\n",
    "\n",
    "@graph2.node()\n",
    "async def start_task(state):\n",
    "    return {}\n",
    "\n",
    "\n",
    "@graph2.node()\n",
    "async def cpu_intensive_task(state):\n",
    "    # Same task as above\n",
    "    await asyncio.sleep(0.1)\n",
    "    print(\"cpu_intensive_task_sequential\")\n",
    "    result = 0\n",
    "    for _ in range(1000000):\n",
    "        result += 1\n",
    "    current_time = time.time()\n",
    "    return {\n",
    "        \"execution_order\": f\"task_{len(state.execution_order)}\",\n",
    "        \"execution_times\": current_time,\n",
    "        \"counter\": 1,\n",
    "    }\n",
    "\n",
    "\n",
    "graph2.add_edge(START, \"start_task\")\n",
    "graph2.add_repeating_edge(\n",
    "    \"start_task\", \"cpu_intensive_task\", END, repeat=10, parallel=False\n",
    ")\n",
    "graph2.compile()\n",
    "\n",
    "start_time = time.time()\n",
    "await graph2.start_async()\n",
    "sequential_time = time.time() - start_time\n",
    "\n",
    "# Parallel execution should be significantly faster\n",
    "assert parallel_time < sequential_time * 0.7  # At least 30% faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_time, sequential_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "rprint(graph.execution_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(graph.detailed_execution_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test - name assign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph()\n",
    "\n",
    "@graph.node()\n",
    "def node_a():\n",
    "    print(\"Node A\")\n",
    "    return {}\n",
    "    \n",
    "\n",
    "@graph.node()\n",
    "def node_b():\n",
    "    print(\"Node B\\n\")\n",
    "    return {}\n",
    "\n",
    "@graph.node()\n",
    "def node_c():\n",
    "    print(\"Node C\")\n",
    "    return {}\n",
    "\n",
    "# Add a repeat edge that runs node_b 3 times sequentially,\n",
    "# with custom names for the repeated nodes.\n",
    "graph.add_edge(START, \"node_a\")\n",
    "graph.add_repeating_edge(\n",
    "    \"node_a\",\n",
    "    \"node_b\",\n",
    "    \"node_c\",\n",
    "    repeat=3,\n",
    "    parallel=False,\n",
    "    repeat_names=[\"node_b_1\", \"node_b_2\", \"node_b_3\"]\n",
    ")\n",
    "graph.add_edge(\"node_c\", END)\n",
    "\n",
    "graph.compile()\n",
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#version using state to route parallel execution\n",
    "from typing import Dict\n",
    "import sys\n",
    "\n",
    "class ExecutionState(GraphState):\n",
    "        route_execution: LastValue[Dict[str, str]]\n",
    "\n",
    "state = ExecutionState(route_execution={'node_a': 'node_a', 'node_b_1': 'node_b1', 'node_b_2': 'node_b2', 'node_b_3': 'node_b3', 'node_c': 'node_c'})\n",
    "\n",
    "graph = Graph()\n",
    "\n",
    "@graph.node()\n",
    "def node_a(self):\n",
    "    current_name = sys._getframe().f_code.co_name\n",
    "    print(f\"Executing {current_name}\")\n",
    "    print(f\"Metadata {self.name}\")\n",
    "    my_task = state.route_execution[self.name]\n",
    "    print(f\"My task: {my_task}\")\n",
    "    return {}\n",
    "    \n",
    "\n",
    "@graph.node()\n",
    "def node_b(self):\n",
    "    current_name = sys._getframe().f_code.co_name\n",
    "    print(f\"Executing {current_name}\")\n",
    "    print(f\"Metadata {self.name}\")\n",
    "    my_task = state.route_execution[self.name]\n",
    "    print(f\"My task: {my_task}\")\n",
    "    return {}\n",
    "\n",
    "@graph.node()\n",
    "def node_c(self):\n",
    "    current_name = sys._getframe().f_code.co_name\n",
    "    print(f\"Executing {current_name}\")\n",
    "    print(f\"Metadata {self.name}\")\n",
    "    my_task = state.route_execution[self.name]\n",
    "    print(f\"My task: {my_task}\")\n",
    "    return {}\n",
    "\n",
    "# Add a repeat edge that runs node_b 3 times sequentially,\n",
    "# with custom names for the repeated nodes.\n",
    "graph.add_edge(START, \"node_a\")\n",
    "graph.add_repeating_edge(\n",
    "    \"node_a\",\n",
    "    \"node_b\",\n",
    "    \"node_c\",\n",
    "    repeat=3,\n",
    "    parallel=True,\n",
    "    repeat_names=[\"node_b_1\", \"node_b_2\", \"node_b_3\"]\n",
    ")\n",
    "graph.add_edge(\"node_c\", END)\n",
    "\n",
    "graph.compile()\n",
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await graph.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Router Nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primeGraph.checkpoint.local_storage import LocalStorage\n",
    "\n",
    "\n",
    "# Example usage\n",
    "class TestState(GraphState):\n",
    "    result: LastValue[dict]  # Store the result from routes\n",
    "    execution_order: History[str]  # Track execution order\n",
    "\n",
    "\n",
    "storage = LocalStorage()\n",
    "graph = Graph(\n",
    "    state=TestState(result={}, execution_order=[]), checkpoint_storage=storage\n",
    ")\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def process_data(state):\n",
    "    print(\"Executing process_data\")\n",
    "    # Router node that returns a node name\n",
    "    if True:\n",
    "        return \"route_a\"\n",
    "    return \"route_b\"\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def route_a(state):\n",
    "    print(\"Executing route_a\")\n",
    "    return {\"result\": {\"result\": \"from route A\"}, \"execution_order\": \"route_a\"}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def route_b(state):\n",
    "    print(\"Executing route_b\")\n",
    "    return {\"result\": {\"result\": \"from route B\"}, \"execution_order\": \"route_b\"}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def route_a2(state):\n",
    "    print(\"Executing route_a2\")\n",
    "    return {\"result\": {\"result\": \"from route A2\"}, \"execution_order\": \"route_a2\"}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def route_b2(state):\n",
    "    print(\"Executing route_b2\")\n",
    "    return {\"result\": {\"result\": \"from route B2\"}, \"execution_order\": \"route_b2\"}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def route_c(state):\n",
    "    print(\"Executing route_c\")\n",
    "    return {\"result\": {\"result\": \"from route C\"}, \"execution_order\": \"route_c\"}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def route_d(state):\n",
    "    print(\"Executing route_d\")\n",
    "    return {\"result\": {\"result\": \"from route D\"}, \"execution_order\": \"route_d\"}\n",
    "\n",
    "\n",
    "# Add edges\n",
    "# graph.add_edge(START, \"process_data\")\n",
    "graph.add_router_edge(START, \"process_data\")  # No need to specify routes\n",
    "graph.add_edge(\"route_a\", \"route_a2\")\n",
    "graph.add_edge(\"route_b\", \"route_b2\")\n",
    "graph.add_edge(\"route_a2\", \"route_c\")\n",
    "graph.add_edge(\"route_b2\", \"route_c\")\n",
    "graph.add_edge(\"route_c\", \"route_d\")\n",
    "graph.add_edge(\"route_d\", END)\n",
    "\n",
    "# Execution will automatically handle routing\n",
    "graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.router_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.start()  # Will pause after process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.execution_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "rprint(graph.detailed_execution_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "rprint(graph.detailed_execution_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(graph.execution_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph._update_execution_plan(\n",
    "    \"process_data\", graph.router_paths[\"process_data\"][\"route_a\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.router_paths[\"process_data\"][\"route_a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.detailed_execution_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "class TestState(GraphState):\n",
    "    result: LastValue[dict]  # Store the result from routes\n",
    "    execution_order: History[str]  # Track execution order\n",
    "\n",
    "\n",
    "graph = Graph(state=TestState(result={}, execution_order=[]))\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def process_data(state):\n",
    "    print(\"Executing process_data\")\n",
    "    # Router node that returns a node name\n",
    "    if True:\n",
    "        return \"route_b\"\n",
    "    return \"route_a\"\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def route_a(state):\n",
    "    print(\"Executing route_a\")\n",
    "    return {\"result\": {\"result\": \"from route A\"}, \"execution_order\": \"route_a\"}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def route_b(state):\n",
    "    print(\"Executing route_b\")\n",
    "    return {\"result\": {\"result\": \"from route B\"}, \"execution_order\": \"route_b\"}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def route_a2(state):\n",
    "    print(\"Executing route_a2\")\n",
    "    return {\"result\": {\"result\": \"from route A2\"}, \"execution_order\": \"route_a2\"}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def route_b2(state):\n",
    "    print(\"Executing route_b2\")\n",
    "    if True:\n",
    "        return \"route_c\"\n",
    "    return \"route_d\"\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def route_c(state):\n",
    "    print(\"Executing route_c\")\n",
    "    return {\"result\": {\"result\": \"from route C\"}, \"execution_order\": \"route_c\"}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def route_d(state):\n",
    "    print(\"Executing route_d\")\n",
    "    return {\"result\": {\"result\": \"from route D\"}, \"execution_order\": \"route_d\"}\n",
    "\n",
    "\n",
    "# Add edges\n",
    "# graph.add_edge(START, \"process_data\")\n",
    "graph.add_router_edge(START, \"process_data\")  # No need to specify routes\n",
    "graph.add_edge(\"route_a\", \"route_a2\")\n",
    "# graph.add_edge(\"route_b\", \"route_b2\")\n",
    "graph.add_edge(\"route_a2\", \"route_c\")\n",
    "graph.add_router_edge(\"route_b\", \"route_b2\")\n",
    "graph.add_edge(\"route_c\", \"route_d\")\n",
    "graph.add_edge(\"route_d\", END)\n",
    "\n",
    "# Execution will automatically handle routing\n",
    "graph.compile()\n",
    "graph.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.router_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.execution_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.detailed_execution_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.detailed_execution_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "rprint(graph.execution_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "class TestState(GraphState):\n",
    "    result: LastValue[dict]  # Store the result from routes\n",
    "    execution_order: History[str]  # Track execution order\n",
    "\n",
    "\n",
    "graph = Graph(state=TestState(result={}, execution_order=[]))\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def route_a(state):\n",
    "    print(\"Executing route_a\")\n",
    "    return {\"result\": {\"result\": \"from route A\"}, \"execution_order\": \"route_a\"}\n",
    "\n",
    "\n",
    "@graph.node(interrupt=\"after\")\n",
    "def route_b(state):\n",
    "    print(\"Executing route_b\")\n",
    "    return {\"result\": {\"result\": \"from route B\"}, \"execution_order\": \"route_b\"}\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def route_c(state):\n",
    "    print(\"Executing route_c\")\n",
    "    if True:\n",
    "        return \"route_b\"\n",
    "    return \"route_d\"\n",
    "\n",
    "\n",
    "@graph.node()\n",
    "def route_d(state):\n",
    "    print(\"Executing route_d\")\n",
    "    return {\"result\": {\"result\": \"from route D\"}, \"execution_order\": \"route_d\"}\n",
    "\n",
    "\n",
    "# Add edges\n",
    "# graph.add_edge(START, \"process_data\")\n",
    "graph.add_edge(START, \"route_a\")  # No need to specify routes\n",
    "graph.add_edge(\"route_a\", \"route_b\")\n",
    "graph.add_router_edge(\"route_b\", \"route_c\")\n",
    "graph.add_edge(\"route_d\", END)\n",
    "\n",
    "# Execution will automatically handle routing\n",
    "graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.executed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.next_execution_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.blocking_execution_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.execution_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.execution_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.execution_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Real World Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cyclical planner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from primeGraph import Graph, START, END\n",
    "from primeGraph.models import GraphState\n",
    "from primeGraph.buffer import History, LastValue\n",
    "from rich import print as rprint\n",
    "from typing import Dict, Union, List, Any\n",
    "\n",
    "\n",
    "\n",
    "class State(GraphState):\n",
    "    conversation: History[Dict[str, str]]\n",
    "    model_message: LastValue[Union[str, None]] # Should be consumed by the bot before moving forward interruptions\n",
    "    user_message: LastValue[Union[str, None]] # Should be consumed by the bot before moving forward interruptions\n",
    "    is_followup: LastValue[Union[None, bool]]\n",
    "    is_forward_permission: LastValue[Union[bool, None]]\n",
    "    is_permission_granted: LastValue[Union[bool, None]]\n",
    "    is_outside_of_step: LastValue[Union[bool, None]]\n",
    "    plan_goal: LastValue[str]\n",
    "    plan_summary: LastValue[str]\n",
    "    plan_details: History[str]\n",
    "    is_ready_to_move_forward: LastValue[bool]\n",
    "    is_summary_approved: LastValue[bool]\n",
    "    interaction_summary: LastValue[str]\n",
    "    \n",
    "\n",
    "def generate_plan_graph(graph_state: State):\n",
    "    \n",
    "    plan_graph = Graph(graph_state, verbose=False)\n",
    "    \n",
    "\n",
    "    @plan_graph.node(interrupt=\"after\")\n",
    "    async def intitialize_conversation(state: State) -> dict[str, Any]:\n",
    "        print(\"Initializing conversation...\")\n",
    "        response = \"Welcome! Please tell me your goal in a single sentence.\"\n",
    "    \n",
    "        \n",
    "        return {\n",
    "            \"conversation\": {\"role\": \"assistant\", \"content\": response},\n",
    "            \"user_message\": None,\n",
    "            \"model_message\": response,\n",
    "        }\n",
    "\n",
    "    @plan_graph.node()\n",
    "    async def process_user_message(state: State) -> dict[str, Any]:\n",
    "        print(\"Processing user message...\")\n",
    "        \n",
    "        # Simulated response\n",
    "        response = \"I understand your goal. Let me ask a follow-up question.\"\n",
    "        plan_goal = \"Sample goal\"\n",
    "        plan_details = [\"Detail 1\", \"Detail 2\"]\n",
    "        \n",
    "        state.conversation.append({\"role\": \"user\", \"content\": state.user_message})\n",
    "        \n",
    "        # Add sample plan details\n",
    "        for detail in plan_details:\n",
    "            state.plan_details.append(detail)\n",
    "\n",
    "        return {\n",
    "            \"conversation\": {\"role\": \"assistant\", \"content\": response},\n",
    "            \"plan_goal\": plan_goal,\n",
    "            \"user_message\": None,\n",
    "            \"model_message\": response,\n",
    "            \"is_followup\": True,\n",
    "            \"is_forward_permission\": False,\n",
    "            \"is_outside_of_step\": False,\n",
    "        }\n",
    "    \n",
    "    @plan_graph.node()\n",
    "    async def response_router(state: State) -> str:\n",
    "        print(\"Routing response...\")\n",
    "        if state.is_followup:\n",
    "            return 'make_followup_questions'\n",
    "        elif state.is_forward_permission:\n",
    "            return 'summarize_and_ask_permission'\n",
    "        return END\n",
    "\n",
    "    @plan_graph.node(interrupt=\"before\")\n",
    "    async def make_followup_questions(state: State):\n",
    "        print(\"Making follow-up questions...\")\n",
    "        response = \"Here's a follow-up question for clarification.\"\n",
    "        plan_details = [\"Follow-up detail 1\", \"Follow-up detail 2\"]\n",
    "        \n",
    "        state.conversation.append({\"role\": \"user\", \"content\": state.user_message})\n",
    "        \n",
    "        for detail in plan_details:\n",
    "            state.plan_details.append(detail)\n",
    "\n",
    "        return {\n",
    "            \"conversation\": {\"role\": \"assistant\", \"content\": response},\n",
    "            \"user_message\": None,\n",
    "            \"model_message\": response,\n",
    "            \"is_followup\": True,\n",
    "            \"is_forward_permission\": False,\n",
    "            \"is_outside_of_step\": False,\n",
    "        }\n",
    "    \n",
    "    @plan_graph.node()\n",
    "    async def check_followup_next_step(state: State):\n",
    "        print(\"Checking follow-up next step...\")\n",
    "        if state.is_followup:\n",
    "            return 'make_followup_questions'\n",
    "        return 'summarize_and_ask_permission'\n",
    "\n",
    "    @plan_graph.node(interrupt=\"before\")\n",
    "    async def summarize_and_ask_permission(state: State):\n",
    "        print(\"Summarizing and asking for permission...\")\n",
    "        response = \"Here's a summary of our discussion. Shall we move forward?\"\n",
    "        \n",
    "        state.conversation.append({\"role\": \"user\", \"content\": state.user_message})\n",
    "\n",
    "        return {\n",
    "            \"conversation\": {\"role\": \"assistant\", \"content\": response},\n",
    "            \"user_message\": None,\n",
    "            \"model_message\": response,\n",
    "            \"is_followup\": False,\n",
    "            \"is_permission_granted\": True,\n",
    "            \"is_outside_of_step\": False,\n",
    "        }\n",
    "    \n",
    "    @plan_graph.node()\n",
    "    async def move_to_next_step(state: State):\n",
    "        print(\"Moving to next step...\")\n",
    "        if state.is_permission_granted:\n",
    "            return END\n",
    "        return 'process_user_message'\n",
    "\n",
    "\n",
    "    plan_graph.add_edge(START, 'intitialize_conversation')\n",
    "    plan_graph.add_edge('intitialize_conversation', 'process_user_message')\n",
    "    plan_graph.add_router_edge('process_user_message', 'response_router')\n",
    "    plan_graph.add_router_edge('make_followup_questions', 'check_followup_next_step')\n",
    "\n",
    "    plan_graph.add_router_edge(\"summarize_and_ask_permission\", \"move_to_next_step\")\n",
    "    \n",
    "    \n",
    "    plan_graph.compile()\n",
    "\n",
    "    return plan_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_graph_state = State(conversation=[], \n",
    "                         model_message=None, \n",
    "                         user_message=None,\n",
    "                         next_interaction={\"worflow_step\": \"user_prompt\"}, \n",
    "                         plan_goal=\"\", \n",
    "                         plan_summary=\"\", \n",
    "                         plan_details=[], \n",
    "                         is_summary_approved=False, \n",
    "                         interaction_summary=\"\", \n",
    "                         is_followup=False,\n",
    "                         is_forward_permission=False,\n",
    "                         is_permission_granted=False,\n",
    "                         is_outside_of_step=False,\n",
    "                         is_ready_to_move_forward=False)\n",
    "\n",
    "graph = generate_plan_graph(plan_graph_state)\n",
    "\n",
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await graph.start_async()\n",
    "rprint(plan_graph_state)\n",
    "rprint(graph.chain_status)\n",
    "rprint(graph.next_execution_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.update_state_and_checkpoint({\"user_message\": \"I to plan a wedding party\"})\n",
    "await graph.resume_async()\n",
    "rprint(plan_graph_state)\n",
    "rprint(graph.chain_status)\n",
    "rprint(graph.next_execution_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.update_state_and_checkpoint({\"user_message\": \"I to plan a wedding party\"})\n",
    "await graph.resume_async()\n",
    "rprint(plan_graph_state)\n",
    "rprint(graph.chain_status)\n",
    "rprint(graph.next_execution_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb\n",
    "\n",
    "graph.update_state_and_checkpoint({\"user_message\": \"I to plan a wedding party\"})\n",
    "await graph.resume_async()\n",
    "rprint(plan_graph_state)\n",
    "rprint(graph.chain_status)\n",
    "rprint(graph.next_execution_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.start_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.start_from = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(graph.execution_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.is_cyclical_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.executed_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cyclical planner - storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from primeGraph import Graph, START, END\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from primeGraph.models import GraphState\n",
    "from primeGraph.buffer import History, LastValue\n",
    "from rich import print as rprint\n",
    "from typing import Dict, Union, List\n",
    "from primeGraph.checkpoint.postgresql import PostgreSQLStorage\n",
    "from rich import print as rprint\n",
    "\n",
    "\n",
    "sys_prompt_0 = \"\"\"\n",
    "Give the user a welcome message and ask them to state their goal in a single sentence.\n",
    "\"\"\"\n",
    "\n",
    "sys_prompt_1 = \"\"\"\n",
    "You are in a planning workflow in this is the first step. Execute all the instructions below exactly as they are.\n",
    "\n",
    "Phase 1: Goal Understanding\n",
    "\n",
    "Ask the user to state their goal in a single sentence\n",
    "Have them specify what success looks like for this goal\n",
    "Determine if there are any absolute constraints or requirements\n",
    "\n",
    "If the goal is clear, just say \"Ok, I got it\"\n",
    "\n",
    "Guidelines Throughout Process\n",
    "DO:\n",
    "\n",
    "Keep questions focused on structural elements\n",
    "Maintain high-level perspective\n",
    "Validate understanding frequently\n",
    "Document assumptions made\n",
    "\n",
    "DON'T:\n",
    "\n",
    "Ask about specific implementation steps\n",
    "Get caught up in technical details\n",
    "Try to solve problems at this stage\n",
    "Make decisions about execution methods\n",
    "\n",
    "Red Flags to Watch For\n",
    "\n",
    "Discussion becoming too detailed\n",
    "Questions about \"how\" instead of \"what\"\n",
    "Focus shifting to specific solutions\n",
    "Getting stuck on implementation challenges\n",
    "\n",
    "Remember: The goal is to create a clear framework for the plan, not to determine how each piece will be executed.\n",
    "\"\"\"\n",
    "\n",
    "sys_prompt_2 = \"\"\"\n",
    "You are in a planning workflow in this is the first step. Execute all the instructions below exactly as they are.\n",
    "\n",
    "    Phase 2: Information Gathering\n",
    "\n",
    "Ask clarifying questions focusing only on high-level aspects:\n",
    "\n",
    "What are the major components or phases?\n",
    "Who are the key stakeholders?\n",
    "What are the critical dependencies?\n",
    "What resources are already available?\n",
    "\n",
    "\n",
    "For each unclear point:\n",
    "\n",
    "Ask one focused question at a time\n",
    "Avoid diving into implementation details\n",
    "Capture only information relevant to overall structure\n",
    "\n",
    "Guidelines Throughout Process\n",
    "DO:\n",
    "\n",
    "Keep questions focused on structural elements\n",
    "Maintain high-level perspective\n",
    "Validate understanding frequently\n",
    "Document assumptions made\n",
    "\n",
    "DON'T:\n",
    "\n",
    "Ask about specific implementation steps\n",
    "Get caught up in technical details\n",
    "Try to solve problems at this stage\n",
    "Make decisions about execution methods\n",
    "\n",
    "Red Flags to Watch For\n",
    "\n",
    "Discussion becoming too detailed\n",
    "Questions about \"how\" instead of \"what\"\n",
    "Focus shifting to specific solutions\n",
    "Getting stuck on implementation challenges\n",
    "\n",
    "Remember: The goal is to create a clear framework for the plan, not to determine how each piece will be executed.\n",
    "\"\"\"\n",
    "\n",
    "sys_prompt_3 = \"\"\"\n",
    "You are in a planning workflow in this is the first step. Execute all the instructions below exactly as they are.\n",
    "Phase 3: Plan Summary & Validation\n",
    "\n",
    "Create a structured summary including:\n",
    "\n",
    "Main goal statement\n",
    "Key success criteria\n",
    "Major components identified\n",
    "Critical dependencies\n",
    "High-level timeline\n",
    "Key stakeholders\n",
    "\n",
    "\n",
    "Present the summary to user:\n",
    "\n",
    "Ask if anything major is missing\n",
    "Confirm if the interpretation is correct\n",
    "Verify if the level of detail is appropriate\n",
    "\n",
    "How to present: \n",
    "\n",
    "- Strucutre things in a way that is easy to understand like: \n",
    "    - Use bullet points\n",
    "    - Use numbered lists\n",
    "    - Use tables\n",
    "    - Use markdown\n",
    "- Make it visually appealing\n",
    "\n",
    "\n",
    "\n",
    "Guidelines Throughout Process\n",
    "DO:\n",
    "\n",
    "Keep questions focused on structural elements\n",
    "Maintain high-level perspective\n",
    "Validate understanding frequently\n",
    "Document assumptions made\n",
    "\n",
    "DON'T:\n",
    "\n",
    "Ask about specific implementation steps\n",
    "Get caught up in technical details\n",
    "Try to solve problems at this stage\n",
    "Make decisions about execution methods\n",
    "\n",
    "Red Flags to Watch For\n",
    "\n",
    "Discussion becoming too detailed\n",
    "Questions about \"how\" instead of \"what\"\n",
    "Focus shifting to specific solutions\n",
    "Getting stuck on implementation challenges\n",
    "\n",
    "Remember: The goal is to create a clear framework for the plan, not to determine how each piece will be executed.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "storage = PostgreSQLStorage.from_config(\n",
    "    host=\"localhost\",\n",
    "    port=5432,\n",
    "    user=\"primegraph\",\n",
    "    password=\"primegraph\",\n",
    "    database=\"primegraph\",\n",
    ")\n",
    "\n",
    "class PlannerState(GraphState):\n",
    "    conversation: History[Dict[str, str]] = Field(default_factory=list)\n",
    "    model_message: LastValue[Union[str, None]] = Field(\n",
    "        default=None\n",
    "    )  # Should be consumed by the bot before moving forward interruptions\n",
    "    user_message: LastValue[Union[str, None]] = Field(\n",
    "        default=None\n",
    "    )  # Should be consumed by the bot before moving forward interruptions\n",
    "    is_followup: LastValue[Union[None, bool]] = Field(default=None)\n",
    "    is_forward_permission: LastValue[Union[bool, None]] = Field(default=None)\n",
    "    is_permission_granted: LastValue[Union[bool, None]] = Field(default=None)\n",
    "    is_outside_of_step: LastValue[Union[bool, None]] = Field(default=None)\n",
    "    plan_goal: LastValue[str] = Field(default=\"\")\n",
    "    plan_summary: LastValue[str] = Field(default=\"\")\n",
    "    plan_details: History[str] = Field(default_factory=list)\n",
    "    is_ready_to_move_forward: LastValue[bool] = Field(default=False)\n",
    "    is_summary_approved: LastValue[bool] = Field(default=False)\n",
    "    interaction_summary: LastValue[str] = Field(default=\"\")\n",
    "\n",
    "\n",
    "def planner_graph(graph_state: PlannerState, graph_storage: Union[PostgreSQLStorage, None]) -> Graph:\n",
    "    plan_graph = Graph(state=graph_state, verbose=True, checkpoint_storage=graph_storage)\n",
    "    client = instructor.from_openai(OpenAI())\n",
    "\n",
    "    @plan_graph.node()\n",
    "    def process_user_message(state: PlannerState) -> dict[str, Any]:\n",
    "        class ProcessMessageResponse(BaseModel):\n",
    "            plan_goal: str = Field(description=\"The main goal of the plan\")\n",
    "            plan_details: List[str] = Field(description=\"Any additional relevant details of the plan\")\n",
    "            response: str = Field(description=\"Your response to the user\")\n",
    "            is_followup: bool = Field(description=\"If you need to ask follow up questions\")\n",
    "            is_forward_permission: bool = Field(description=\"If there are not follow up questions on your end.\")\n",
    "            is_outside_of_step: bool = Field(\n",
    "                description=\"If the user is explicitly asking to move out of the plan workflow\"\n",
    "            )\n",
    "\n",
    "        # Extract structured data from natural language\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            response_model=ProcessMessageResponse,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt_1},\n",
    "                *state.conversation,\n",
    "                {\"role\": \"user\", \"content\": state.user_message},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        if state.user_message:\n",
    "            state.conversation.append({\"role\": \"user\", \"content\": state.user_message})\n",
    "\n",
    "        # unpack plan_details:\n",
    "        if completion.plan_details:\n",
    "            for detail in completion.plan_details:\n",
    "                state.plan_details.append(detail)\n",
    "\n",
    "        return {\n",
    "            \"conversation\": {\"role\": \"assistant\", \"content\": completion.response},\n",
    "            \"plan_goal\": completion.plan_goal,\n",
    "            \"user_message\": None,\n",
    "            \"model_message\": completion.response,\n",
    "            \"is_followup\": completion.is_followup,\n",
    "            \"is_forward_permission\": completion.is_forward_permission,\n",
    "            \"is_outside_of_step\": completion.is_outside_of_step,\n",
    "        }\n",
    "\n",
    "    @plan_graph.node()\n",
    "    def response_router(state: PlannerState) -> str:\n",
    "        if state.is_followup:\n",
    "            return \"make_followup_questions\"\n",
    "        elif state.is_forward_permission:\n",
    "            return \"summarize_and_ask_permission\"\n",
    "        return END\n",
    "\n",
    "    @plan_graph.node(interrupt=\"before\")\n",
    "    def make_followup_questions(state: PlannerState):\n",
    "        class FollowupResponse(BaseModel):\n",
    "            plan_details: List[str] = Field(description=\"Any additional you got from your follow up questions\")\n",
    "            response: str = Field(description=\"Your response to the user\")\n",
    "            is_followup: bool = Field(description=\"If you need to ask follow up questions\")\n",
    "            is_forward_permission: bool = Field(description=\"If the user is asking to move forward to the next step\")\n",
    "            is_outside_of_step: bool = Field(\n",
    "                description=\"If the user is explicitly asking to move out of the plan workflow\"\n",
    "            )\n",
    "\n",
    "        # Extract structured data from natural language\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            response_model=FollowupResponse,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt_2},\n",
    "                *state.conversation,\n",
    "                {\"role\": \"user\", \"content\": state.user_message},\n",
    "            ],\n",
    "        )\n",
    "        if state.user_message:\n",
    "            state.conversation.append({\"role\": \"user\", \"content\": state.user_message})\n",
    "\n",
    "        rprint(completion)\n",
    "        # unpack plan_details:\n",
    "        if completion.plan_details:\n",
    "            for detail in completion.plan_details:\n",
    "                state.plan_details.append(detail)\n",
    "\n",
    "        return {\n",
    "            \"conversation\": {\"role\": \"assistant\", \"content\": completion.response},\n",
    "            \"user_message\": None,\n",
    "            \"model_message\": completion.response,\n",
    "            \"is_followup\": completion.is_followup,\n",
    "            \"is_forward_permission\": completion.is_forward_permission,\n",
    "            \"is_outside_of_step\": completion.is_outside_of_step,\n",
    "        }\n",
    "\n",
    "    @plan_graph.node()\n",
    "    def check_followup_next_step(state: PlannerState):\n",
    "        if state.is_followup:\n",
    "            return \"make_followup_questions\"\n",
    "        else:\n",
    "            return \"summarize_and_ask_permission\"\n",
    "\n",
    "    @plan_graph.node(interrupt=\"before\")\n",
    "    def summarize_and_ask_permission(state: PlannerState):\n",
    "        class PermissionResponse(BaseModel):\n",
    "            response: str = Field(description=\"Your response to the user\")\n",
    "            is_followup: bool = Field(description=\"If you need to ask follow up questions\")\n",
    "            is_outside_of_step: bool = Field(description=\"If the user is explicitly asking to move to a different step\")\n",
    "            is_permission_granted: bool = Field(description=\"If the user authorized moving to the next step\")\n",
    "\n",
    "        # Extract structured data from natural language\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            response_model=PermissionResponse,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt_3},\n",
    "                *state.conversation,\n",
    "                {\"role\": \"user\", \"content\": state.user_message},\n",
    "            ],\n",
    "        )\n",
    "        if state.user_message:\n",
    "            state.conversation.append({\"role\": \"user\", \"content\": state.user_message})\n",
    "\n",
    "        return {\n",
    "            \"conversation\": {\"role\": \"assistant\", \"content\": completion.response},\n",
    "            \"user_message\": None,\n",
    "            \"model_message\": completion.response,\n",
    "            \"is_followup\": completion.is_followup,\n",
    "            \"is_permission_granted\": completion.is_permission_granted,\n",
    "            \"is_outside_of_step\": completion.is_outside_of_step,\n",
    "        }\n",
    "\n",
    "\n",
    "    @plan_graph.node()\n",
    "    def move_to_next_step(state: PlannerState):\n",
    "        if state.is_permission_granted:\n",
    "            return END\n",
    "        else:\n",
    "            return \"process_user_message\"\n",
    "\n",
    "    plan_graph.add_edge(START, \"process_user_message\")\n",
    "    plan_graph.add_router_edge(\"process_user_message\", \"response_router\")\n",
    "    plan_graph.add_router_edge(\"make_followup_questions\", \"check_followup_next_step\")\n",
    "\n",
    "    plan_graph.add_router_edge(\"summarize_and_ask_permission\", \"move_to_next_step\")\n",
    "\n",
    "    plan_graph.compile()\n",
    "\n",
    "    return plan_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_graph_state = PlannerState()\n",
    "\n",
    "graph = planner_graph(plan_graph_state, graph_storage=storage)\n",
    "\n",
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_id = await graph.start_async()\n",
    "rprint(plan_graph_state)\n",
    "rprint(graph.chain_status)\n",
    "rprint(graph.next_execution_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plan_graph_state = PlannerState()\n",
    "graph = planner_graph(plan_graph_state, storage)\n",
    "graph.load_from_checkpoint(chain_id)\n",
    "\n",
    "graph.update_state_and_checkpoint({\"user_message\": \"I want to plan a wedding party\"})\n",
    "print(\"state_pre_checkpoint\")\n",
    "rprint(graph.state)\n",
    "\n",
    "plan_graph_state = PlannerState()\n",
    "graph = planner_graph(plan_graph_state, storage)\n",
    "graph.load_from_checkpoint(chain_id)\n",
    "\n",
    "await graph.resume_async()\n",
    "print(\"state_post_checkpoint\") \n",
    "rprint(graph.state)\n",
    "rprint(graph.chain_status)\n",
    "rprint(graph.next_execution_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plan_graph_state = PlannerState()\n",
    "graph = planner_graph(plan_graph_state, storage)\n",
    "graph.load_from_checkpoint(chain_id)\n",
    "\n",
    "graph.update_state_and_checkpoint({\"user_message\": \"sorry lets go back to planning\"})\n",
    "\n",
    "plan_graph_state = PlannerState()\n",
    "graph = planner_graph(plan_graph_state, storage)\n",
    "graph.load_from_checkpoint(chain_id)\n",
    "\n",
    "await graph.resume_async()\n",
    "rprint(graph.state)\n",
    "rprint(graph.chain_status)\n",
    "rprint(graph.next_execution_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cyclical planner 2- storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from primeGraph import Graph, START, END\n",
    "from pydantic import Field\n",
    "from primeGraph.models import GraphState\n",
    "from primeGraph.buffer import History, LastValue\n",
    "from rich import print as rprint\n",
    "from typing import Dict, Union, List\n",
    "from primeGraph.checkpoint.postgresql import PostgreSQLStorage\n",
    "from rich import print as rprint\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "\n",
    "storage = PostgreSQLStorage.from_config(\n",
    "    host=\"localhost\",\n",
    "    port=5432,\n",
    "    user=\"primegraph\",\n",
    "    password=\"primegraph\",\n",
    "    database=\"primegraph\",\n",
    ")\n",
    "\n",
    "sys_prompt_start = \"\"\"\n",
    "You are a helpful assistant that is able to help the user with their goals.\n",
    "\n",
    "You are part of a workflow for the user to plan for something. This is the first step of the workflow.\n",
    "\n",
    "Give the user a welcome in a brief way and prepare them to start sharing their goal. Things you need to let the user know:\n",
    "\n",
    "- They should share their goal in a clear and concise manner\n",
    "- [OPTIONAL] they should share any relevant context or details about the goal\n",
    "- [OPTIONAL] They should express what success looks like for this goal\n",
    "- [OPTIONAL] They should share any constraints or requirements for the goal\n",
    "\"\"\"\n",
    "\n",
    "sys_prompt_process_message = \"\"\"\n",
    "\n",
    "==== OVERALL GUIDANCE =====\n",
    "You are in a planning workflow in this is the second (and more important) step.\n",
    "\n",
    "Your goal is to analyze the user's message and route them to the next step in the workflow.\n",
    "\n",
    "You will be given the user's message and the conversation history. Give more weight to the user's message than the conversation history.\n",
    "\n",
    "__ The main goal for this entire planning process: __\n",
    "\n",
    "- Break down the user goal in the planning steps that are:\n",
    "    - Clear\n",
    "    - Concise\n",
    "    - Easy to understand\n",
    "    - Easy to follow\n",
    "\n",
    "__ The main goal with this step is to: __\n",
    "\n",
    "- Analyze current information about the goal and the conversation history\n",
    "- Assess if the information gathered is enough to create a good plan\n",
    "- Route user to the next step in the workflow\n",
    "- Make sure you capture user's intent and route them to the correct step\n",
    "- Make the user experience seemless and seamless\n",
    "\n",
    "==== WORKFLOW STEPS =====\n",
    "\n",
    "Everything evolve around you capacity to create a good plan in the end. \n",
    "Based on the information gathered, evaluate the following:\n",
    "\n",
    "IF information about the goal, details, or anything that can help you create a good plan is needed:\n",
    "- [Follow up questions]\n",
    "    - Ask follow up questions to gather more information about the goal\n",
    "    - Analyze all the information gathered and judge if addional information is needed\n",
    "    - Be clear and concise with the follow up questions\n",
    "\n",
    "IF all the information is gathered and/or the user is ready to move forward, choose between:\n",
    "- [Summarize and ask permission]: \n",
    "    - Help the user visualize the high level plan\n",
    "    - Share your plan into macro steps with a brief summary of what each step entails\n",
    "    - Check if the user would like to proceed with the next step\n",
    "- [Finalize]: If the summary is approved, or the user is saying that they are ready to move forward, you should proceed to the next step\n",
    "\n",
    "Unrelated:\n",
    "- [Outside of the workflow]: The user is explicitly asking to move out of the plan workflow or explicitly saying that they are done or want to cancel.\n",
    "\n",
    "\n",
    "==== GUIDELINES ON HOW TO ACT =====\n",
    "\n",
    "- Pay extreme attention to the user's message\n",
    "- Try to understand the user's intent\n",
    "- Always pick only ONE of the options presented to you on WORKFLOW STEPS\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sys_prompt_followup = \"\"\"\n",
    "==== OVERALL GUIDANCE =====\n",
    "You are in a planning workflow and this is a follow up step.\n",
    "\n",
    "Your goal is to analyze the user's goal, the information gathered and the conversation history.\n",
    "Based on the information gathered, you should ask follow up questions to gather more information about the goal.\n",
    "\n",
    "\n",
    "__ The main goal for this entire planning process: __\n",
    "\n",
    "- Break down the user goal in the planning steps that are:\n",
    "    - Clear\n",
    "    - Concise\n",
    "    - Easy to understand\n",
    "    - Easy to follow\n",
    "\n",
    "__ The main goal with this step is to: __\n",
    "\n",
    "- Analyze current information\n",
    "- Make additional follow up questions ONLY IF NEEDED\n",
    "    - The act of gather more information should be to make sure that the planning process has its goals achieved\n",
    "\n",
    "==== GUIDELINES ON HOW TO ACT =====\n",
    "\n",
    "- Pay extreme attention to all the curren information gathered\n",
    "- Identify the gaps into a good planning to achieve the goal and the information needed to fill those gaps\n",
    "- Make follow up questions ONLY IF NEEDED\n",
    "    - The act of gather more information should be to make sure that the planning process has its goals achieved\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sys_prompt_summarize = \"\"\"\n",
    "==== OVERALL GUIDANCE =====\n",
    "You are in a planning workflow and this is a summarize step.\n",
    "\n",
    "Your goal is to share with the user a high level overview of the plan you are about to create.\n",
    "Make it short anc concise, but also include all the important details.\n",
    "Make it visually easy to understand and review (on the user's end). \n",
    "Ask if the user would like to proceed with the next step.\n",
    "\n",
    "\n",
    "__ The main goal for this entire planning process: __\n",
    "\n",
    "- Break down the user goal in the planning steps that are:\n",
    "    - Clear\n",
    "    - Concise\n",
    "    - Easy to understand\n",
    "    - Easy to follow\n",
    "\n",
    "__ The main goal with this step is to: __\n",
    "\n",
    "- Summarize the plan in a way that is easy to understand and review\n",
    "- Make it visually easy to understand and review (on the user's end)\n",
    "- Ask if the user would like to proceed with the next step\n",
    "\n",
    "==== GUIDELINES ON HOW TO ACT =====\n",
    "\n",
    "- Make sure you include all the important details\n",
    "- Make sure you have a decent break down of the plan\n",
    "- Make it visually easy to understand and review (on the user's end)\n",
    "- Ask if the user would like to proceed with the next step\n",
    "\"\"\"\n",
    "\n",
    "sys_prompt_finalize = \"\"\"\n",
    "==== OVERALL GUIDANCE =====\n",
    "You are in a planning workflow and this is a finalize step.\n",
    "\n",
    "Your goal is to say goodbye to the user and thank them for using your service.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class PlannerState(GraphState):\n",
    "    conversation: History[Dict[str, str]] = Field(default_factory=list)\n",
    "    model_message: LastValue[Union[str, None]] = Field(\n",
    "        default=None\n",
    "    )  # Should be consumed by the bot before moving forward interruptions\n",
    "    user_message: LastValue[Union[str, None]] = Field(\n",
    "        default=None\n",
    "    )  # Should be consumed by the bot before moving forward interruptions\n",
    "    is_followup: LastValue[Union[None, bool]] = Field(default=None)\n",
    "    is_summarize: LastValue[Union[bool, None]] = Field(default=None)\n",
    "    is_finalize: LastValue[Union[bool, None]] = Field(default=None)\n",
    "    is_outside_of_the_workflow: LastValue[Union[bool, None]] = Field(default=None)\n",
    "    plan_goal: LastValue[str] = Field(default=\"\")\n",
    "    plan_summary: LastValue[str] = Field(default=\"\")\n",
    "    plan_steps: LastValue[List[str]] = Field(default_factory=list)\n",
    "    plan_details: History[str] = Field(default_factory=list)\n",
    "    current_information_assessment: LastValue[str] = Field(default=\"\")\n",
    "    follow_up_questions: LastValue[List[str]] = Field(default_factory=list)\n",
    "    plan_summary: LastValue[str] = Field(default=\"\")\n",
    "    plan_steps: LastValue[List[str]] = Field(default_factory=list)\n",
    "\n",
    "def planner_graph(graph_state: PlannerState, checkpoint_storage: Union[PostgreSQLStorage, None]) -> Graph:\n",
    "    plan_graph = Graph(state=graph_state, verbose=True, checkpoint_storage=checkpoint_storage)\n",
    "    client = instructor.from_openai(OpenAI())\n",
    "\n",
    "    @plan_graph.node(interrupt=\"after\")\n",
    "    def start_conversation(state: PlannerState) -> dict[str, Any]:\n",
    "        class StartConversationResponse(BaseModel):\n",
    "            response: str = Field(description=\"Your response to the user\")\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            response_model=StartConversationResponse,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt_start},\n",
    "            ],\n",
    "        )\n",
    "        return {\"conversation\": {\"role\": \"assistant\", \"content\": completion.response}}\n",
    "\n",
    "    @plan_graph.node()\n",
    "    def process_user_message(state: PlannerState) -> dict[str, Any]:\n",
    "        class ProcessMessageResponse(BaseModel):\n",
    "            plan_goal: str = Field(description=\"The main goal of the plan\")\n",
    "            plan_details: List[str] = Field(description=\"Any additional relevant details of the plan\")\n",
    "            is_followup: bool # = Field(description=\"If you need to ask follow up questions\")\n",
    "            is_summarize: bool # = Field(description=\"If there are not follow up questions on your end.\")\n",
    "            is_finalize: bool # = Field(\n",
    "            #     description=\"If the user is explicitly asking to move out of the plan workflow\"\n",
    "            # )\n",
    "            is_outside_of_the_workflow: bool # = Field(\n",
    "            #     description=\"If the user is explicitly asking to move out of the plan workflow\"\n",
    "            # )\n",
    "\n",
    "        rprint(\"state before processing user message\")\n",
    "        rprint(state)\n",
    "        # add user message to the conversation\n",
    "        if state.user_message:\n",
    "            state.conversation.append({\"role\": \"user\", \"content\": state.user_message})\n",
    "\n",
    "        # Extract structured data from natural language\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            response_model=ProcessMessageResponse,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt_process_message},\n",
    "                *state.conversation,\n",
    "                {\"role\": \"user\", \"content\": state.user_message},\n",
    "            ],\n",
    "        )\n",
    "        rprint(\"chat completion\")\n",
    "        rprint(completion)\n",
    "\n",
    "        # unpack plan_details:\n",
    "        if completion.plan_details:\n",
    "            for detail in completion.plan_details:\n",
    "                state.plan_details.append(detail)\n",
    "\n",
    "        return {\n",
    "            \"plan_goal\": completion.plan_goal,\n",
    "            \"user_message\": None,\n",
    "            \"model_message\": None,\n",
    "            \"is_followup\": completion.is_followup,\n",
    "            \"is_summarize\": completion.is_summarize,\n",
    "            \"is_finalize\": completion.is_finalize,\n",
    "            \"is_outside_of_the_workflow\": completion.is_outside_of_the_workflow,\n",
    "        }\n",
    "\n",
    "    @plan_graph.node()\n",
    "    def response_router(state: PlannerState) -> str:\n",
    "        if state.is_finalize:\n",
    "            return \"finalize_plan\"\n",
    "        elif state.is_summarize:\n",
    "            return \"summarize_plan\"\n",
    "        elif state.is_followup:\n",
    "            return \"make_followup_questions\"\n",
    "        else:\n",
    "            return \"process_user_message\"\n",
    "                \n",
    "    @plan_graph.node(interrupt=\"after\")\n",
    "    def make_followup_questions(state: PlannerState):\n",
    "        class FollowupResponse(BaseModel):\n",
    "            plan_goal: str = Field(description=\"The main goal of the plan\")\n",
    "            plan_details: List[str] = Field(description=\"Any additional you got from your interactions with the user\")\n",
    "            current_information_assessment: str = Field(description=\"A brief assessment of the current information gathered\")\n",
    "            follow_up_questions: List[str] = Field(description=\"The follow up questions that you think are needed to gather more information about the goal\")\n",
    "            response: str = Field(description=\"Your response with the follow up questions\")\n",
    "\n",
    "    \n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            response_model=FollowupResponse,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt_followup},\n",
    "                *state.conversation,\n",
    "                {\"role\": \"user\", \"content\": state.user_message},\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        # unpack plan_details:\n",
    "        if completion.plan_details:\n",
    "            for detail in completion.plan_details:\n",
    "                state.plan_details.append(detail)\n",
    "\n",
    "        return {\n",
    "            \"conversation\": {\"role\": \"assistant\", \"content\": completion.response},\n",
    "            \"user_message\": None,\n",
    "            \"model_message\": completion.response,\n",
    "            \"current_information_assessment\": completion.current_information_assessment,\n",
    "            \"follow_up_questions\": completion.follow_up_questions,\n",
    "        }\n",
    "  \n",
    "    @plan_graph.node(interrupt=\"after\")\n",
    "    def summarize_plan(state: PlannerState):\n",
    "        class SummarizeResponse(BaseModel):\n",
    "            response: str = Field(description=\"Your response to the user\")\n",
    "            plan_summary: str = Field(description=\"A summary of the plan\")\n",
    "            plan_steps: List[str] = Field(description=\"A list of the steps that are part of the plan\")\n",
    "\n",
    "            \n",
    "        # Extract structured data from natural language\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            response_model=SummarizeResponse,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt_summarize},\n",
    "                *state.conversation,\n",
    "                {\"role\": \"user\", \"content\": state.user_message},\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "        return {\n",
    "            \"conversation\": {\"role\": \"assistant\", \"content\": completion.response},\n",
    "            \"user_message\": None,\n",
    "            \"model_message\": completion.response,\n",
    "            \"plan_summary\": completion.plan_summary,\n",
    "            \"plan_steps\": completion.plan_steps,\n",
    "        }\n",
    "    \n",
    "    @plan_graph.node()\n",
    "    def finalize_plan(state: PlannerState):\n",
    "        class FinalizeResponse(BaseModel):\n",
    "            response: str = Field(description=\"Your response to the user\")\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            response_model=FinalizeResponse,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt_finalize},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"conversation\": {\"role\": \"assistant\", \"content\": completion.response},\n",
    "            \"user_message\": None,\n",
    "            \"model_message\": completion.response,\n",
    "        }\n",
    "\n",
    "\n",
    "    plan_graph.add_edge(START, \"start_conversation\")\n",
    "    plan_graph.add_edge(\"start_conversation\", \"process_user_message\")\n",
    "    plan_graph.add_router_edge(\"process_user_message\", \"response_router\")\n",
    "\n",
    "    plan_graph.add_edge(\"summarize_plan\", \"process_user_message\")\n",
    "    plan_graph.add_edge(\"make_followup_questions\", \"process_user_message\")\n",
    "    plan_graph.add_edge(\"finalize_plan\", END)\n",
    "    \n",
    "    plan_graph.compile()\n",
    "\n",
    "    return plan_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_graph_state = PlannerState()\n",
    "\n",
    "graph = planner_graph(plan_graph_state, checkpoint_storage=storage)\n",
    "\n",
    "graph.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run without using checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_id = await graph.execute()\n",
    "# rprint(plan_graph_state)\n",
    "# rprint(graph.chain_status)\n",
    "rprint(graph.execution_engine.get_full_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"state before resume\")\n",
    "rprint(graph.execution_engine.get_full_state())\n",
    "\n",
    "message = \"Ok this looks good\"\n",
    "\n",
    "graph.update_state_and_checkpoint({\"user_message\": message, \n",
    "                                   \"conversation\": {\"role\": \"user\", \"content\": message}})\n",
    "print(\"state after resume\")\n",
    "chain_id = await graph.resume()\n",
    "# rprint(plan_graph_state)\n",
    "# rprint(graph.chain_status)\n",
    "rprint(graph.execution_engine.get_full_state())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run using checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"first execution\")\n",
    "chain_id = await graph.execute()\n",
    "# rprint(plan_graph_state)\n",
    "# rprint(graph.chain_status)\n",
    "rprint(graph.execution_engine.get_full_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"state before state update\")\n",
    "rprint(graph.execution_engine.get_full_state())\n",
    "\n",
    "plan_graph_state = PlannerState()\n",
    "graph = planner_graph(plan_graph_state, storage)\n",
    "graph.load_from_checkpoint(chain_id)\n",
    "\n",
    "graph.update_state_and_checkpoint({\"user_message\": \"I want to plan a wedding party\", \n",
    "                                   \"conversation\": {\"role\": \"user\", \"content\": \"I want to plan a wedding party\"}})\n",
    "print(\"state after state update\")\n",
    "# rprint(graph.state)\n",
    "rprint(graph.execution_engine.get_full_state())\n",
    "\n",
    "plan_graph_state = PlannerState()\n",
    "graph = planner_graph(plan_graph_state, storage)\n",
    "graph.load_from_checkpoint(chain_id)\n",
    "\n",
    "print(\"state right before resume\")\n",
    "rprint(graph.state)\n",
    "rprint(graph.execution_engine.get_full_state())\n",
    "\n",
    "await graph.resume()\n",
    "print(\"state post resume\") \n",
    "# rprint(graph.state)\n",
    "# rprint(graph.chain_status)\n",
    "rprint(graph.execution_engine.get_full_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await graph.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(graph.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.execution_engine.get_full_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_graph_state = PlannerState()\n",
    "graph = planner_graph(plan_graph_state, storage)\n",
    "graph.load_from_checkpoint(chain_id)\n",
    "\n",
    "graph.update_state_and_checkpoint({\"user_message\": \"I don't have any of these details\"})\n",
    "print(\"state_pre_checkpoint\")\n",
    "rprint(graph.state)\n",
    "\n",
    "plan_graph_state = PlannerState()\n",
    "graph = planner_graph(plan_graph_state, storage)\n",
    "graph.load_from_checkpoint(chain_id)\n",
    "\n",
    "await graph.resume_async()\n",
    "print(\"state_post_checkpoint\") \n",
    "rprint(graph.state)\n",
    "rprint(graph.chain_status)\n",
    "rprint(graph.next_execution_node)\n",
    "rprint(graph.execution_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(graph.detailed_execution_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "rprint(graph.execution_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(graph.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(graph.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(graph.detailed_execution_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(graph.execution_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested BaseModels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from primeGraph import Graph, START, END\n",
    "from pydantic import Field\n",
    "from primeGraph.models import GraphState\n",
    "from primeGraph.buffer import History, LastValue\n",
    "from rich import print as rprint\n",
    "from typing import Dict, Union, List\n",
    "from primeGraph.checkpoint.postgresql import PostgreSQLStorage\n",
    "from rich import print as rprint\n",
    "\n",
    "storage = PostgreSQLStorage.from_config(\n",
    "    host=\"localhost\",\n",
    "    port=5432,\n",
    "    user=\"primegraph\",\n",
    "    password=\"primegraph\",\n",
    "    database=\"primegraph\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BreakdownInstruction(BaseModel):\n",
    "    step_name: str = Field(description=\"The name of the step\")\n",
    "    step_instructions: str = Field(description=\"The instructions for the step\")\n",
    "\n",
    "\n",
    "class BreakdownState(GraphState):\n",
    "    instructions: LastValue[List[BreakdownInstruction]] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "def breakdown_graph(graph_state: BreakdownState, graph_storage: Union[PostgreSQLStorage, None]) -> Graph:\n",
    "    plan_graph = Graph(state=graph_state, verbose=True, checkpoint_storage=graph_storage)\n",
    "    client = instructor.from_openai(OpenAI())\n",
    "\n",
    "    @plan_graph.node(interrupt=\"after\")\n",
    "    def start_conversation(state: BreakdownState) -> dict[str, Any]:\n",
    "        pass\n",
    "\n",
    "    @plan_graph.node()\n",
    "    def process_user_message(state: BreakdownState) -> dict[str, Any]:\n",
    "        pass\n",
    "\n",
    "    @plan_graph.node()\n",
    "    def response_router(state: BreakdownState) -> str:\n",
    "        if True:\n",
    "            return \"prepare_output_instructions\"\n",
    "        elif state.is_edit_and_iterate or not state.is_finalize:\n",
    "            return \"edit_and_iterate\"\n",
    "        else:\n",
    "            return \"process_user_message\"\n",
    "\n",
    "    @plan_graph.node(interrupt=\"after\")\n",
    "    def edit_and_iterate(state: BreakdownState):\n",
    "        pass\n",
    "\n",
    "    @plan_graph.node(interrupt=\"after\")\n",
    "    def ask_approval(state: BreakdownState):\n",
    "        pass\n",
    "\n",
    "    @plan_graph.node()\n",
    "    def process_approval(state: BreakdownState):\n",
    "        pass\n",
    "\n",
    "    @plan_graph.node()\n",
    "    def approval_router(state: BreakdownState):\n",
    "        if state.is_approved:\n",
    "            return END\n",
    "        else:\n",
    "            return \"edit_and_iterate\"\n",
    "\n",
    "    @plan_graph.node()\n",
    "    def summarize_breakdown(state: BreakdownState):\n",
    "        pass\n",
    "\n",
    "    @plan_graph.node()\n",
    "    def prepare_output_instructions(state: BreakdownState):\n",
    "        pass\n",
    "\n",
    "    plan_graph.add_edge(START, \"start_conversation\")\n",
    "    plan_graph.add_edge(\"start_conversation\", \"process_user_message\")\n",
    "    plan_graph.add_router_edge(\"process_user_message\", \"response_router\")\n",
    "    plan_graph.add_edge(\"edit_and_iterate\", \"process_user_message\")\n",
    "    plan_graph.add_edge(\"prepare_output_instructions\", \"summarize_breakdown\")\n",
    "    plan_graph.add_edge(\"summarize_breakdown\", \"ask_approval\")\n",
    "    plan_graph.add_edge(\"ask_approval\", \"process_approval\")\n",
    "    plan_graph.add_router_edge(\"process_approval\", \"approval_router\")\n",
    "\n",
    "    plan_graph.compile()\n",
    "\n",
    "    return plan_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 08:07:53.049 - primeGraph.checkpoint.postgresql - INFO - Checkpoint 'checkpoint_53bf4ef8-efc0-43fc-8349-8197b0f1a41c' saved to PostgreSQL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BreakdownState</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">version</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'1c03d4f0e96b60f818ba71e45f2ef8c0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">instructions</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BreakdownInstruction</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">step_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Research SpaceX Missions'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">step_instructions</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Start by researching SpaceX's upcoming missions...\"</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BreakdownInstruction</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">step_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Budget Planning'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">step_instructions</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Calculate the estimated costs...'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BreakdownInstruction</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">step_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Testing'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">step_instructions</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Testing...'</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mBreakdownState\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mversion\u001b[0m=\u001b[32m'1c03d4f0e96b60f818ba71e45f2ef8c0'\u001b[0m,\n",
       "    \u001b[33minstructions\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mBreakdownInstruction\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mstep_name\u001b[0m=\u001b[32m'Research SpaceX Missions'\u001b[0m,\n",
       "            \u001b[33mstep_instructions\u001b[0m=\u001b[32m\"Start\u001b[0m\u001b[32m by researching SpaceX's upcoming missions...\"\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mBreakdownInstruction\u001b[0m\u001b[1m(\u001b[0m\u001b[33mstep_name\u001b[0m=\u001b[32m'Budget Planning'\u001b[0m, \u001b[33mstep_instructions\u001b[0m=\u001b[32m'Calculate the estimated costs...'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mBreakdownInstruction\u001b[0m\u001b[1m(\u001b[0m\u001b[33mstep_name\u001b[0m=\u001b[32m'Testing'\u001b[0m, \u001b[33mstep_instructions\u001b[0m=\u001b[32m'Testing...'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "initial_instructions = [\n",
    "        BreakdownInstruction(\n",
    "            step_name=\"Research SpaceX Missions\",\n",
    "            step_instructions=\"Start by researching SpaceX's upcoming missions...\"\n",
    "        ),\n",
    "        BreakdownInstruction(\n",
    "            step_name=\"Budget Planning\",\n",
    "            step_instructions=\"Calculate the estimated costs...\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "updated_instructions = [\n",
    "    BreakdownInstruction(\n",
    "        step_name=\"Research SpaceX Missions\",\n",
    "        step_instructions=\"Start by researching SpaceX's upcoming missions...\"\n",
    "    ),\n",
    "    BreakdownInstruction(\n",
    "        step_name=\"Budget Planning\",\n",
    "        step_instructions=\"Calculate the estimated costs...\"\n",
    "    ),\n",
    "    BreakdownInstruction(\n",
    "        step_name=\"Testing\",\n",
    "        step_instructions=\"Testing...\"\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "initial_state = BreakdownState(instructions=initial_instructions)\n",
    "first_graph = Graph(state=initial_state, checkpoint_storage=storage)\n",
    "\n",
    "first_graph.update_state_and_checkpoint({\"instructions\": updated_instructions})\n",
    "\n",
    "first_graph_chain_id = first_graph.chain_id\n",
    "\n",
    "# starting fresh\n",
    "fresh_state = BreakdownState(instructions=[])\n",
    "fresh_graph = Graph(state=fresh_state, checkpoint_storage=storage)\n",
    "\n",
    "fresh_graph.load_from_checkpoint(first_graph_chain_id)\n",
    "\n",
    "from rich import print as rprint\n",
    "\n",
    "rprint(fresh_graph.state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

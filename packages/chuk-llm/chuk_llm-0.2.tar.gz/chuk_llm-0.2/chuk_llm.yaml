# chuk_llm.yaml - Complete Configuration with TEXT feature added
# Unified configuration combining providers and capabilities

##############################################################################
# Global Configuration
##############################################################################
__global__:
  # Default provider and model when none specified
  active_provider: openai
  active_model: gpt-4o-mini
  # Runtime settings
  default_timeout: 30
  max_retries: 3

##############################################################################
# Global Model Aliases (cross-provider shortcuts)
##############################################################################
__global_aliases__:
  # Ultra-short aliases for most popular models
  gpt: openai/gpt-4.1
  gpt_mini: openai/gpt-4.1-mini
  
  # Anthropic aliases
  claude: anthropic/claude-sonnet-4-20250514
  sonnet: anthropic/claude-sonnet-4-20250514
  opus: anthropic/claude-opus-4-20250514
  
  # Open source models
  llama: groq/llama-3.3-70b-versatile
  mixtral: groq/mixtral-8x7b-32768
  
  # Google models
  gemini: gemini/gemini-2.0-flash
  gemini_pro: gemini/gemini-1.5-pro
  
  # Specialized models
  mistral: mistral/magistral-medium-2506
  deepseek: deepseek/deepseek-chat
  deepseek_reasoning: deepseek/deepseek-reasoner
  
  # Local models
  qwen: ollama/qwen3
  granite: ollama/granite3.3
  phi: ollama/phi3
  
  # Enterprise models
  watsonx: watsonx/ibm/granite-3-3-8b-instruct
  
  # Reasoning models
  reasoning: mistral/magistral-medium-2506
  reasoning_free: mistral/magistral-small-2506
  reasoning_advanced: deepseek/deepseek-reasoner

##############################################################################
# Provider Configurations
##############################################################################

# OpenAI – most popular models
openai:
  client_class: "chuk_llm.llm.providers.openai_client.OpenAILLMClient"
  api_key_env: "OPENAI_API_KEY"
  default_model: "gpt-4o-mini"
  
  # Provider baseline features - TEXT is fundamental
  features: [text, streaming, system_messages, json_mode]
  max_context_length: 128000
  max_output_tokens: 4096
  rate_limits:
    default: 3500
    tier_1: 500
  
  models:
    - "gpt-4.1"
    - "gpt-4.1-mini"
    - "gpt-4.1-nano"
    - "gpt-4o"
    - "gpt-4o-mini"
    - "gpt-4-turbo"
    - "gpt-4"
    - "gpt-3.5-turbo"
    - "o1"
    - "o1-mini"
    - "o3"
    - "o3-mini"
    - "o4-mini"
  
  model_aliases:
    # Ultra-short aliases
    gpt4o: "gpt-4o"
    gpt4o_mini: "gpt-4o-mini"
    gpt4_turbo: "gpt-4-turbo"
    gpt4: "gpt-4"
    gpt3_5: "gpt-3.5-turbo"
    gpt4_1: "gpt-4.1"
    # O-series aliases
    o1_mini: "o1-mini"
    o3_mini: "o3-mini"
    o4_mini: "o4-mini"
    # Version aliases
    latest: "gpt-4.1"
    mini: "gpt-4.1-mini"
    nano: "gpt-4.1-nano"
    turbo: "gpt-4-turbo"
    # Reasoning aliases
    reasoning: "o3"
    reasoning_mini: "o3-mini"
  
  model_capabilities:
    # O-series (reasoning models) - includes text
    - pattern: "o[1-4].*"
      features: [text, streaming, tools, vision, reasoning, parallel_calls]
      max_context_length: 200000
      max_output_tokens: 32768
    
    # GPT-4o (Omni family) - full capabilities including text
    - pattern: "gpt-4o.*"
      features: [text, streaming, tools, vision, parallel_calls]
      max_context_length: 128000
      max_output_tokens: 8192
    
    # GPT-4.0/4.1/4-Turbo - text + advanced features
    - pattern: "gpt-4\\.[01].*|gpt-4-(turbo|preview).*"
      features: [text, streaming, tools, vision, parallel_calls]
      max_context_length: 128000
      max_output_tokens: 8192
    
    # GPT-4 - text + reasoning
    - pattern: "gpt-4$"
      features: [text, streaming, tools, vision, parallel_calls]
      max_context_length: 8192
      max_output_tokens: 4096
    
    # GPT-3.5-Turbo - text + basic features
    - pattern: "gpt-3\\.5-turbo.*"
      features: [text, streaming]
      max_context_length: 16384
      max_output_tokens: 4096

# DeepSeek – reasoning specialist
deepseek:
  inherits: "openai"
  client_class: "chuk_llm.llm.providers.openai_client.OpenAILLMClient"
  api_key_env: "DEEPSEEK_API_KEY"
  api_base: "https://api.deepseek.com"
  default_model: "deepseek-reasoner"
  
  # All text models have text capability
  features: [text, streaming, json_mode, system_messages]
  max_context_length: 65536
  max_output_tokens: 8192
  rate_limits:
    default: 3000
  
  models:
    - "deepseek-chat"
    - "deepseek-reasoner"
  
  model_aliases:
    chat: "deepseek-chat"
    reasoner: "deepseek-reasoner"
    reasoning: "deepseek-reasoner"
    thinking: "deepseek-reasoner"
    default: "deepseek-reasoner"
  
  model_capabilities:
    - pattern: "deepseek-chat"
      features: [text, streaming, json_mode]
      max_context_length: 65536
      max_output_tokens: 8192
    
    - pattern: "deepseek-reasoner"
      features: [text, streaming, json_mode, reasoning]
      max_context_length: 65536
      max_output_tokens: 8192

# Anthropic – Claude models
anthropic:
  client_class: "chuk_llm.llm.providers.anthropic_client.AnthropicLLMClient"
  api_key_env: "ANTHROPIC_API_KEY"
  default_model: "claude-sonnet-4-20250514"
  
  # All Claude models have excellent text capabilities
  features: [text, streaming, system_messages, reasoning]
  max_context_length: 200000
  max_output_tokens: 4096
  rate_limits:
    default: 4000
  
  models:
    - "claude-opus-4-20250514"
    - "claude-sonnet-4-20250514"
    - "claude-3-7-sonnet-20250219"
    - "claude-3-5-sonnet-20241022"
    - "claude-3-5-haiku-20241022"
    - "claude-3-sonnet-20240229"
    - "claude-3-opus-20240229"
    - "claude-3-haiku-20240307"
  
  model_aliases:
    # Version aliases
    sonnet4: "claude-sonnet-4-20250514"
    opus4: "claude-opus-4-20250514"
    sonnet37: "claude-3-7-sonnet-20250219"
    haiku37: "claude-3-5-haiku-20241022"
    sonnet35: "claude-3-5-sonnet-20241022"
    haiku35: "claude-3-5-sonnet-20241022"
    # Simple aliases
    opus: "claude-opus-4-20250514"
    sonnet: "claude-sonnet-4-20250514"
    haiku: "claude-3-5-haiku-20241022"
    latest: "claude-sonnet-4-20250514"
    # Reasoning aliases
    reasoning: "claude-opus-4-20250514"
    thinking: "claude-sonnet-4-20250514"
  
  model_capabilities:
    # Claude 4 models - text + advanced capabilities
    - pattern: "claude-(opus|sonnet)-4-.*"
      features: [text, streaming, tools, vision, parallel_calls, reasoning]
      max_context_length: 200000
      max_output_tokens: 8192
    
    # Claude 3.7 models - text + advanced capabilities
    - pattern: "claude-3-7-.*"
      features: [text, streaming, tools, vision, parallel_calls, reasoning]
      max_context_length: 200000
      max_output_tokens: 8192
    
    # Claude 3.5 models - text + capabilities
    - pattern: "claude-3-5-.*"
      features: [text, streaming, tools, vision, parallel_calls]
      max_context_length: 200000
      max_output_tokens: 4096
    
    # Other Claude 3 models - text + basic capabilities
    - pattern: "claude-3-.*"
      features: [text, streaming, tools, vision]
      max_context_length: 200000
      max_output_tokens: 4096

# Groq – ultra-fast inference
groq:
  inherits: "openai"
  client_class: "chuk_llm.llm.providers.groq_client.GroqAILLMClient"
  api_key_env: "GROQ_API_KEY"
  api_base: "https://api.groq.com"
  default_model: "llama-3.3-70b-versatile"
  
  # All Groq models support text
  features: [text, streaming, tools, parallel_calls]
  max_context_length: 32768
  max_output_tokens: 8192
  rate_limits:
    default: 30
  
  models:
    - "llama-3.3-70b-versatile"
    - "llama-3.1-8b-instant"
    - "llama-3.1-70b-versatile"
    - "mixtral-8x7b-32768"
    - "gemma2-9b-it"
  
  model_aliases:
    # Model aliases
    llama: "llama-3.3-70b-versatile"
    llama_fast: "llama-3.1-8b-instant"
    mixtral: "mixtral-8x7b-32768"
    # Speed aliases
    latest: "llama-3.3-70b-versatile"
    fast: "llama-3.1-8b-instant"
    powerful: "llama-3.3-70b-versatile"
  
  model_capabilities:
    # All Groq models have text + capabilities
    - pattern: ".*"
      features: [text, streaming, tools]
      max_context_length: 32768
      max_output_tokens: 8192

# Google Gemini
gemini:
  client_class: "chuk_llm.llm.providers.gemini_client.GeminiLLMClient"
  api_key_env: "GEMINI_API_KEY"
  default_model: "gemini-2.0-flash"
  
  # All Gemini models support text
  features: [text, streaming, tools, vision, json_mode, system_messages, reasoning]
  max_context_length: 1000000
  max_output_tokens: 8192
  rate_limits:
    default: 1500
  
  models:
    - "gemini-2.0-flash"
    - "gemini-1.5-pro"
    - "gemini-1.5-flash"
    - "gemini-1.5-flash-8b"
  
  model_aliases:
    # Simple aliases
    flash: "gemini-2.0-flash"
    pro: "gemini-1.5-pro"
    latest: "gemini-2.0-flash"
    # Reasoning aliases
    reasoning: "gemini-1.5-pro"
    thinking: "gemini-2.0-flash"
  
  model_capabilities:
    # All Gemini models have text + advanced capabilities
    - pattern: "gemini-.*"
      features: [text, streaming, tools, vision, json_mode, reasoning]
      max_context_length: 1000000
      max_output_tokens: 8192

# Mistral AI – official cloud with Magistral reasoning
# Mistral AI – fixed to use correct models for tools
mistral:
  client_class: "chuk_llm.llm.providers.mistral_client.MistralLLMClient"
  api_key_env: "MISTRAL_API_KEY"
  default_model: "mistral-medium-2505"  # CHANGED: Use tools-capable model as default
  
  # All Mistral models support text
  features: [text, streaming, tools, vision, system_messages, parallel_calls, reasoning]
  max_context_length: 128000
  max_output_tokens: 8192
  rate_limits:
    default: 1000
    premium: 5000
  
  models:
    # Premier models (June 2025)
    - "magistral-medium-2506"                    # Reasoning-only model
    - "magistral-small-2506"                     # Small reasoning model (free)
    - "mistral-medium-2505"                      # Frontier multimodal model with tools
    - "codestral-2501"                           # Latest coding model
    - "mistral-saba-2502"                        # Middle East/South Asia languages
    - "mistral-large-2411"                       # Top-tier reasoning model
    - "pixtral-large-2411"                       # Frontier multimodal model
    - "ministral-3b-2410"                        # Best edge model
    - "ministral-8b-2410"                        # Powerful edge model
    
    # Free models
    - "devstral-small-2505"                      # 24B code model (open source)
    - "mistral-small-2503"                       # Small with vision (v3.1)
    - "pixtral-12b-2409"                         # 12B vision model
  
  model_aliases:
    # Magistral (reasoning) aliases
    magistral: "magistral-medium-2506"
    magistral_medium: "magistral-medium-2506"
    magistral_small: "magistral-small-2506"
    reasoning: "magistral-medium-2506"
    reasoning_small: "magistral-small-2506"
    thinking: "magistral-medium-2506"
    
    # Size aliases (current generation) - UPDATED to use tools-capable models
    large: "mistral-large-2411"
    medium: "mistral-medium-2505"               # Tools-capable multimodal
    small: "mistral-small-2503"                 # Tools-capable with vision
    
    # Edge model aliases
    ministral: "ministral-8b-2410"
    
    # Specialized aliases
    pixtral: "pixtral-large-2411"
    pixtral_small: "pixtral-12b-2409"
    codestral: "codestral-2501"
    devstral: "devstral-small-2505"
    saba: "mistral-saba-2502"
    
    # Vision aliases - Point to tools+vision capable models
    vision: "pixtral-large-2411"
    vision_small: "pixtral-12b-2409"
    multimodal: "mistral-medium-2505"
    
    # Code aliases
    code: "codestral-2501"
    code_open: "devstral-small-2505"
    coding: "codestral-2501"
    
    # Tools aliases - ADDED: Point to tools-capable models
    tools: "mistral-medium-2505"
    tools_vision: "pixtral-large-2411"
    
    # Capability aliases - UPDATED to use multimodal model as latest
    latest: "mistral-medium-2505"              # CHANGED: Use tools-capable model
    default: "mistral-medium-2505"             # CHANGED: Use tools-capable model
  
  model_capabilities:
    # Magistral (reasoning) models - text + reasoning only (NO TOOLS)
    - pattern: "magistral-.*"
      features: [text, streaming, reasoning]
      max_context_length: 40960  # 40k tokens for reasoning
      max_output_tokens: 8192
    
    # Mistral Large 2411 - text + tools capabilities
    - pattern: "mistral-large-2411"
      features: [text, streaming, tools, parallel_calls, reasoning]
      max_context_length: 131072  # 128k
      max_output_tokens: 8192
    
    # Mistral Medium 2505 - text + multimodal + tools capabilities (FLAGSHIP)
    - pattern: "mistral-medium-2505"
      features: [text, streaming, tools, vision, multimodal, parallel_calls, reasoning]
      max_context_length: 131072  # 128k
      max_output_tokens: 8192
    
    # Mistral Small 2503 - text + vision + tools capabilities
    - pattern: "mistral-small-2503"
      features: [text, streaming, tools, vision, multimodal, reasoning]
      max_context_length: 131072  # 128k
      max_output_tokens: 4096
    
    # Pixtral models - text + vision + tools capabilities
    - pattern: "pixtral-.*"
      features: [text, streaming, tools, vision, multimodal, parallel_calls, reasoning]
      max_context_length: 131072  # 128k
      max_output_tokens: 8192
    
    # Codestral models - text + coding + tools capabilities
    - pattern: "codestral-.*"
      features: [text, streaming, tools, reasoning]
      max_context_length: 262144  # 256k for code
      max_output_tokens: 8192
    
    # Devstral - text + coding + tools capabilities
    - pattern: "devstral-.*"
      features: [text, streaming, tools, reasoning]
      max_context_length: 131072  # 128k
      max_output_tokens: 8192
    
    # Ministral - text + edge + tools capabilities
    - pattern: "ministral-.*"
      features: [text, streaming, tools, reasoning]
      max_context_length: 131072  # 128k
      max_output_tokens: 4096
    
    # Saba - text + multilingual + tools capabilities
    - pattern: "mistral-saba-.*"
      features: [text, streaming, tools, reasoning]
      max_context_length: 32768
      max_output_tokens: 4096

# Local Ollama daemon
ollama:
  client_class: "chuk_llm.llm.providers.ollama_client.OllamaLLMClient"
  api_base: "http://localhost:11434"
  default_model: "granite3.3"
  
  # All Ollama models support text
  features: [text, streaming, system_messages, reasoning]
  max_context_length: 8192
  max_output_tokens: 4096
  rate_limits: {}
  
  models:
    - "llama3.3"
    - "qwen3"
    - "granite3.3"
    - "mistral"
    - "gemma3"
    - "phi3"
    - "codellama"
  
  model_aliases:
    # Simple aliases
    llama: "llama3.3"
    qwen: "qwen3"
    granite: "granite3.3"
    mistral_local: "mistral"
    code: "codellama"
    phi: "phi3"
    # Capability aliases
    smart: "llama3.3"
    fast: "llama3.3"
    creative: "llama3.3"
    programming: "codellama"
    latest: "llama3.3"
    default: "llama3.3"
    reasoning: "llama3.3"
  
  model_capabilities:
    # Gemma 3 models - text + capabilities
    - pattern: "gemma.*"
      features: [text, streaming, tools]
      max_context_length: 8192
      max_output_tokens: 4096
    
    # Llama 3.3/3.2 models - text + capabilities
    - pattern: "llama3\\.[23].*"
      features: [text, streaming, tools]
      max_context_length: 32768
      max_output_tokens: 8192
    
    # Mistral local models - text + capabilities
    - pattern: "mistral.*"
      features: [text, streaming, tools]
      max_context_length: 32768
      max_output_tokens: 8192
    
    # Qwen models - text + capabilities
    - pattern: "qwen.*"
      features: [text, streaming, tools, reasoning]
      max_context_length: 32768
      max_output_tokens: 8192
    
    # CodeLlama models - text + coding capabilities
    - pattern: ".*codellama.*|.*code.*"
      features: [text, streaming]  # text + reasoning for code
      max_context_length: 16384
      max_output_tokens: 8192
    
    # Phi models - text + basic capabilities
    - pattern: "phi.*"
      features: [text, streaming]
      max_context_length: 4096
      max_output_tokens: 2048
    
    # Vision models - text + vision capabilities
    - pattern: "llama3\\.2.*vision.*"
      features: [text, streaming, tools, vision, multimodal]
      max_context_length: 8192
      max_output_tokens: 4096
    
    # Embedding models - NO text completion (specialized)
    - pattern: ".*embed.*|.*embedding.*"
      features: []
      max_context_length: 512
    
    # Granite models - text + capabilities
    - pattern: "granite.*"
      features: [text, streaming, tools, reasoning]
      max_context_length: 8192
      max_output_tokens: 4096

# IBM watsonx.ai
watsonx:
  client_class: "chuk_llm.llm.providers.watsonx_client.WatsonXLLMClient"
  api_key_env: "WATSONX_API_KEY"
  api_key_fallback_env: "IBM_CLOUD_API_KEY"
  watsonx_ai_url: "https://us-south.ml.cloud.ibm.com"
  default_model: "ibm/granite-3-3-8b-instruct"
  
  # All WatsonX models support text
  features: [text, streaming, system_messages, reasoning]
  max_context_length: 131072
  max_output_tokens: 4096
  rate_limits:
    default: 500
    enterprise: 2000
  
  models:
    # IBM Granite models (current)
    - "ibm/granite-3-3-8b-instruct"
    - "ibm/granite-3-2-8b-instruct"
    - "ibm/granite-3-8b-instruct"
    - "ibm/granite-3-2b-instruct"
    - "ibm/granite-vision-3-2-2b-instruct"
    
    # Meta Llama models (non-deprecated only)
    - "meta-llama/llama-4-scout-17b-16e-instruct"        # New - Free preview
    - "meta-llama/llama-4-maverick-17b-128e-instruct-fp8" # New - Paid
    - "meta-llama/llama-3-3-70b-instruct"                # Current
    - "meta-llama/llama-3-2-90b-vision-instruct"         # Vision model
    - "meta-llama/llama-3-2-11b-vision-instruct"         # Vision model
    - "meta-llama/llama-3-2-1b-instruct"                 # Small model
    - "meta-llama/llama-3-2-3b-instruct"                 # Small model
    - "meta-llama/llama-3-405b-instruct"                 # Large model
    
    # Mistral models (non-deprecated only)
    - "mistralai/mistral-medium-2505"                    # New
    - "mistralai/mistral-small-3-1-24b-instruct-2503"   # New
    - "mistralai/pixtral-12b"                            # Vision model
    - "mistralai/mistral-large-2"                        # Current large model
  
  model_aliases:
    # Granite aliases
    granite8b: "ibm/granite-3-3-8b-instruct"
    granite2b: "ibm/granite-3-2b-instruct"
    granite: "ibm/granite-3-3-8b-instruct"
    granite_vision: "ibm/granite-vision-3-2-2b-instruct"
    
    # Llama 4 aliases (new models)
    llama4_scout: "meta-llama/llama-4-scout-17b-16e-instruct"
    llama4_maverick: "meta-llama/llama-4-maverick-17b-128e-instruct-fp8"
    llama4: "meta-llama/llama-4-maverick-17b-128e-instruct-fp8"
    
    # Llama 3.x aliases
    llama3_3: "meta-llama/llama-3-3-70b-instruct"
    llama3_3_70b: "meta-llama/llama-3-3-70b-instruct"
    llama3_405b: "meta-llama/llama-3-405b-instruct"
    llama3_2_90b_vision: "meta-llama/llama-3-2-90b-vision-instruct"
    llama3_2_11b_vision: "meta-llama/llama-3-2-11b-vision-instruct"
    llama3_2_1b: "meta-llama/llama-3-2-1b-instruct"
    llama3_2_3b: "meta-llama/llama-3-2-3b-instruct"
    
    # Simple Llama aliases (Llama 4 as defaults)
    llama: "meta-llama/llama-4-maverick-17b-128e-instruct-fp8"
    llama_vision: "meta-llama/llama-4-maverick-17b-128e-instruct-fp8"
    
    # Mistral aliases
    mistral_medium: "mistralai/mistral-medium-2505"
    mistral_small: "mistralai/mistral-small-3-1-24b-instruct-2503"
    mistral_large: "mistralai/mistral-large-2"
    mistral: "mistralai/mistral-large-2"
    pixtral: "mistralai/pixtral-12b"
    mistral_vision: "mistralai/pixtral-12b"
    
    # Capability aliases
    latest: "ibm/granite-3-3-8b-instruct"
    vision: "ibm/granite-vision-3-2-2b-instruct"
    reasoning: "ibm/granite-3-3-8b-instruct"
  
  extra:
    project_id_env: "WATSONX_PROJECT_ID"
    space_id_env: "WATSONX_SPACE_ID"
  
  model_capabilities:
    # Llama 4 models - text + multimodal capabilities
    - pattern: "meta-llama/llama-4-.*"
      features: [text, streaming, tools, multimodal]
      max_context_length: 131072  # 128k as specified
      max_output_tokens: 4096
    
    # Llama 3.3 70B - text + capabilities
    - pattern: "meta-llama/llama-3-3-70b-instruct"
      features: [text, streaming, tools]
      max_context_length: 131072  # 128k
      max_output_tokens: 4096
    
    # Llama 3.2 Vision models - text + vision capabilities
    - pattern: "meta-llama/llama-3-2-.*vision-instruct"
      features: [text, streaming, tools, vision, multimodal]
      max_context_length: 131072  # 128k
      max_output_tokens: 4096
    
    # Llama 3.2 Small models - text + capabilities
    - pattern: "meta-llama/llama-3-2-[13]b-instruct"
      features: [text, streaming, tools]
      max_context_length: 131072  # 128k
      max_output_tokens: 4096
    
    # Llama 3 405B - text + advanced capabilities
    - pattern: "meta-llama/llama-3-405b-instruct"
      features: [text, streaming, tools, parallel_calls]
      max_context_length: 131072  # 128k
      max_output_tokens: 4096
    
    # IBM Granite family - text + capabilities
    - pattern: "ibm/granite-.*"
      features: [text, streaming, tools, reasoning]
      max_context_length: 131072
      max_output_tokens: 4096
    
    # IBM Granite Vision - text + vision capabilities
    - pattern: "ibm/granite-vision-.*"
      features: [text, streaming, tools, vision, multimodal]
      max_context_length: 131072
      max_output_tokens: 4096
    
    # Mistral Medium 2505 - text + multimodal capabilities
    - pattern: "mistralai/mistral-medium-2505"
      features: [text, streaming, tools, vision, multimodal, parallel_calls]
      max_context_length: 131072  # 128k
      max_output_tokens: 8192
    
    # Mistral Small 2503 - text + vision capabilities
    - pattern: "mistralai/mistral-small-3-1-24b-instruct-2503"
      features: [text, streaming, tools, vision, multimodal]
      max_context_length: 131072  # 128k
      max_output_tokens: 4096
    
    # Pixtral - text + vision capabilities
    - pattern: "mistralai/pixtral-12b"
      features: [text, streaming, tools, vision, multimodal]
      max_context_length: 131072  # 128k
      max_output_tokens: 4096
    
    # Mistral Large 2 - text + advanced capabilities
    - pattern: "mistralai/mistral-large-2"
      features: [text, streaming, tools, parallel_calls]
      max_context_length: 131072  # 128k
      max_output_tokens: 8192
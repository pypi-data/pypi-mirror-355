"""å¸‚åœºæ•°æ®æœåŠ¡æ¨¡å—ã€‚

æä¾›åŠ å¯†è´§å¸å¸‚åœºæ•°æ®è·å–ã€å¤„ç†å’Œå­˜å‚¨åŠŸèƒ½ã€‚
"""

import logging
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from contextlib import nullcontext
from datetime import datetime, timedelta
from decimal import Decimal
from pathlib import Path
from threading import Lock
from typing import Any, Dict, List, Optional
import threading

import pandas as pd
from rich.logging import RichHandler
from rich.progress import (
    BarColumn,
    Progress,
    SpinnerColumn,
    TextColumn,
    TimeElapsedColumn,
)

from cryptoservice.client import BinanceClientFactory
from cryptoservice.config import settings, RetryConfig
from cryptoservice.data import MarketDB
from cryptoservice.exceptions import (
    InvalidSymbolError,
    MarketDataFetchError,
)
from cryptoservice.interfaces import IMarketDataService
from cryptoservice.models import (
    DailyMarketTicker,
    Freq,
    HistoricalKlinesType,
    KlineMarketTicker,
    PerpetualMarketTicker,
    SortBy,
    SymbolTicker,
    UniverseConfig,
    UniverseDefinition,
    UniverseSnapshot,
    ErrorSeverity,
    IntegrityReport,
)
from cryptoservice.utils import DataConverter

# é…ç½® rich logger
logging.basicConfig(
    level=logging.INFO,
    format="%(message)s",
    handlers=[RichHandler(rich_tracebacks=True)],
)
logger = logging.getLogger(__name__)

cache_lock = Lock()


class RateLimitManager:
    """APIé¢‘ç‡é™åˆ¶ç®¡ç†å™¨"""

    def __init__(self, base_delay: float = 0.5):
        self.base_delay = base_delay
        self.current_delay = base_delay
        self.last_request_time = 0.0
        self.request_count = 0
        self.window_start_time = time.time()
        self.consecutive_errors = 0
        self.max_requests_per_minute = 1800  # ä¿å®ˆä¼°è®¡ï¼Œä½äºAPIé™åˆ¶
        self.lock = threading.Lock()

    def wait_before_request(self):
        """åœ¨è¯·æ±‚å‰ç­‰å¾…é€‚å½“çš„æ—¶é—´"""
        with self.lock:
            current_time = time.time()

            # é‡ç½®è®¡æ•°çª—å£ï¼ˆæ¯åˆ†é’Ÿï¼‰
            if current_time - self.window_start_time >= 60:
                self.request_count = 0
                self.window_start_time = current_time
                # å¦‚æœé•¿æ—¶é—´æ²¡æœ‰é”™è¯¯ï¼Œé€æ¸é™ä½å»¶è¿Ÿ
                if self.consecutive_errors == 0:
                    self.current_delay = max(self.base_delay, self.current_delay * 0.9)

                    # æ£€æŸ¥æ˜¯å¦æ¥è¿‘é¢‘ç‡é™åˆ¶
            requests_this_minute = self.request_count

            if requests_this_minute >= self.max_requests_per_minute * 0.8:  # è¾¾åˆ°80%é™åˆ¶æ—¶å¼€å§‹å‡é€Ÿ
                additional_delay = 2.0
                logger.warning(f"âš ï¸ æ¥è¿‘é¢‘ç‡é™åˆ¶ï¼Œå¢åŠ å»¶è¿Ÿ: {additional_delay}ç§’")
            else:
                additional_delay = 0

            # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´
            time_since_last = current_time - self.last_request_time
            total_delay = self.current_delay + additional_delay

            if time_since_last < total_delay:
                wait_time = total_delay - time_since_last
                if wait_time > 0.1:  # åªè®°å½•è¾ƒé•¿çš„ç­‰å¾…æ—¶é—´
                    logger.debug(f"ç­‰å¾… {wait_time:.2f}ç§’ (å½“å‰å»¶è¿Ÿ: {self.current_delay:.2f}ç§’)")
                time.sleep(wait_time)

            self.last_request_time = time.time()
            self.request_count += 1

    def handle_rate_limit_error(self):
        """å¤„ç†é¢‘ç‡é™åˆ¶é”™è¯¯"""
        with self.lock:
            self.consecutive_errors += 1

            # åŠ¨æ€å¢åŠ å»¶è¿Ÿ
            if self.consecutive_errors <= 3:
                self.current_delay = min(10.0, self.current_delay * 2)
                wait_time = 60  # ç­‰å¾…1åˆ†é’Ÿ
            elif self.consecutive_errors <= 6:
                self.current_delay = min(15.0, self.current_delay * 1.5)
                wait_time = 120  # ç­‰å¾…2åˆ†é’Ÿ
            else:
                self.current_delay = 20.0
                wait_time = 300  # ç­‰å¾…5åˆ†é’Ÿ

            logger.warning(
                f"ğŸš« é¢‘ç‡é™åˆ¶é”™è¯¯ #{self.consecutive_errors}ï¼Œç­‰å¾… {wait_time}ç§’ï¼Œè°ƒæ•´å»¶è¿Ÿè‡³ {self.current_delay:.2f}ç§’"
            )

            # é‡ç½®è¯·æ±‚è®¡æ•°
            self.request_count = 0
            self.window_start_time = time.time()

            return wait_time

    def handle_success(self):
        """å¤„ç†æˆåŠŸè¯·æ±‚"""
        with self.lock:
            if self.consecutive_errors > 0:
                self.consecutive_errors = max(0, self.consecutive_errors - 1)
                if self.consecutive_errors == 0:
                    logger.info(f"âœ… æ¢å¤æ­£å¸¸ï¼Œå½“å‰å»¶è¿Ÿ: {self.current_delay:.2f}ç§’")


class ExponentialBackoff:
    """æŒ‡æ•°é€€é¿å®ç°"""

    def __init__(self, config: RetryConfig):
        self.config = config
        self.attempt = 0

    def reset(self):
        """é‡ç½®é‡è¯•è®¡æ•°"""
        self.attempt = 0

    def wait(self) -> float:
        """è®¡ç®—å¹¶æ‰§è¡Œç­‰å¾…æ—¶é—´"""
        if self.attempt >= self.config.max_retries:
            raise Exception(f"è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°: {self.config.max_retries}")

        # è®¡ç®—åŸºç¡€å»¶è¿Ÿ
        delay = min(
            self.config.base_delay * (self.config.backoff_multiplier**self.attempt),
            self.config.max_delay,
        )

        # æ·»åŠ æŠ–åŠ¨ä»¥é¿å…æƒŠç¾¤æ•ˆåº”
        if self.config.jitter:
            delay *= 0.5 + random.random() * 0.5

        self.attempt += 1

        logger.debug(f"æŒ‡æ•°é€€é¿: ç¬¬{self.attempt}æ¬¡é‡è¯•, ç­‰å¾…{delay:.2f}ç§’")
        time.sleep(delay)

        return delay


class EnhancedErrorHandler:
    """å¢å¼ºé”™è¯¯å¤„ç†å™¨"""

    @staticmethod
    def classify_error(error: Exception) -> ErrorSeverity:
        """é”™è¯¯åˆ†ç±»"""
        error_str = str(error).lower()

        # APIé¢‘ç‡é™åˆ¶
        if any(
            keyword in error_str
            for keyword in [
                "too many requests",
                "rate limit",
                "429",
                "request limit",
                "-1003",
            ]
        ):
            return ErrorSeverity.MEDIUM

        # ç½‘ç»œç›¸å…³é”™è¯¯
        if any(keyword in error_str for keyword in ["connection", "timeout", "network", "dns", "socket"]):
            return ErrorSeverity.MEDIUM

        # æ— æ•ˆäº¤æ˜“å¯¹
        if any(keyword in error_str for keyword in ["invalid symbol", "symbol not found", "unknown symbol"]):
            return ErrorSeverity.LOW

        # æœåŠ¡å™¨é”™è¯¯
        if any(
            keyword in error_str
            for keyword in [
                "500",
                "502",
                "503",
                "504",
                "server error",
                "internal error",
            ]
        ):
            return ErrorSeverity.HIGH

        # è®¤è¯é”™è¯¯
        if any(keyword in error_str for keyword in ["unauthorized", "forbidden", "api key", "signature"]):
            return ErrorSeverity.CRITICAL

        # é»˜è®¤ä¸ºä¸­ç­‰ä¸¥é‡æ€§
        return ErrorSeverity.MEDIUM

    @staticmethod
    def should_retry(error: Exception, attempt: int, max_retries: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•"""
        severity = EnhancedErrorHandler.classify_error(error)

        if severity == ErrorSeverity.CRITICAL:
            return False

        if severity == ErrorSeverity.LOW and attempt > 1:
            return False

        return attempt < max_retries

    @staticmethod
    def get_recommended_action(error: Exception) -> str:
        """è·å–æ¨èå¤„ç†åŠ¨ä½œ"""
        severity = EnhancedErrorHandler.classify_error(error)
        error_str = str(error).lower()

        if severity == ErrorSeverity.CRITICAL:
            return "æ£€æŸ¥APIå¯†é’¥å’Œæƒé™è®¾ç½®"
        elif "rate limit" in error_str or "-1003" in error_str:
            return "é¢‘ç‡é™åˆ¶ï¼Œè‡ªåŠ¨è°ƒæ•´è¯·æ±‚é—´éš”"
        elif "connection" in error_str:
            return "æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œè€ƒè™‘ä½¿ç”¨ä»£ç†"
        elif "invalid symbol" in error_str:
            return "éªŒè¯äº¤æ˜“å¯¹æ˜¯å¦å­˜åœ¨å’Œå¯äº¤æ˜“"
        else:
            return "æ£€æŸ¥APIæ–‡æ¡£å’Œé”™è¯¯è¯¦æƒ…"

    @staticmethod
    def is_rate_limit_error(error: Exception) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé¢‘ç‡é™åˆ¶é”™è¯¯"""
        error_str = str(error).lower()
        return any(keyword in error_str for keyword in ["too many requests", "rate limit", "429", "-1003"])


class MarketDataService(IMarketDataService):
    """å¸‚åœºæ•°æ®æœåŠ¡å®ç°ç±»ã€‚"""

    def __init__(self, api_key: str, api_secret: str) -> None:
        """åˆå§‹åŒ–å¸‚åœºæ•°æ®æœåŠ¡ã€‚

        Args:
            api_key: ç”¨æˆ·APIå¯†é’¥
            api_secret: ç”¨æˆ·APIå¯†é’¥
        """
        self.client = BinanceClientFactory.create_client(api_key, api_secret)
        self.converter = DataConverter()
        self.db: MarketDB | None = None
        self.rate_limit_manager = RateLimitManager()

    def _validate_and_prepare_path(self, path: Path | str, is_file: bool = False, file_name: str | None = None) -> Path:
        """éªŒè¯å¹¶å‡†å¤‡è·¯å¾„ã€‚

        Args:
            path: è·¯å¾„å­—ç¬¦ä¸²æˆ–Pathå¯¹è±¡
            is_file: æ˜¯å¦ä¸ºæ–‡ä»¶è·¯å¾„
            file_name: æ–‡ä»¶å
        Returns:
            Path: éªŒè¯åçš„Pathå¯¹è±¡

        Raises:
            ValueError: è·¯å¾„ä¸ºç©ºæˆ–æ— æ•ˆæ—¶
        """
        if not path:
            raise ValueError("è·¯å¾„ä¸èƒ½ä¸ºç©ºï¼Œå¿…é¡»æ‰‹åŠ¨æŒ‡å®š")

        path_obj = Path(path)

        # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼Œç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
        if is_file:
            if path_obj.is_dir():
                path_obj = path_obj.joinpath(file_name) if file_name else path_obj
            else:
                path_obj.parent.mkdir(parents=True, exist_ok=True)
        else:
            # å¦‚æœæ˜¯ç›®å½•è·¯å¾„ï¼Œç¡®ä¿ç›®å½•å­˜åœ¨
            path_obj.mkdir(parents=True, exist_ok=True)

        return path_obj

    def get_symbol_ticker(self, symbol: str | None = None) -> SymbolTicker | list[SymbolTicker]:
        """è·å–å•ä¸ªæˆ–æ‰€æœ‰äº¤æ˜“å¯¹çš„è¡Œæƒ…æ•°æ®ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹åç§°

        Returns:
            SymbolTicker | list[SymbolTicker]: å•ä¸ªäº¤æ˜“å¯¹çš„è¡Œæƒ…æ•°æ®æˆ–æ‰€æœ‰äº¤æ˜“å¯¹çš„è¡Œæƒ…æ•°æ®
        """
        try:
            ticker = self.client.get_symbol_ticker(symbol=symbol)
            if not ticker:
                raise InvalidSymbolError(f"Invalid symbol: {symbol}")

            if isinstance(ticker, list):
                return [SymbolTicker.from_binance_ticker(t) for t in ticker]
            return SymbolTicker.from_binance_ticker(ticker)

        except Exception as e:
            logger.error(f"[red]Error fetching ticker for {symbol}: {e}[/red]")
            raise MarketDataFetchError(f"Failed to fetch ticker: {e}") from e

    def get_perpetual_symbols(self, only_trading: bool = True, quote_asset: str = "USDT") -> list[str]:
        """è·å–å½“å‰å¸‚åœºä¸Šæ‰€æœ‰æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹ã€‚

        Args:
            only_trading: æ˜¯å¦åªè¿”å›å½“å‰å¯äº¤æ˜“çš„äº¤æ˜“å¯¹
            quote_asset: åŸºå‡†èµ„äº§ï¼Œé»˜è®¤ä¸ºUSDTï¼Œåªè¿”å›ä»¥è¯¥èµ„äº§ç»“å°¾çš„äº¤æ˜“å¯¹

        Returns:
            list[str]: æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹åˆ—è¡¨
        """
        try:
            logger.info(f"è·å–å½“å‰æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹åˆ—è¡¨ï¼ˆç­›é€‰æ¡ä»¶ï¼š{quote_asset}ç»“å°¾ï¼‰")
            futures_info = self.client.futures_exchange_info()
            perpetual_symbols = [
                symbol["symbol"]
                for symbol in futures_info["symbols"]
                if symbol["contractType"] == "PERPETUAL"
                and (not only_trading or symbol["status"] == "TRADING")
                and symbol["symbol"].endswith(quote_asset)
            ]

            logger.info(f"æ‰¾åˆ° {len(perpetual_symbols)} ä¸ª{quote_asset}æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹")
            return perpetual_symbols

        except Exception as e:
            logger.error(f"[red]è·å–æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹å¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"è·å–æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹å¤±è´¥: {e}") from e

    def _date_to_timestamp_range(self, date: str) -> tuple[str, str]:
        """å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´æˆ³èŒƒå›´ï¼ˆå¼€å§‹å’Œç»“æŸï¼‰ã€‚

        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'

        Returns:
            tuple[str, str]: (å¼€å§‹æ—¶é—´æˆ³, ç»“æŸæ—¶é—´æˆ³)ï¼Œéƒ½æ˜¯æ¯«ç§’çº§æ—¶é—´æˆ³å­—ç¬¦ä¸²
            - å¼€å§‹æ—¶é—´æˆ³: å½“å¤©çš„ 00:00:00
            - ç»“æŸæ—¶é—´æˆ³: å½“å¤©çš„ 23:59:59
        """
        start_time = int(datetime.strptime(f"{date} 00:00:00", "%Y-%m-%d %H:%M:%S").timestamp() * 1000)
        end_time = int(datetime.strptime(f"{date} 23:59:59", "%Y-%m-%d %H:%M:%S").timestamp() * 1000)
        return str(start_time), str(end_time)

    def _date_to_timestamp_start(self, date: str) -> str:
        """å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºå½“å¤©å¼€å§‹çš„æ—¶é—´æˆ³ã€‚

        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'

        Returns:
            str: å½“å¤© 00:00:00 çš„æ¯«ç§’çº§æ—¶é—´æˆ³å­—ç¬¦ä¸²
        """
        timestamp = int(datetime.strptime(f"{date} 00:00:00", "%Y-%m-%d %H:%M:%S").timestamp() * 1000)
        return str(timestamp)

    def _date_to_timestamp_end(self, date: str) -> str:
        """å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºå½“å¤©ç»“æŸçš„æ—¶é—´æˆ³ã€‚

        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'

        Returns:
            str: å½“å¤© 23:59:59 çš„æ¯«ç§’çº§æ—¶é—´æˆ³å­—ç¬¦ä¸²
        """
        timestamp = int(datetime.strptime(f"{date} 23:59:59", "%Y-%m-%d %H:%M:%S").timestamp() * 1000)
        return str(timestamp)

    def check_symbol_exists_on_date(self, symbol: str, date: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šæ—¥æœŸæ˜¯å¦å­˜åœ¨è¯¥äº¤æ˜“å¯¹ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹åç§°
            date: æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'

        Returns:
            bool: æ˜¯å¦å­˜åœ¨è¯¥äº¤æ˜“å¯¹
        """
        try:
            # å°†æ—¥æœŸè½¬æ¢ä¸ºæ—¶é—´æˆ³èŒƒå›´
            start_time, end_time = self._date_to_timestamp_range(date)

            # å°è¯•è·å–è¯¥æ—¶é—´èŒƒå›´å†…çš„Kçº¿æ•°æ®
            klines = self.client.futures_klines(
                symbol=symbol,
                interval="1d",
                startTime=start_time,
                endTime=end_time,
                limit=1,
            )

            # å¦‚æœæœ‰æ•°æ®ï¼Œè¯´æ˜è¯¥æ—¥æœŸå­˜åœ¨è¯¥äº¤æ˜“å¯¹
            return bool(klines and len(klines) > 0)

        except Exception as e:
            logger.debug(f"æ£€æŸ¥äº¤æ˜“å¯¹ {symbol} åœ¨ {date} æ˜¯å¦å­˜åœ¨æ—¶å‡ºé”™: {e}")
            return False

    def get_top_coins(
        self,
        limit: int = settings.DEFAULT_LIMIT,
        sort_by: SortBy = SortBy.QUOTE_VOLUME,
        quote_asset: str | None = None,
    ) -> list[DailyMarketTicker]:
        """è·å–å‰Nä¸ªäº¤æ˜“å¯¹ã€‚

        Args:
            limit: æ•°é‡
            sort_by: æ’åºæ–¹å¼
            quote_asset: åŸºå‡†èµ„äº§

        Returns:
            list[DailyMarketTicker]: å‰Nä¸ªäº¤æ˜“å¯¹
        """
        try:
            tickers = self.client.get_ticker()
            market_tickers = [DailyMarketTicker.from_binance_ticker(t) for t in tickers]

            if quote_asset:
                market_tickers = [t for t in market_tickers if t.symbol.endswith(quote_asset)]

            return sorted(
                market_tickers,
                key=lambda x: getattr(x, sort_by.value),
                reverse=True,
            )[:limit]

        except Exception as e:
            logger.error(f"[red]Error getting top coins: {e}[/red]")
            raise MarketDataFetchError(f"Failed to get top coins: {e}") from e

    def get_market_summary(self, interval: Freq = Freq.d1) -> dict[str, Any]:
        """è·å–å¸‚åœºæ¦‚è§ˆã€‚

        Args:
            interval: æ—¶é—´é—´éš”

        Returns:
            dict[str, Any]: å¸‚åœºæ¦‚è§ˆ
        """
        try:
            summary: dict[str, Any] = {"snapshot_time": datetime.now(), "data": {}}
            tickers_result = self.get_symbol_ticker()
            if isinstance(tickers_result, list):
                tickers = [ticker.to_dict() for ticker in tickers_result]
            else:
                tickers = [tickers_result.to_dict()]
            summary["data"] = tickers

            return summary

        except Exception as e:
            logger.error(f"[red]Error getting market summary: {e}[/red]")
            raise MarketDataFetchError(f"Failed to get market summary: {e}") from e

    def get_historical_klines(
        self,
        symbol: str,
        start_time: str | datetime,
        end_time: str | datetime | None = None,
        interval: Freq = Freq.h1,
        klines_type: HistoricalKlinesType = HistoricalKlinesType.SPOT,
    ) -> list[KlineMarketTicker]:
        """è·å–å†å²è¡Œæƒ…æ•°æ®ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹åç§°
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ºå½“å‰æ—¶é—´
            interval: æ—¶é—´é—´éš”
            klines_type: Kçº¿ç±»å‹ï¼ˆç°è´§æˆ–æœŸè´§ï¼‰

        Returns:
            list[KlineMarketTicker]: å†å²è¡Œæƒ…æ•°æ®
        """
        try:
            # å¤„ç†æ—¶é—´æ ¼å¼
            if isinstance(start_time, str):
                start_time = datetime.fromisoformat(start_time)
            if end_time is None:
                end_time = datetime.now()
            elif isinstance(end_time, str):
                end_time = datetime.fromisoformat(end_time)

            # è½¬æ¢ä¸ºæ—¶é—´æˆ³
            start_ts = self._date_to_timestamp_start(start_time.strftime("%Y-%m-%d"))
            end_ts = self._date_to_timestamp_end(end_time.strftime("%Y-%m-%d"))

            logger.info(f"è·å– {symbol} çš„å†å²æ•°æ® ({interval.value})")

            # æ ¹æ®klines_typeé€‰æ‹©API
            if klines_type == HistoricalKlinesType.FUTURES:
                klines = self.client.futures_klines(
                    symbol=symbol,
                    interval=interval.value,
                    startTime=start_ts,
                    endTime=end_ts,
                    limit=1500,
                )
            else:  # SPOT
                klines = self.client.get_klines(
                    symbol=symbol,
                    interval=interval.value,
                    startTime=start_ts,
                    endTime=end_ts,
                    limit=1500,
                )

            data = list(klines)
            if not data:
                logger.warning(f"æœªæ‰¾åˆ°äº¤æ˜“å¯¹ {symbol} åœ¨æŒ‡å®šæ—¶é—´æ®µå†…çš„æ•°æ®")
                return []

            # è½¬æ¢ä¸ºKlineMarketTickerå¯¹è±¡
            return [
                KlineMarketTicker(
                    symbol=symbol,
                    last_price=Decimal(str(kline[4])),  # æ”¶ç›˜ä»·ä½œä¸ºæœ€æ–°ä»·æ ¼
                    open_price=Decimal(str(kline[1])),
                    high_price=Decimal(str(kline[2])),
                    low_price=Decimal(str(kline[3])),
                    volume=Decimal(str(kline[5])),
                    close_time=kline[6],
                )
                for kline in data
            ]

        except Exception as e:
            logger.error(f"[red]Error getting historical data for {symbol}: {e}[/red]")
            raise MarketDataFetchError(f"Failed to get historical data: {e}") from e

    def _fetch_symbol_data(
        self,
        symbol: str,
        start_ts: str,
        end_ts: str,
        interval: Freq,
        klines_type: HistoricalKlinesType = HistoricalKlinesType.FUTURES,
        retry_config: Optional[RetryConfig] = None,
    ) -> list[PerpetualMarketTicker]:
        """è·å–å•ä¸ªäº¤æ˜“å¯¹çš„æ•°æ® (å¢å¼ºç‰ˆ).

        Args:
            symbol: äº¤æ˜“å¯¹åç§°
            start_ts: å¼€å§‹æ—¶é—´æˆ³ (æ¯«ç§’)
            end_ts: ç»“æŸæ—¶é—´æˆ³ (æ¯«ç§’)
            interval: æ—¶é—´é—´éš”
            klines_type: è¡Œæƒ…ç±»å‹
            retry_config: é‡è¯•é…ç½®
        """
        if retry_config is None:
            retry_config = RetryConfig()

        backoff = ExponentialBackoff(retry_config)
        error_handler = EnhancedErrorHandler()

        while True:
            try:
                # æ•°æ®é¢„æ£€æŸ¥
                if start_ts and end_ts:
                    start_date = datetime.fromtimestamp(int(start_ts) / 1000).strftime("%Y-%m-%d")
                    logger.debug(f"è·å– {symbol} æ•°æ®: {start_date} ({start_ts} - {end_ts})")

                # é¢‘ç‡é™åˆ¶æ§åˆ¶ - åœ¨APIè°ƒç”¨å‰ç­‰å¾…
                self.rate_limit_manager.wait_before_request()

                # APIè°ƒç”¨
                klines = self.client.get_historical_klines_generator(
                    symbol=symbol,
                    interval=interval.value,
                    start_str=start_ts,
                    end_str=end_ts,
                    limit=1500,
                    klines_type=HistoricalKlinesType.to_binance(klines_type),
                )

                data = list(klines)
                if not data:
                    logger.debug(f"äº¤æ˜“å¯¹ {symbol} åœ¨æŒ‡å®šæ—¶é—´æ®µå†…æ— æ•°æ®")
                    self.rate_limit_manager.handle_success()
                    return []

                # æ•°æ®è´¨é‡æ£€æŸ¥
                valid_data = self._validate_kline_data(data, symbol)

                # è½¬æ¢ä¸ºå¯¹è±¡
                result = [
                    PerpetualMarketTicker(
                        symbol=symbol,
                        open_time=kline[0],
                        raw_data=kline,
                    )
                    for kline in valid_data
                ]

                logger.debug(f"æˆåŠŸè·å– {symbol}: {len(result)} æ¡è®°å½•")
                self.rate_limit_manager.handle_success()
                return result

            except Exception as e:
                severity = error_handler.classify_error(e)

                # ç‰¹æ®Šå¤„ç†é¢‘ç‡é™åˆ¶é”™è¯¯
                if error_handler.is_rate_limit_error(e):
                    wait_time = self.rate_limit_manager.handle_rate_limit_error()
                    logger.warning(f"ğŸš« é¢‘ç‡é™åˆ¶ - {symbol}ï¼Œç­‰å¾… {wait_time}ç§’åé‡è¯•")
                    time.sleep(wait_time)
                    # é‡ç½®é€€é¿è®¡æ•°ï¼Œå› ä¸ºè¿™ä¸æ˜¯çœŸæ­£çš„"é”™è¯¯"
                    backoff.reset()
                    continue

                # å¤„ç†ä¸å¯é‡è¯•çš„é”™è¯¯
                if severity == ErrorSeverity.CRITICAL:
                    logger.error(f"âŒ è‡´å‘½é”™è¯¯ - {symbol}: {e}")
                    logger.error(f"å»ºè®®: {error_handler.get_recommended_action(e)}")
                    raise e

                if "Invalid symbol" in str(e):
                    logger.warning(f"âš ï¸ æ— æ•ˆäº¤æ˜“å¯¹: {symbol}")
                    raise InvalidSymbolError(f"æ— æ•ˆçš„äº¤æ˜“å¯¹: {symbol}") from e

                # åˆ¤æ–­æ˜¯å¦é‡è¯•
                if not error_handler.should_retry(e, backoff.attempt, retry_config.max_retries):
                    logger.error(f"âŒ é‡è¯•å¤±è´¥ - {symbol}: {e}")
                    if severity == ErrorSeverity.LOW:
                        # å¯¹äºä½ä¸¥é‡æ€§é”™è¯¯ï¼Œè¿”å›ç©ºç»“æœè€Œä¸æŠ›å‡ºå¼‚å¸¸
                        return []
                    raise MarketDataFetchError(f"è·å–äº¤æ˜“å¯¹ {symbol} æ•°æ®å¤±è´¥: {e}") from e

                # æ‰§è¡Œé‡è¯•
                logger.warning(f"ğŸ”„ é‡è¯• {backoff.attempt + 1}/{retry_config.max_retries} - {symbol}: {e}")
                logger.info(f"ğŸ’¡ å»ºè®®: {error_handler.get_recommended_action(e)}")

                try:
                    backoff.wait()
                except Exception:
                    logger.error(f"âŒ è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•° - {symbol}")
                    raise MarketDataFetchError(f"è·å–äº¤æ˜“å¯¹ {symbol} æ•°æ®å¤±è´¥: è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°") from e

    def _validate_kline_data(self, data: List, symbol: str) -> List:
        """éªŒè¯Kçº¿æ•°æ®è´¨é‡"""
        if not data:
            return data

        valid_data = []
        issues = []

        for i, kline in enumerate(data):
            try:
                # æ£€æŸ¥æ•°æ®ç»“æ„
                if len(kline) < 8:
                    issues.append(f"è®°å½•{i}: æ•°æ®å­—æ®µä¸è¶³")
                    continue

                # æ£€æŸ¥ä»·æ ¼æ•°æ®æœ‰æ•ˆæ€§
                open_price = float(kline[1])
                high_price = float(kline[2])
                low_price = float(kline[3])
                close_price = float(kline[4])
                volume = float(kline[5])

                # åŸºç¡€é€»è¾‘æ£€æŸ¥
                if high_price < max(open_price, close_price, low_price):
                    issues.append(f"è®°å½•{i}: æœ€é«˜ä»·å¼‚å¸¸")
                    continue

                if low_price > min(open_price, close_price, high_price):
                    issues.append(f"è®°å½•{i}: æœ€ä½ä»·å¼‚å¸¸")
                    continue

                if volume < 0:
                    issues.append(f"è®°å½•{i}: æˆäº¤é‡ä¸ºè´Ÿ")
                    continue

                valid_data.append(kline)

            except (ValueError, IndexError) as e:
                issues.append(f"è®°å½•{i}: æ•°æ®æ ¼å¼é”™è¯¯ - {e}")
                continue

        if issues:
            issue_count = len(issues)
            total_count = len(data)
            if issue_count > total_count * 0.1:  # è¶…è¿‡10%çš„æ•°æ®æœ‰é—®é¢˜
                logger.warning(f"âš ï¸ {symbol} æ•°æ®è´¨é‡é—®é¢˜: {issue_count}/{total_count} æ¡è®°å½•å¼‚å¸¸")
                for issue in issues[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé—®é¢˜
                    logger.debug(f"   - {issue}")
                if len(issues) > 5:
                    logger.debug(f"   - ... è¿˜æœ‰ {len(issues) - 5} ä¸ªé—®é¢˜")

        return valid_data

    def _create_integrity_report(
        self,
        symbols: List[str],
        successful_symbols: List[str],
        failed_symbols: List[str],
        missing_periods: List[Dict[str, str]],
        start_time: str,
        end_time: str,
        interval: Freq,
        db_file_path: Path,
    ) -> IntegrityReport:
        """åˆ›å»ºæ•°æ®å®Œæ•´æ€§æŠ¥å‘Š"""
        try:
            if not self.db:
                raise ValueError("æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–")

            logger.info("ğŸ” æ‰§è¡Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥...")

            # è®¡ç®—åŸºç¡€æŒ‡æ ‡
            total_symbols = len(symbols)
            success_count = len(successful_symbols)
            basic_quality_score = success_count / total_symbols if total_symbols > 0 else 0

            recommendations = []
            detailed_issues = []

            # æ£€æŸ¥æˆåŠŸä¸‹è½½çš„æ•°æ®è´¨é‡ï¼ˆå¯¹äºæµ‹è¯•æ•°æ®é‡‡ç”¨å®½æ¾ç­–ç•¥ï¼‰
            quality_issues = 0
            sample_symbols = successful_symbols[: min(5, len(successful_symbols))]  # å‡å°‘æŠ½æ ·æ•°é‡

            # å¦‚æœæ˜¯å•æ—¥æµ‹è¯•æ•°æ®ï¼Œè·³è¿‡å®Œæ•´æ€§æ£€æŸ¥
            if start_time == end_time:
                logger.debug("æ£€æµ‹åˆ°å•æ—¥æµ‹è¯•æ•°æ®ï¼Œè·³è¿‡è¯¦ç»†å®Œæ•´æ€§æ£€æŸ¥")
                sample_symbols = []

            for symbol in sample_symbols:
                try:
                    # è¯»å–æ•°æ®è¿›è¡Œè´¨é‡æ£€æŸ¥
                    # ç¡®ä¿æ—¶é—´æ ¼å¼æ­£ç¡®
                    check_start_time = pd.to_datetime(start_time).strftime("%Y-%m-%d")
                    check_end_time = pd.to_datetime(end_time).strftime("%Y-%m-%d")

                    df = self.db.read_data(
                        start_time=check_start_time,
                        end_time=check_end_time,
                        freq=interval,
                        symbols=[symbol],
                        raise_on_empty=False,
                    )

                    if df is not None and not df.empty:
                        # æ£€æŸ¥æ•°æ®è¿ç»­æ€§
                        symbol_data = (
                            df.loc[symbol] if symbol in df.index.get_level_values("symbol") else pd.DataFrame()
                        )
                        if not symbol_data.empty:
                            # è®¡ç®—æœŸæœ›çš„æ•°æ®ç‚¹æ•°é‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                            time_diff = pd.to_datetime(check_end_time) - pd.to_datetime(check_start_time)
                            expected_points = self._calculate_expected_data_points(time_diff, interval)
                            actual_points = len(symbol_data)

                            completeness = actual_points / expected_points if expected_points > 0 else 0
                            if completeness < 0.8:  # å°‘äº80%è®¤ä¸ºæœ‰é—®é¢˜
                                quality_issues += 1
                                detailed_issues.append(
                                    f"{symbol}: æ•°æ®å®Œæ•´æ€§{completeness:.1%} ({actual_points}/{expected_points})"
                                )
                    else:
                        quality_issues += 1
                        detailed_issues.append(f"{symbol}: æ— æ³•è¯»å–å·²ä¸‹è½½çš„æ•°æ®")

                except Exception as e:
                    quality_issues += 1
                    detailed_issues.append(f"{symbol}: æ£€æŸ¥å¤±è´¥ - {e}")

            # è°ƒæ•´è´¨é‡åˆ†æ•°
            if successful_symbols:
                sample_size = min(10, len(successful_symbols))
                quality_penalty = (quality_issues / sample_size) * 0.3  # æœ€å¤šå‡å°‘30%åˆ†æ•°
                final_quality_score = max(0, basic_quality_score - quality_penalty)
            else:
                final_quality_score = 0

            # ç”Ÿæˆå»ºè®®
            if final_quality_score < 0.5:
                recommendations.append("ğŸš¨ æ•°æ®è´¨é‡ä¸¥é‡ä¸è¶³ï¼Œå»ºè®®é‡æ–°ä¸‹è½½")
            elif final_quality_score < 0.8:
                recommendations.append("âš ï¸ æ•°æ®è´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®æ£€æŸ¥å¤±è´¥çš„äº¤æ˜“å¯¹")
            else:
                recommendations.append("âœ… æ•°æ®è´¨é‡è‰¯å¥½")

            if failed_symbols:
                recommendations.append(f"ğŸ“ {len(failed_symbols)}ä¸ªäº¤æ˜“å¯¹ä¸‹è½½å¤±è´¥ï¼Œå»ºè®®å•ç‹¬é‡è¯•")
                if len(failed_symbols) <= 5:
                    recommendations.append(f"å¤±è´¥äº¤æ˜“å¯¹: {', '.join(failed_symbols)}")

            if quality_issues > 0:
                recommendations.append(f"âš ï¸ å‘ç°{quality_issues}ä¸ªæ•°æ®è´¨é‡é—®é¢˜")
                recommendations.extend(detailed_issues[:3])  # åªæ˜¾ç¤ºå‰3ä¸ªé—®é¢˜

            # ç½‘ç»œå’ŒAPIå»ºè®®
            if len(failed_symbols) > total_symbols * 0.3:
                recommendations.append("ğŸŒ å¤±è´¥ç‡è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé™åˆ¶")

            logger.info(f"âœ… å®Œæ•´æ€§æ£€æŸ¥å®Œæˆ: è´¨é‡åˆ†æ•° {final_quality_score:.1%}")

            return IntegrityReport(
                total_symbols=total_symbols,
                successful_symbols=success_count,
                failed_symbols=failed_symbols,
                missing_periods=missing_periods,
                data_quality_score=final_quality_score,
                recommendations=recommendations,
            )

        except Exception as e:
            logger.warning(f"âš ï¸ å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {e}")
            # è¿”å›åŸºç¡€æŠ¥å‘Š
            return IntegrityReport(
                total_symbols=len(symbols),
                successful_symbols=len(successful_symbols),
                failed_symbols=failed_symbols,
                missing_periods=missing_periods,
                data_quality_score=(len(successful_symbols) / len(symbols) if symbols else 0),
                recommendations=[f"å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {e}", "å»ºè®®æ‰‹åŠ¨éªŒè¯æ•°æ®è´¨é‡"],
            )

    def _calculate_expected_data_points(self, time_diff: timedelta, interval: Freq) -> int:
        """è®¡ç®—æœŸæœ›çš„æ•°æ®ç‚¹æ•°é‡"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šåŸºäºæ—¶é—´å·®å’Œé¢‘ç‡è®¡ç®—æœŸæœ›æ•°æ®ç‚¹
        total_minutes = time_diff.total_seconds() / 60

        interval_minutes = {
            Freq.m1: 1,
            Freq.m3: 3,
            Freq.m5: 5,
            Freq.m15: 15,
            Freq.m30: 30,
            Freq.h1: 60,
            Freq.h4: 240,
            Freq.d1: 1440,
        }.get(interval, 1)

        # ç¡®ä¿è‡³å°‘è¿”å›1ä¸ªæ•°æ®ç‚¹ï¼Œé¿å…é™¤é›¶é”™è¯¯
        expected_points = int(total_minutes / interval_minutes)
        return max(1, expected_points)

    def get_perpetual_data(
        self,
        symbols: list[str],
        start_time: str,
        db_path: Path | str,
        end_time: str | None = None,
        interval: Freq = Freq.h1,
        max_workers: int = 5,
        max_retries: int = 3,
        progress: Progress | None = None,
        request_delay: float = 0.5,
        # é¢å¤–å‚æ•°ï¼Œä¿æŒå‘åå…¼å®¹
        retry_config: Optional[RetryConfig] = None,
        enable_integrity_check: bool = True,
    ) -> IntegrityReport:
        """è·å–æ°¸ç»­åˆçº¦æ•°æ®å¹¶å­˜å‚¨ (å¢å¼ºç‰ˆ).

        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            start_time: å¼€å§‹æ—¶é—´ (YYYY-MM-DD)
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„ (å¿…é¡»æŒ‡å®šï¼Œå¦‚: /path/to/market.db)
            end_time: ç»“æŸæ—¶é—´ (YYYY-MM-DD)
            interval: æ—¶é—´é—´éš”
            max_workers: æœ€å¤§çº¿ç¨‹æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_config: é‡è¯•é…ç½®
            progress: è¿›åº¦æ˜¾ç¤ºå™¨
            enable_integrity_check: æ˜¯å¦å¯ç”¨å®Œæ•´æ€§æ£€æŸ¥
            request_delay: æ¯æ¬¡è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤0.5ç§’

        Returns:
            IntegrityReport: æ•°æ®å®Œæ•´æ€§æŠ¥å‘Š
        """
        if retry_config is None:
            retry_config = RetryConfig(max_retries=max_retries)

        # åˆå§‹åŒ–ç»“æœç»Ÿè®¡
        successful_symbols = []
        failed_symbols = []
        missing_periods = []

        try:
            if not symbols:
                raise ValueError("Symbols list cannot be empty")

            # éªŒè¯å¹¶å‡†å¤‡æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            db_file_path = self._validate_and_prepare_path(db_path, is_file=True)
            end_time = end_time or datetime.now().strftime("%Y-%m-%d")

            # å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´æˆ³
            start_ts = self._date_to_timestamp_start(start_time)
            end_ts = self._date_to_timestamp_end(end_time)

            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
            if self.db is None:
                self.db = MarketDB(str(db_file_path))

            # é‡æ–°åˆå§‹åŒ–é¢‘ç‡é™åˆ¶ç®¡ç†å™¨ï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„åŸºç¡€å»¶è¿Ÿ
            self.rate_limit_manager = RateLimitManager(base_delay=request_delay)

            logger.info(f"ğŸš€ å¼€å§‹ä¸‹è½½ {len(symbols)} ä¸ªäº¤æ˜“å¯¹çš„æ•°æ®")
            logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_time} åˆ° {end_time}")
            logger.info(f"âš™ï¸ é‡è¯•é…ç½®: æœ€å¤§{retry_config.max_retries}æ¬¡, åŸºç¡€å»¶è¿Ÿ{retry_config.base_delay}ç§’")
            logger.info(f"â±ï¸ æ™ºèƒ½é¢‘ç‡æ§åˆ¶: åŸºç¡€å»¶è¿Ÿ{request_delay}ç§’ï¼ŒåŠ¨æ€è°ƒæ•´")

            # åˆ›å»ºè¿›åº¦è·Ÿè¸ª
            if progress is None:
                progress = Progress(
                    SpinnerColumn(),
                    TextColumn("[progress.description]{task.description}"),
                    BarColumn(),
                    TimeElapsedColumn(),
                )

            def process_symbol(symbol: str) -> Dict[str, Any]:
                """å¤„ç†å•ä¸ªäº¤æ˜“å¯¹çš„æ•°æ®è·å– (å¢å¼ºç‰ˆ)"""
                result = {
                    "symbol": symbol,
                    "success": False,
                    "records": 0,
                    "error": None,
                }

                try:
                    data = self._fetch_symbol_data(
                        symbol=symbol,
                        start_ts=start_ts,
                        end_ts=end_ts,
                        interval=interval,
                        retry_config=retry_config,
                    )

                    if data:
                        if self.db is None:
                            raise MarketDataFetchError("Database is not initialized")

                        self.db.store_data(data, interval)
                        result.update(
                            {
                                "success": True,
                                "records": len(data),
                                "time_range": f"{data[0].open_time} - {data[-1].open_time}",
                            }
                        )
                        logger.debug(f"âœ… {symbol}: {len(data)} æ¡è®°å½•")
                        successful_symbols.append(symbol)
                    else:
                        result["error"] = "æ— æ•°æ®"
                        logger.debug(f"âš ï¸ {symbol}: æ— æ•°æ®")
                        missing_periods.append(
                            {
                                "symbol": symbol,
                                "period": f"{start_time} - {end_time}",
                                "reason": "no_data",
                            }
                        )

                except InvalidSymbolError as e:
                    result["error"] = f"æ— æ•ˆäº¤æ˜“å¯¹: {e}"
                    logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆäº¤æ˜“å¯¹ {symbol}")
                    failed_symbols.append(symbol)

                except Exception as e:
                    result["error"] = str(e)
                    logger.error(f"âŒ {symbol} å¤±è´¥: {e}")
                    failed_symbols.append(symbol)
                    missing_periods.append(
                        {
                            "symbol": symbol,
                            "period": f"{start_time} - {end_time}",
                            "reason": str(e),
                        }
                    )

                return result

            # æ‰§è¡Œå¹¶è¡Œä¸‹è½½
            results = []
            with progress if progress is not None else nullcontext():
                overall_task = progress.add_task("[cyan]ä¸‹è½½äº¤æ˜“å¯¹æ•°æ®", total=len(symbols)) if progress else None

                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = [executor.submit(process_symbol, symbol) for symbol in symbols]

                    for future in as_completed(futures):
                        try:
                            result = future.result()
                            results.append(result)

                            if progress and overall_task is not None:
                                progress.update(overall_task, advance=1)

                        except Exception as e:
                            logger.error(f"âŒ å¤„ç†å¼‚å¸¸: {e}")

            # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            total_records = sum(r.get("records", 0) for r in results)
            success_rate = len(successful_symbols) / len(symbols) if symbols else 0

            logger.info("ğŸ“Š ä¸‹è½½å®Œæˆç»Ÿè®¡:")
            logger.info(f"   âœ… æˆåŠŸ: {len(successful_symbols)}/{len(symbols)} ({success_rate:.1%})")
            logger.info(f"   âŒ å¤±è´¥: {len(failed_symbols)} ä¸ª")
            logger.info(f"   ğŸ“ˆ æ€»è®°å½•æ•°: {total_records:,} æ¡")
            logger.info(f"   ğŸ’¾ æ•°æ®åº“: {db_file_path}")

            # æ‰§è¡Œå®Œæ•´æ€§æ£€æŸ¥
            if enable_integrity_check and self.db:
                integrity_report = self._create_integrity_report(
                    symbols=symbols,
                    successful_symbols=successful_symbols,
                    failed_symbols=failed_symbols,
                    missing_periods=missing_periods,
                    start_time=start_time,
                    end_time=end_time,
                    interval=interval,
                    db_file_path=db_file_path,
                )
            else:
                # ç”ŸæˆåŸºç¡€æŠ¥å‘Š
                data_quality_score = len(successful_symbols) / len(symbols) if symbols else 0
                recommendations = []
                if data_quality_score < 0.8:
                    recommendations.append("æ•°æ®æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œå’ŒAPIé…ç½®")
                if failed_symbols:
                    recommendations.append(f"æœ‰{len(failed_symbols)}ä¸ªäº¤æ˜“å¯¹ä¸‹è½½å¤±è´¥ï¼Œå»ºè®®å•ç‹¬é‡è¯•")

                integrity_report = IntegrityReport(
                    total_symbols=len(symbols),
                    successful_symbols=len(successful_symbols),
                    failed_symbols=failed_symbols,
                    missing_periods=missing_periods,
                    data_quality_score=data_quality_score,
                    recommendations=recommendations,
                )

            return integrity_report

        except Exception as e:
            logger.error(f"âŒ æ•°æ®ä¸‹è½½å¤±è´¥: {e}")
            # å³ä½¿å¤±è´¥ä¹Ÿè¦è¿”å›æŠ¥å‘Š
            return IntegrityReport(
                total_symbols=len(symbols),
                successful_symbols=len(successful_symbols),
                failed_symbols=failed_symbols,
                missing_periods=missing_periods,
                data_quality_score=0.0,
                recommendations=[f"ä¸‹è½½å¤±è´¥: {e}", "æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®"],
            )

    def define_universe(
        self,
        start_date: str,
        end_date: str,
        t1_months: int,
        t2_months: int,
        t3_months: int,
        output_path: Path | str,
        top_k: int | None = None,
        top_ratio: float | None = None,
        description: str | None = None,
        delay_days: int = 7,
        api_delay_seconds: float = 1.0,
        batch_delay_seconds: float = 3.0,
        batch_size: int = 5,
        quote_asset: str = "USDT",
    ) -> UniverseDefinition:
        """å®šä¹‰universeå¹¶ä¿å­˜åˆ°æ–‡ä»¶.

        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD æˆ– YYYYMMDD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD æˆ– YYYYMMDD)
            t1_months: T1æ—¶é—´çª—å£ï¼ˆæœˆï¼‰ï¼Œç”¨äºè®¡ç®—mean daily amount
            t2_months: T2æ»šåŠ¨é¢‘ç‡ï¼ˆæœˆï¼‰ï¼Œuniverseé‡æ–°é€‰æ‹©çš„é¢‘ç‡
            t3_months: T3åˆçº¦æœ€å°åˆ›å»ºæ—¶é—´ï¼ˆæœˆï¼‰ï¼Œç”¨äºç­›é™¤æ–°åˆçº¦
            output_path: universeè¾“å‡ºæ–‡ä»¶è·¯å¾„ (å¿…é¡»æŒ‡å®š)
            top_k: é€‰å–çš„topåˆçº¦æ•°é‡ (ä¸ top_ratio äºŒé€‰ä¸€)
            top_ratio: é€‰å–çš„topåˆçº¦æ¯”ç‡ (ä¸ top_k äºŒé€‰ä¸€)
            description: æè¿°ä¿¡æ¯
            delay_days: åœ¨é‡æ–°å¹³è¡¡æ—¥æœŸå‰é¢å¤–å¾€å‰æ¨çš„å¤©æ•°ï¼Œé»˜è®¤7å¤©
            api_delay_seconds: æ¯ä¸ªAPIè¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°ï¼Œé»˜è®¤1.0ç§’
            batch_delay_seconds: æ¯æ‰¹æ¬¡è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°ï¼Œé»˜è®¤3.0ç§’
            batch_size: æ¯æ‰¹æ¬¡çš„è¯·æ±‚æ•°é‡ï¼Œé»˜è®¤5ä¸ª
            quote_asset: åŸºå‡†èµ„äº§ï¼Œé»˜è®¤ä¸ºUSDTï¼Œåªç­›é€‰ä»¥è¯¥èµ„äº§ç»“å°¾çš„äº¤æ˜“å¯¹

        Returns:
            UniverseDefinition: å®šä¹‰çš„universe
        """
        try:
            # éªŒè¯å¹¶å‡†å¤‡è¾“å‡ºè·¯å¾„
            output_path_obj = self._validate_and_prepare_path(
                output_path,
                is_file=True,
                file_name=(
                    f"universe_{start_date}_{end_date}_{t1_months}_{t2_months}_{t3_months}_{top_k or top_ratio}.json"
                ),
            )

            # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
            start_date = self._standardize_date_format(start_date)
            end_date = self._standardize_date_format(end_date)

            # åˆ›å»ºé…ç½®
            config = UniverseConfig(
                start_date=start_date,
                end_date=end_date,
                t1_months=t1_months,
                t2_months=t2_months,
                t3_months=t3_months,
                delay_days=delay_days,
                quote_asset=quote_asset,
                top_k=top_k,
                top_ratio=top_ratio,
            )

            logger.info(f"å¼€å§‹å®šä¹‰universe: {start_date} åˆ° {end_date}")
            log_selection_criteria = f"Top-K={top_k}" if top_k else f"Top-Ratio={top_ratio}"
            logger.info(f"å‚æ•°: T1={t1_months}æœˆ, T2={t2_months}æœˆ, T3={t3_months}æœˆ, {log_selection_criteria}")

            # ç”Ÿæˆé‡æ–°é€‰æ‹©æ—¥æœŸåºåˆ— (æ¯T2ä¸ªæœˆ)
            # ä»èµ·å§‹æ—¥æœŸå¼€å§‹ï¼Œæ¯éš”T2ä¸ªæœˆç”Ÿæˆé‡å¹³è¡¡æ—¥æœŸï¼Œè¡¨ç¤ºuniverseé‡æ–°é€‰æ‹©çš„æ—¶é—´ç‚¹
            rebalance_dates = self._generate_rebalance_dates(start_date, end_date, t2_months)

            logger.info("é‡å¹³è¡¡è®¡åˆ’:")
            logger.info(f"  - æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
            logger.info(f"  - é‡å¹³è¡¡é—´éš”: æ¯{t2_months}ä¸ªæœˆ")
            logger.info(f"  - æ•°æ®å»¶è¿Ÿ: {delay_days}å¤©")
            logger.info(f"  - T1æ•°æ®çª—å£: {t1_months}ä¸ªæœˆ")
            logger.info(f"  - é‡å¹³è¡¡æ—¥æœŸ: {rebalance_dates}")

            if not rebalance_dates:
                raise ValueError("æ— æ³•ç”Ÿæˆé‡å¹³è¡¡æ—¥æœŸï¼Œè¯·æ£€æŸ¥æ—¶é—´èŒƒå›´å’ŒT2å‚æ•°")

            # æ”¶é›†æ‰€æœ‰å‘¨æœŸçš„snapshots
            all_snapshots = []

            # åœ¨æ¯ä¸ªé‡æ–°é€‰æ‹©æ—¥æœŸè®¡ç®—universe
            for i, rebalance_date in enumerate(rebalance_dates):
                logger.info(f"å¤„ç†æ—¥æœŸ {i + 1}/{len(rebalance_dates)}: {rebalance_date}")

                # è®¡ç®—åŸºå‡†æ—¥æœŸï¼ˆé‡æ–°å¹³è¡¡æ—¥æœŸå‰delay_dayså¤©ï¼‰
                base_date = pd.to_datetime(rebalance_date) - timedelta(days=delay_days)
                calculated_t1_end = base_date.strftime("%Y-%m-%d")

                # è®¡ç®—T1å›çœ‹æœŸé—´çš„å¼€å§‹æ—¥æœŸï¼ˆä»base_dateå¾€å‰æ¨T1ä¸ªæœˆï¼‰
                calculated_t1_start = self._subtract_months(calculated_t1_end, t1_months)

                logger.info(
                    f"å‘¨æœŸ {i + 1}: åŸºå‡†æ—¥æœŸ={calculated_t1_end} (é‡æ–°å¹³è¡¡æ—¥æœŸå‰{delay_days}å¤©), "
                    f"T1æ•°æ®æœŸé—´={calculated_t1_start} åˆ° {calculated_t1_end}"
                )

                # è·å–ç¬¦åˆæ¡ä»¶çš„äº¤æ˜“å¯¹å’Œå®ƒä»¬çš„mean daily amount
                universe_symbols, mean_amounts = self._calculate_universe_for_date(
                    calculated_t1_start,
                    calculated_t1_end,
                    t3_months=t3_months,
                    top_k=top_k,
                    top_ratio=top_ratio,
                    api_delay_seconds=api_delay_seconds,
                    batch_delay_seconds=batch_delay_seconds,
                    batch_size=batch_size,
                    quote_asset=quote_asset,
                )

                # åˆ›å»ºè¯¥å‘¨æœŸçš„snapshot
                snapshot = UniverseSnapshot.create_with_dates_and_timestamps(
                    usage_t1_start=rebalance_date,  # å®é™…ä½¿ç”¨å¼€å§‹æ—¥æœŸ
                    usage_t1_end=min(
                        end_date,
                        (pd.to_datetime(rebalance_date) + pd.DateOffset(months=t1_months)).strftime("%Y-%m-%d"),
                    ),  # å®é™…ä½¿ç”¨ç»“æŸæ—¥æœŸ
                    calculated_t1_start=calculated_t1_start,  # è®¡ç®—å‘¨æœŸå¼€å§‹æ—¥æœŸ
                    calculated_t1_end=calculated_t1_end,  # è®¡ç®—å‘¨æœŸç»“æŸæ—¥æœŸï¼ˆåŸºå‡†æ—¥æœŸï¼‰
                    symbols=universe_symbols,
                    mean_daily_amounts=mean_amounts,
                    metadata={
                        "calculated_t1_start": calculated_t1_start,
                        "calculated_t1_end": calculated_t1_end,
                        "delay_days": delay_days,
                        "quote_asset": quote_asset,
                        "selected_symbols_count": len(universe_symbols),
                    },
                )

                all_snapshots.append(snapshot)

                logger.info(f"âœ… æ—¥æœŸ {rebalance_date}: é€‰æ‹©äº† {len(universe_symbols)} ä¸ªäº¤æ˜“å¯¹")

            # åˆ›å»ºå®Œæ•´çš„universeå®šä¹‰
            universe_def = UniverseDefinition(
                config=config,
                snapshots=all_snapshots,
                creation_time=datetime.now(),
                description=description,
            )

            # ä¿å­˜æ±‡æ€»çš„universeå®šä¹‰
            universe_def.save_to_file(output_path_obj)

            logger.info("ğŸ‰ Universeå®šä¹‰å®Œæˆï¼")
            logger.info(f"ğŸ“ åŒ…å« {len(all_snapshots)} ä¸ªé‡æ–°å¹³è¡¡å‘¨æœŸ")
            logger.info(f"ğŸ“‹ æ±‡æ€»æ–‡ä»¶: {output_path_obj}")

            return universe_def

        except Exception as e:
            logger.error(f"[red]å®šä¹‰universeå¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"å®šä¹‰universeå¤±è´¥: {e}") from e

    def _standardize_date_format(self, date_str: str) -> str:
        """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ä¸º YYYY-MM-DDã€‚"""
        if len(date_str) == 8:  # YYYYMMDD
            return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        return date_str

    def _generate_rebalance_dates(self, start_date: str, end_date: str, t2_months: int) -> list[str]:
        """ç”Ÿæˆé‡æ–°é€‰æ‹©universeçš„æ—¥æœŸåºåˆ—ã€‚

        ä»èµ·å§‹æ—¥æœŸå¼€å§‹ï¼Œæ¯éš”T2ä¸ªæœˆç”Ÿæˆé‡å¹³è¡¡æ—¥æœŸï¼Œè¿™äº›æ—¥æœŸè¡¨ç¤ºuniverseé‡æ–°é€‰æ‹©çš„æ—¶é—´ç‚¹ã€‚

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            t2_months: é‡æ–°å¹³è¡¡é—´éš”ï¼ˆæœˆï¼‰

        Returns:
            list[str]: é‡å¹³è¡¡æ—¥æœŸåˆ—è¡¨
        """
        dates = []
        start_date_obj = pd.to_datetime(start_date)
        end_date_obj = pd.to_datetime(end_date)

        # ä»èµ·å§‹æ—¥æœŸå¼€å§‹ï¼Œæ¯éš”T2ä¸ªæœˆç”Ÿæˆé‡å¹³è¡¡æ—¥æœŸ
        current_date = start_date_obj

        while current_date <= end_date_obj:
            dates.append(current_date.strftime("%Y-%m-%d"))
            current_date = current_date + pd.DateOffset(months=t2_months)

        return dates

    def _subtract_months(self, date_str: str, months: int) -> str:
        """ä»æ—¥æœŸå‡å»æŒ‡å®šæœˆæ•°ã€‚"""
        date_obj = pd.to_datetime(date_str)
        # ä½¿ç”¨pandasçš„DateOffsetæ¥æ­£ç¡®å¤„ç†æœˆä»½è¾¹ç•Œé—®é¢˜
        result_date = date_obj - pd.DateOffset(months=months)
        return str(result_date.strftime("%Y-%m-%d"))

    def _get_available_symbols_for_period(self, start_date: str, end_date: str, quote_asset: str = "USDT") -> list[str]:
        """è·å–æŒ‡å®šæ—¶é—´æ®µå†…å®é™…å¯ç”¨çš„æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹ã€‚

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            quote_asset: åŸºå‡†èµ„äº§ï¼Œç”¨äºç­›é€‰äº¤æ˜“å¯¹

        Returns:
            list[str]: åœ¨è¯¥æ—¶é—´æ®µå†…æœ‰æ•°æ®çš„äº¤æ˜“å¯¹åˆ—è¡¨
        """
        try:
            # å…ˆè·å–å½“å‰æ‰€æœ‰æ°¸ç»­åˆçº¦ä½œä¸ºå€™é€‰ï¼ˆç­›é€‰æŒ‡å®šçš„åŸºå‡†èµ„äº§ï¼‰
            candidate_symbols = self.get_perpetual_symbols(only_trading=True, quote_asset=quote_asset)
            logger.info(
                f"æ£€æŸ¥ {len(candidate_symbols)} ä¸ª{quote_asset}å€™é€‰äº¤æ˜“å¯¹åœ¨ {start_date} åˆ° {end_date} æœŸé—´çš„å¯ç”¨æ€§..."
            )

            available_symbols = []
            batch_size = 50
            for i in range(0, len(candidate_symbols), batch_size):
                batch = candidate_symbols[i : i + batch_size]
                for symbol in batch:
                    # æ£€æŸ¥åœ¨èµ·å§‹æ—¥æœŸæ˜¯å¦æœ‰æ•°æ®
                    if self.check_symbol_exists_on_date(symbol, start_date):
                        available_symbols.append(symbol)

                # æ˜¾ç¤ºè¿›åº¦
                processed = min(i + batch_size, len(candidate_symbols))
                logger.info(
                    f"å·²æ£€æŸ¥ {processed}/{len(candidate_symbols)} ä¸ªäº¤æ˜“å¯¹ï¼Œæ‰¾åˆ° {len(available_symbols)} ä¸ªå¯ç”¨äº¤æ˜“å¯¹"
                )

                # é¿å…APIé¢‘ç‡é™åˆ¶
                time.sleep(0.1)

            logger.info(
                f"åœ¨ {start_date} åˆ° {end_date} æœŸé—´æ‰¾åˆ° {len(available_symbols)} ä¸ªå¯ç”¨çš„{quote_asset}æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹"
            )
            return available_symbols

        except Exception as e:
            logger.warning(f"è·å–å¯ç”¨äº¤æ˜“å¯¹æ—¶å‡ºé”™: {e}")
            # å¦‚æœAPIæ£€æŸ¥å¤±è´¥ï¼Œè¿”å›å½“å‰æ‰€æœ‰æ°¸ç»­åˆçº¦
            return self.get_perpetual_symbols(only_trading=True, quote_asset=quote_asset)

    def _calculate_universe_for_date(
        self,
        calculated_t1_start: str,
        calculated_t1_end: str,
        t3_months: int,
        top_k: int | None = None,
        top_ratio: float | None = None,
        api_delay_seconds: float = 1.0,
        batch_delay_seconds: float = 3.0,
        batch_size: int = 5,
        quote_asset: str = "USDT",
    ) -> tuple[list[str], dict[str, float]]:
        """è®¡ç®—æŒ‡å®šæ—¥æœŸçš„universeã€‚

        Args:
            rebalance_date: é‡å¹³è¡¡æ—¥æœŸ
            t1_start_date: T1å¼€å§‹æ—¥æœŸ
            t3_months: T3æœˆæ•°
            top_k: é€‰æ‹©çš„topæ•°é‡
            top_ratio: é€‰æ‹©çš„topæ¯”ç‡
            api_delay_seconds: æ¯ä¸ªAPIè¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°
            batch_delay_seconds: æ¯æ‰¹æ¬¡è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°
            batch_size: æ¯æ‰¹æ¬¡çš„è¯·æ±‚æ•°é‡
            quote_asset: åŸºå‡†èµ„äº§ï¼Œç”¨äºç­›é€‰äº¤æ˜“å¯¹
        """
        try:
            # è·å–åœ¨è¯¥æ—¶é—´æ®µå†…å®é™…å­˜åœ¨çš„æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹
            actual_symbols = self._get_available_symbols_for_period(calculated_t1_start, calculated_t1_end, quote_asset)

            # ç­›é™¤æ–°åˆçº¦ (åˆ›å»ºæ—¶é—´ä¸è¶³T3ä¸ªæœˆçš„)
            cutoff_date = self._subtract_months(calculated_t1_end, t3_months)
            eligible_symbols = [
                symbol for symbol in actual_symbols if self._symbol_exists_before_date(symbol, cutoff_date)
            ]

            if not eligible_symbols:
                logger.warning(f"æ—¥æœŸ {calculated_t1_start} åˆ° {calculated_t1_end}: æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„äº¤æ˜“å¯¹")
                return [], {}

            # é€šè¿‡APIè·å–æ•°æ®è®¡ç®—mean daily amount
            mean_amounts = {}

            logger.info(f"å¼€å§‹é€šè¿‡APIè·å– {len(eligible_symbols)} ä¸ªäº¤æ˜“å¯¹çš„å†å²æ•°æ®...")

            # åˆå§‹åŒ–ä¸“é—¨ç”¨äºuniverseè®¡ç®—çš„é¢‘ç‡ç®¡ç†å™¨
            universe_rate_manager = RateLimitManager(base_delay=api_delay_seconds)

            for i, symbol in enumerate(eligible_symbols):
                try:
                    # å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´æˆ³
                    start_ts = self._date_to_timestamp_start(calculated_t1_start)
                    end_ts = self._date_to_timestamp_end(calculated_t1_end)

                    # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯10ä¸ªäº¤æ˜“å¯¹æ˜¾ç¤ºä¸€æ¬¡ï¼‰
                    if i % 10 == 0:
                        logger.info(f"å·²å¤„ç† {i}/{len(eligible_symbols)} ä¸ªäº¤æ˜“å¯¹...")

                    # ä¸´æ—¶ä½¿ç”¨è¿™ä¸ªé¢‘ç‡ç®¡ç†å™¨
                    original_manager = self.rate_limit_manager
                    self.rate_limit_manager = universe_rate_manager

                    try:
                        # è·å–å†å²Kçº¿æ•°æ®
                        klines = self._fetch_symbol_data(
                            symbol=symbol,
                            start_ts=start_ts,
                            end_ts=end_ts,
                            interval=Freq.d1,
                        )
                    finally:
                        # æ¢å¤åŸæ¥çš„é¢‘ç‡ç®¡ç†å™¨
                        self.rate_limit_manager = original_manager

                    if klines:
                        # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
                        expected_days = (
                            pd.to_datetime(calculated_t1_end) - pd.to_datetime(calculated_t1_start)
                        ).days + 1
                        actual_days = len(klines)

                        if actual_days < expected_days * 0.8:  # å…è®¸20%çš„æ•°æ®ç¼ºå¤±
                            logger.warning(f"äº¤æ˜“å¯¹ {symbol} æ•°æ®ä¸å®Œæ•´: æœŸæœ›{expected_days}å¤©ï¼Œå®é™…{actual_days}å¤©")

                        # è®¡ç®—å¹³å‡æ—¥æˆäº¤é¢
                        amounts = []
                        for kline in klines:
                            try:
                                # kline.raw_data[7] æ˜¯æˆäº¤é¢ï¼ˆUSDTï¼‰
                                if kline.raw_data and len(kline.raw_data) > 7:
                                    amount = float(kline.raw_data[7])
                                    amounts.append(amount)
                            except (ValueError, IndexError):
                                continue

                        if amounts:
                            mean_amount = sum(amounts) / len(amounts)
                            mean_amounts[symbol] = mean_amount
                        else:
                            logger.warning(f"äº¤æ˜“å¯¹ {symbol} åœ¨æœŸé—´å†…æ²¡æœ‰æœ‰æ•ˆçš„æˆäº¤é‡æ•°æ®")

                except Exception as e:
                    logger.warning(f"è·å– {symbol} æ•°æ®æ—¶å‡ºé”™ï¼Œè·³è¿‡: {e}")
                    continue

            # æŒ‰mean daily amountæ’åºå¹¶é€‰æ‹©top_kæˆ–top_ratio
            if mean_amounts:
                sorted_symbols = sorted(mean_amounts.items(), key=lambda x: x[1], reverse=True)

                if top_ratio is not None:
                    num_to_select = int(len(sorted_symbols) * top_ratio)
                elif top_k is not None:
                    num_to_select = top_k
                else:
                    # é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæ²¡æœ‰æä¾›top_kæˆ–top_ratioï¼Œåˆ™é€‰æ‹©æ‰€æœ‰
                    num_to_select = len(sorted_symbols)

                top_symbols = sorted_symbols[:num_to_select]

                universe_symbols = [symbol for symbol, _ in top_symbols]
                final_amounts = dict(top_symbols)

                # æ˜¾ç¤ºé€‰æ‹©ç»“æœ
                if len(universe_symbols) <= 10:
                    logger.info(f"é€‰ä¸­çš„äº¤æ˜“å¯¹: {universe_symbols}")
                else:
                    logger.info(f"Top 5: {universe_symbols[:5]}")
                    logger.info("å®Œæ•´åˆ—è¡¨å·²ä¿å­˜åˆ°æ–‡ä»¶ä¸­")
            else:
                # å¦‚æœæ²¡æœ‰å¯ç”¨æ•°æ®ï¼Œè¿”å›ç©º
                universe_symbols = []
                final_amounts = {}
                logger.warning("æ— æ³•é€šè¿‡APIè·å–æ•°æ®ï¼Œè¿”å›ç©ºçš„universe")

            return universe_symbols, final_amounts

        except Exception as e:
            logger.error(f"è®¡ç®—æ—¥æœŸ {calculated_t1_start} åˆ° {calculated_t1_end} çš„universeæ—¶å‡ºé”™: {e}")
            return [], {}

    def _symbol_exists_before_date(self, symbol: str, cutoff_date: str) -> bool:
        """æ£€æŸ¥äº¤æ˜“å¯¹æ˜¯å¦åœ¨æŒ‡å®šæ—¥æœŸä¹‹å‰å°±å­˜åœ¨ã€‚"""
        try:
            # æ£€æŸ¥åœ¨cutoff_dateä¹‹å‰æ˜¯å¦æœ‰æ•°æ®
            # è¿™é‡Œæˆ‘ä»¬æ£€æŸ¥cutoff_dateå‰ä¸€å¤©çš„æ•°æ®
            check_date = (pd.to_datetime(cutoff_date) - timedelta(days=1)).strftime("%Y-%m-%d")
            return self.check_symbol_exists_on_date(symbol, check_date)
        except Exception:
            # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œé»˜è®¤è®¤ä¸ºå­˜åœ¨
            return True

    def download_universe_data(
        self,
        universe_file: Path | str,
        db_path: Path | str,
        data_path: Path | str | None = None,
        interval: Freq = Freq.m1,
        max_workers: int = 4,
        max_retries: int = 3,
        include_buffer_days: int = 7,
        retry_config: RetryConfig | None = None,
        request_delay: float = 0.5,  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
    ) -> None:
        """æŒ‰å‘¨æœŸåˆ†åˆ«ä¸‹è½½universeæ•°æ®ï¼ˆæ›´ç²¾ç¡®çš„ä¸‹è½½æ–¹å¼ï¼‰ã€‚

        è¿™ç§æ–¹å¼ä¸ºæ¯ä¸ªé‡å¹³è¡¡å‘¨æœŸå•ç‹¬ä¸‹è½½æ•°æ®ï¼Œå¯ä»¥é¿å…ä¸‹è½½ä¸å¿…è¦çš„æ•°æ®ã€‚

        Args:
            universe_file: universeå®šä¹‰æ–‡ä»¶è·¯å¾„ (å¿…é¡»æŒ‡å®š)
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„ (å¿…é¡»æŒ‡å®šï¼Œå¦‚: /path/to/market.db)
            data_path: æ•°æ®æ–‡ä»¶å­˜å‚¨è·¯å¾„ (å¯é€‰ï¼Œç”¨äºå­˜å‚¨å…¶ä»–æ•°æ®æ–‡ä»¶)
            interval: æ•°æ®é¢‘ç‡
            max_workers: å¹¶å‘çº¿ç¨‹æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            include_buffer_days: ç¼“å†²å¤©æ•°
            request_delay: æ¯æ¬¡è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤0.5ç§’
        """
        try:
            # éªŒè¯è·¯å¾„
            universe_file_obj = self._validate_and_prepare_path(universe_file, is_file=True)
            db_file_path = self._validate_and_prepare_path(db_path, is_file=True)

            # data_pathæ˜¯å¯é€‰çš„ï¼Œå¦‚æœæä¾›åˆ™éªŒè¯
            data_path_obj = None
            if data_path:
                data_path_obj = self._validate_and_prepare_path(data_path, is_file=False)

            # æ£€æŸ¥universeæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not universe_file_obj.exists():
                raise FileNotFoundError(f"Universeæ–‡ä»¶ä¸å­˜åœ¨: {universe_file_obj}")

            # åŠ è½½universeå®šä¹‰
            universe_def = UniverseDefinition.load_from_file(universe_file_obj)

            logger.info("ğŸ“Š æŒ‰å‘¨æœŸä¸‹è½½æ•°æ®:")
            logger.info(f"   - æ€»å¿«ç…§æ•°: {len(universe_def.snapshots)}")
            logger.info(f"   - æ•°æ®é¢‘ç‡: {interval.value}")
            logger.info(f"   - å¹¶å‘çº¿ç¨‹: {max_workers}")
            logger.info(f"   - è¯·æ±‚é—´éš”: {request_delay}ç§’")
            logger.info(f"   - æ•°æ®åº“è·¯å¾„: {db_file_path}")
            if data_path_obj:
                logger.info(f"   - æ•°æ®æ–‡ä»¶è·¯å¾„: {data_path_obj}")

            # ä¸ºæ¯ä¸ªå‘¨æœŸå•ç‹¬ä¸‹è½½æ•°æ®
            for i, snapshot in enumerate(universe_def.snapshots):
                logger.info(f"ğŸ“… å¤„ç†å¿«ç…§ {i + 1}/{len(universe_def.snapshots)}: {snapshot.effective_date}")

                logger.info(f"   - äº¤æ˜“å¯¹æ•°é‡: {len(snapshot.symbols)}")
                logger.info(
                    f"   - è®¡ç®—æœŸé—´: {snapshot.calculated_t1_start} åˆ° {snapshot.calculated_t1_end} (å®šä¹‰universe)"
                )
                logger.info(f"   - ä½¿ç”¨æœŸé—´: {snapshot.start_date} åˆ° {snapshot.end_date} (å®é™…ä½¿ç”¨)")
                logger.info(
                    f"   - ä¸‹è½½èŒƒå›´: {snapshot.start_date} åˆ° {snapshot.end_date} (å«{include_buffer_days}å¤©ç¼“å†²)"
                )

                # ä¸‹è½½è¯¥å‘¨æœŸçš„ä½¿ç”¨æœŸé—´æ•°æ®
                self.get_perpetual_data(
                    symbols=snapshot.symbols,
                    start_time=snapshot.start_date,
                    end_time=snapshot.end_date,
                    db_path=db_file_path,
                    interval=interval,
                    max_workers=max_workers,
                    max_retries=max_retries,
                    retry_config=retry_config,
                    enable_integrity_check=True,
                    request_delay=request_delay,
                )

                logger.info(f"   âœ… å¿«ç…§ {snapshot.effective_date} ä¸‹è½½å®Œæˆ")

            logger.info("ğŸ‰ æ‰€æœ‰universeæ•°æ®ä¸‹è½½å®Œæˆ!")
            logger.info(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°: {db_file_path}")

        except Exception as e:
            logger.error(f"[red]æŒ‰å‘¨æœŸä¸‹è½½universeæ•°æ®å¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"æŒ‰å‘¨æœŸä¸‹è½½universeæ•°æ®å¤±è´¥: {e}") from e

    def _analyze_universe_data_requirements(
        self,
        universe_def: UniverseDefinition,
        buffer_days: int = 7,
        extend_to_present: bool = True,
    ) -> dict[str, Any]:
        """åˆ†æuniverseæ•°æ®ä¸‹è½½éœ€æ±‚ã€‚

        æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•è®¡ç®—æ€»ä½“èŒƒå›´ï¼Œä½†å®é™…ä¸‹è½½åº”è¯¥ä½¿ç”¨å„å¿«ç…§çš„ä½¿ç”¨æœŸé—´ã€‚
        æ¨èä½¿ç”¨ download_universe_data_by_periods æ–¹æ³•è¿›è¡Œç²¾ç¡®ä¸‹è½½ã€‚

        Args:
            universe_def: Universeå®šä¹‰
            buffer_days: ç¼“å†²å¤©æ•°
            extend_to_present: æ˜¯å¦æ‰©å±•åˆ°å½“å‰æ—¥æœŸ

        Returns:
            Dict: ä¸‹è½½è®¡åˆ’è¯¦æƒ…
        """
        import pandas as pd

        # æ”¶é›†æ‰€æœ‰çš„äº¤æ˜“å¯¹å’Œå®é™…ä½¿ç”¨æ—¶é—´èŒƒå›´
        all_symbols = set()
        usage_dates = []  # ä½¿ç”¨æœŸé—´çš„æ—¥æœŸ
        calculation_dates = []  # è®¡ç®—æœŸé—´çš„æ—¥æœŸ

        for snapshot in universe_def.snapshots:
            all_symbols.update(snapshot.symbols)

            # ä½¿ç”¨æœŸé—´ - å®é™…éœ€è¦ä¸‹è½½çš„æ•°æ®
            usage_dates.extend(
                [
                    snapshot.start_date,  # å®é™…ä½¿ç”¨å¼€å§‹
                    snapshot.end_date,  # å®é™…ä½¿ç”¨ç»“æŸ
                ]
            )

            # è®¡ç®—æœŸé—´ - ç”¨äºå®šä¹‰universeçš„æ•°æ®
            calculation_dates.extend(
                [
                    snapshot.calculated_t1_start,
                    snapshot.calculated_t1_end,
                    snapshot.effective_date,
                ]
            )

        # è®¡ç®—æ€»ä½“æ—¶é—´èŒƒå›´ - åŸºäºä½¿ç”¨æœŸé—´è€Œä¸æ˜¯è®¡ç®—æœŸé—´
        start_date = pd.to_datetime(min(usage_dates)) - timedelta(days=buffer_days)
        end_date = pd.to_datetime(max(usage_dates)) + timedelta(days=buffer_days)

        if extend_to_present:
            end_date = max(end_date, pd.to_datetime("today"))

        # æ·»åŠ æ›´å¤šè¯¦ç»†ä¿¡æ¯
        return {
            "unique_symbols": sorted(all_symbols),
            "total_symbols": len(all_symbols),
            "overall_start_date": start_date.strftime("%Y-%m-%d"),
            "overall_end_date": end_date.strftime("%Y-%m-%d"),
            "usage_period_start": pd.to_datetime(min(usage_dates)).strftime("%Y-%m-%d"),
            "usage_period_end": pd.to_datetime(max(usage_dates)).strftime("%Y-%m-%d"),
            "calculation_period_start": pd.to_datetime(min(calculation_dates)).strftime("%Y-%m-%d"),
            "calculation_period_end": pd.to_datetime(max(calculation_dates)).strftime("%Y-%m-%d"),
            "snapshots_count": len(universe_def.snapshots),
            "note": "æ¨èä½¿ç”¨download_universe_data_by_periodsæ–¹æ³•è¿›è¡Œç²¾ç¡®ä¸‹è½½",
        }

    def _verify_universe_data_integrity(
        self,
        universe_def: UniverseDefinition,
        db_path: Path,
        interval: Freq,
        download_plan: dict[str, Any],
    ) -> None:
        """éªŒè¯ä¸‹è½½çš„universeæ•°æ®å®Œæ•´æ€§ã€‚

        Args:
            universe_def: Universeå®šä¹‰
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            interval: æ•°æ®é¢‘ç‡
            download_plan: ä¸‹è½½è®¡åˆ’
        """
        try:
            from cryptoservice.data import MarketDB

            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ - ç›´æ¥ä½¿ç”¨æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            db = MarketDB(str(db_path))

            logger.info("ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
            incomplete_symbols: list[str] = []
            missing_data: list[dict[str, str]] = []
            successful_snapshots = 0

            for snapshot in universe_def.snapshots:
                try:
                    # æ£€æŸ¥è¯¥å¿«ç…§çš„ä¸»è¦äº¤æ˜“å¯¹æ•°æ®ï¼ŒåŸºäºä½¿ç”¨æœŸé—´éªŒè¯
                    # ä½¿ç”¨æ‰©å±•çš„æ—¶é—´èŒƒå›´ä»¥ç¡®ä¿èƒ½å¤Ÿæ‰¾åˆ°æ•°æ®
                    usage_start = pd.to_datetime(snapshot.start_date) - timedelta(days=3)
                    usage_end = pd.to_datetime(snapshot.end_date) + timedelta(days=3)

                    df = db.read_data(
                        symbols=snapshot.symbols[:3],  # åªæ£€æŸ¥å‰3ä¸ªä¸»è¦äº¤æ˜“å¯¹
                        start_time=usage_start.strftime("%Y-%m-%d"),
                        end_time=usage_end.strftime("%Y-%m-%d"),
                        freq=interval,
                        raise_on_empty=False,  # ä¸åœ¨æ²¡æœ‰æ•°æ®æ—¶æŠ›å‡ºå¼‚å¸¸
                    )

                    if df is not None and not df.empty:
                        # æ£€æŸ¥æ•°æ®è¦†ç›–çš„äº¤æ˜“å¯¹æ•°é‡
                        available_symbols = df.index.get_level_values("symbol").unique()
                        missing_symbols = set(snapshot.symbols[:3]) - set(available_symbols)
                        if missing_symbols:
                            incomplete_symbols.extend(missing_symbols)
                            logger.debug(f"å¿«ç…§ {snapshot.effective_date}ç¼ºå°‘äº¤æ˜“å¯¹: {list(missing_symbols)}")
                        else:
                            successful_snapshots += 1
                            logger.debug(f"å¿«ç…§ {snapshot.effective_date} éªŒè¯æˆåŠŸ")
                    else:
                        logger.debug(f"å¿«ç…§ {snapshot.effective_date} åœ¨æ‰©å±•æ—¶é—´èŒƒå›´å†…æœªæ‰¾åˆ°æ•°æ®")
                        missing_data.append(
                            {
                                "snapshot_date": snapshot.effective_date,
                                "error": "No data in extended time range",
                            }
                        )

                except Exception as e:
                    logger.debug(f"éªŒè¯å¿«ç…§ {snapshot.effective_date} æ—¶å‡ºé”™: {e}")
                    # ä¸å†è®°å½•ä¸ºä¸¥é‡é”™è¯¯ï¼Œåªæ˜¯è®°å½•è°ƒè¯•ä¿¡æ¯
                    missing_data.append({"snapshot_date": snapshot.effective_date, "error": str(e)})

            # æŠ¥å‘ŠéªŒè¯ç»“æœ - æ›´å‹å¥½çš„æŠ¥å‘Šæ–¹å¼
            total_snapshots = len(universe_def.snapshots)
            success_rate = successful_snapshots / total_snapshots if total_snapshots > 0 else 0

            logger.info("âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯å®Œæˆ")
            logger.info(f"   - å·²ä¸‹è½½äº¤æ˜“å¯¹: {download_plan['total_symbols']} ä¸ª")
            logger.info(f"   - æ—¶é—´èŒƒå›´: {download_plan['overall_start_date']} åˆ° {download_plan['overall_end_date']}")
            logger.info(f"   - æ•°æ®é¢‘ç‡: {interval.value}")
            logger.info(f"   - æˆåŠŸéªŒè¯å¿«ç…§: {successful_snapshots}/{total_snapshots} ({success_rate:.1%})")

            # åªæœ‰åœ¨æˆåŠŸç‡å¾ˆä½æ—¶æ‰ç»™å‡ºè­¦å‘Š
            if success_rate < 0.5:
                logger.warning(f"âš ï¸ éªŒè¯æˆåŠŸç‡è¾ƒä½: {success_rate:.1%}")
                if incomplete_symbols:
                    unique_incomplete = set(incomplete_symbols)
                    logger.warning(f"   - æ•°æ®ä¸å®Œæ•´çš„äº¤æ˜“å¯¹: {len(unique_incomplete)} ä¸ª")
                    if len(unique_incomplete) <= 5:
                        logger.warning(f"   - å…·ä½“äº¤æ˜“å¯¹: {list(unique_incomplete)}")

                if missing_data:
                    logger.warning(f"   - æ— æ³•éªŒè¯çš„å¿«ç…§: {len(missing_data)} ä¸ª")
            else:
                logger.info("ğŸ“Š æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå»ºè®®è¿›è¡Œåç»­åˆ†æ")

        except Exception as e:
            logger.warning(f"æ•°æ®å®Œæ•´æ€§éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œä½†ä¸å½±å“æ•°æ®ä½¿ç”¨: {e}")
            logger.info("ğŸ’¡ æç¤º: éªŒè¯å¤±è´¥ä¸ä»£è¡¨æ•°æ®ä¸‹è½½å¤±è´¥ï¼Œå¯ä»¥å°è¯•æŸ¥è¯¢å…·ä½“æ•°æ®è¿›è¡Œç¡®è®¤")

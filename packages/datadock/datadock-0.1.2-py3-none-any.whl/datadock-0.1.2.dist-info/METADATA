Metadata-Version: 2.1
Name: datadock
Version: 0.1.2
Summary: Datadock is a PySpark-based data interoperability library. It automatically detects schemas from heterogeneous files (CSV, JSON, Parquet), groups them by structural similarity, and performs standardized batch reads. Designed for pipelines handling non-uniform large-scale data, enabling robust integration and reuse in distributed environments.
Author: Otavio Oliveira
Author-email: datadock.sup@gmail.com
Requires-Python: >=3.10,<4.0
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Requires-Dist: loguru (>=0.7.3,<0.8.0)
Requires-Dist: pyarrow (>=20.0.0,<21.0.0)
Requires-Dist: pyspark (>=3.5.5,<4.0.0)
Description-Content-Type: text/markdown

# Datadock

**Datadock** is a Python library built on top of PySpark, designed to simplify **data interoperability** between files of different formats and schemas in modern data engineering pipelines.

It automatically detects schemas from CSV, JSON and Parquet files, groups structurally similar files, and allows standardized reading of all grouped files into a single Spark DataFrame â€” even in highly heterogeneous datasets.


## âœ¨ Key Features

- ðŸš€ **Automatic parsing** of multiple file formats: `.csv`, `.json`, `.parquet`
- ðŸ§  **Schema-based file grouping** by structural similarity
- ðŸ“Š **Auto-selection of dominant schemas**
- ðŸ› ï¸ **Unified read** across similar files into a single PySpark DataFrame
- ðŸ” **Schema insight** for diagnostics and inspection


## ðŸ”§ Installation

```bash
pip install datadock
```


## ðŸ—‚ï¸ Expected Input Structure

Place your data files (CSV, JSON or Parquet) inside a single folder. The library will automatically detect supported files and organize them by schema similarity.

```bash
/data/input/
â”œâ”€â”€ sales_2020.csv
â”œâ”€â”€ sales_2021.csv
â”œâ”€â”€ products.json
â”œâ”€â”€ archive.parquet
â”œâ”€â”€ log.parquet
```


## ðŸ§ª Usage Example

```python
from datadock import scan_schema, get_schema_info, read_data

path = "/path/to/your/data"

# Logs schema groups detected
scan_schema(path)

# Retrieves schema metadata
info = get_schema_info(path)
print(info)

# Loads all files from schema group 1
df = read_data(path, schema_id=1, logs=True)
df.show()
```


## ðŸ“Œ Public API

### `scan_schema`
Logs the identified schema groups found in the specified folder.


### `get_schema_info`
Returns a list of dictionaries containing:
- `schema_id`: ID of the schema group
- `file_count`: number of files in the group
- `column_count`: number of columns in the schema
- `files`: list of file names in the group


### `read_data`
Reads and merges all files that share the same schema.  
If `schema_id` is not specified, the group with the most columns will be selected.


## âœ… Requirements

- Python 3.10+
- PySpark


## ðŸ“š Motivation

In real-world data engineering workflows, it's common to deal with files that represent the same data domain but have slight structural variations â€” such as missing columns, different orders, or evolving schemas.  
**Datadock** automates the process of grouping, inspecting, and reading these files reliably, allowing you to build pipelines that are schema-aware, scalable, and format-agnostic.


## ðŸ“„ License

This project is licensed under the **MIT License**.
